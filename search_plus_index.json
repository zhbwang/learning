{"./":{"url":"./","title":"前言","keywords":"","body":"前言 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-04-03 20:00:24 "},"20190606_how-to-build-your-own-knowledge-base.html":{"url":"20190606_how-to-build-your-own-knowledge-base.html","title":"如何建立自己的知识库","keywords":"","body":"如何建立自己的知识库 知识库建立原则 使用知识的三层模型构建自己的知识库：卡片、文件、项目。 知识库建立最终极的目的是生成一套知识体系，提升生成新知识的能力，完成知识创造的伟大历程。 知识库类型。 知识库包括三大类：工作类、学习类、生活类 工作类包括一切公司内部公共知识与产品文档、项目文档等。 学习类包括一切通过学习获取的知识，包括技术学习、阅读书籍、听书、浏览文章等。这些都是已有的形式知识，需要通过学习总结不断内化为自己的暗隐知识，从而生成新的知识。 生活类主要是记录生活的重要事情，比如：买车买房的过程、日记等。 知识库操作指南 知识库的建立少不了工具的支持，工具的选择必须要精简，减少工具使用过程中不断做选择的精力耗散。 收集过程：使用Evernote、滴答清单进行收集。使用WorkFlowy也可以进行快速记录。 卡片类：所有的卡片全部记录在WorkFlowy中。 文件类：文件的目的是为了分享，所以每一篇文章都需要具备成果意识。用Markdown来写，使用MWeb工具。 学习与生活类文件的组织逻辑：放在 gitbook.zhbwang.com 域名文件夹中。 文件层级尽量不超过两层。比如学习类文件全部放在一个文件夹 wiki@learning 中；生活类文件全部放在一个文件夹 wiki@life 中。这种方法有一个很大的好处，就是所有文章都在一个文件夹，能形成合力。 在文件夹中新建 drafts 文件夹，临时草稿区。根目录中就是已经定稿的。 工作类的文件暂时还没有想好。 gitbook.zhbwang.com 域名文件夹使用坚果云存储。 项目类：项目是一个成果，比如一本书，一个产品，或者一个完成的产品文档。如果说把所有资料都丢掉，只能保留很小的一部分，那么能保留的部分就是这个项目类的成果了。所以项目类的成果一定要精简，并且有结构。 在文件类，已经将一篇一篇的文章写好了，那么项目就要使用README或者SUMMARY文件驱动，将文章形成有结构的一本书，或者一个体系。 这部分也同样使用 GitBook 完成。 在另一个目录新建 GitBook 文件夹，将生活类文件夹、学习类文件夹建立一个软连接到 GitBook 文件夹，写个脚本，自动编译为 html，本机中的 nginx 代理，通过 http://localhost 即可访问。 可以使用 Fluid 将 http://localhost 网址转换为APP的形式，可以很方便的打开。 下一步思考：工作类文件应该如何处理？ 参考资料 ChangeLog 20190606 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 19:39:53 "},"20190515_let-hadoop-run.html":{"url":"20190515_let-hadoop-run.html","title":"让Hadoop在MacOS上跑起来","keywords":"","body":"让Hadoop在MacOS上跑起来 大数据学习之路系列01 已发布博客：腾讯云社区、CSDN博客、语雀。 本安装文档是在MacOS中安装单机版Hadoop。 安装目录 WZB-MacBook:50_bigdata wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata 准备工作 JDK Mac安装JDK的过程略，参考：MAC下安装多版本JDK和切换几种方式 WZB-MacBook:50_bigdata wangzhibin$ java -version java version \"1.7.0_80\" Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) WZB-MacBook:50_bigdata wangzhibin$ echo $JAVA_HOME /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 下载Hadoop brew install wget wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/core/hadoop-2.8.4/hadoop-2.8.4.tar.gz WZB-MacBook:50_bigdata wangzhibin$ tar -zxvf hadoop-2.8.4.tar.gz 安装与配置Hadoop 修改JDK配置 WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hadoop-env.sh export JAVA_HOME=${JAVA_HOME}改为 export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 验证Hadoop WZB-MacBook:hadoop-2.8.4 wangzhibin$ bin/hadoop Usage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar run a jar file note: please use \"yarn jar\" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive classpath prints the class path needed to get the credential interact with credential providers Hadoop jar and the required libraries daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settings Most commands print help when invoked w/o parameters. 单机模式执行 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar grep input output 'dfs[a-z.]+' $ cat output/* 1 dfsadmin 配置core-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ mkdir -p hdfs/tmp WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/core-site.xml 增加如下配置： hadoop.tmp.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp Abase for other temporary directories. fs.defaultFS hdfs://localhost:9000 配置hdfs-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hdfs-site.xml 增加如下配置： dfs.replication 1 dfs.namenode.name.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name dfs.datanode.data.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/data 启动与停止Hadoop 配置.bash_profile # set hadoop export HADOOP_HOME=/Users/wangzhibin/00_dev_suite/50_bigdata/hadoop export PATH=$PATH:$HADOOP_HOME/bin 第一次启动hdfs需要格式化 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./bin/hdfs namenode -format ... 19/05/15 22:30:47 INFO common.Storage: Storage directory /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name has been successfully formatted. ... 启动HDFS ./sbin/start-dfs.sh 停止HDFS ./sbin/stop-dfs.sh HDFS启动状态查看 HDFS 状态：http://localhost:50070/dfshealth.html#tab-overview Secordary NameNode 状态：http://localhost:50090/status.html 本地官方文档：API文档 验证HDFS 简单的验证hadoop命令： $ hadoop fs -mkdir /test WZB-MacBook:hadoop wangzhibin$ hadoop fs -ls / Found 1 items drwxr-xr-x - wangzhibin supergroup 0 2019-05-16 11:26 /test 启动时遇到的坑 一、sh: connect to host localhost port 22: Connection refused 此时可能会出现如下错误。是因为没有配置ssh免密登录。 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:38:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: ssh: connect to host localhost port 22: Connection refused localhost: ssh: connect to host localhost port 22: Connection refused Starting secondary namenodes [0.0.0.0] 0.0.0.0: ssh: connect to host 0.0.0.0 port 22: Connection refused 19/05/15 22:38:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 采用如下方法解决： 1）解决方法是选择系统偏好设置->选择共享->点击远程登录 2）设置免密登录 $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys $ ssh localhost 二、Unable to load native-hadoop library for your platform WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:50:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: starting namenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-namenode-WZB-MacBook.local.out localhost: starting datanode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-datanode-WZB-MacBook.local.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out 19/05/15 22:50:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 参考： 官方-Native Libraries Guide Mac OSX 下 Hadoop 使用本地库提高效率 Hadoop native libraries: Installation on Mac Osx 解决方案：重新编译hadoop，将编译后的hadoop-dist/target/hadoop-2.8.4/lib/native替换$HADOOP_HOME/lib/native。 安装基础组件 $ brew install gcc autoconf automake libtool cmake snappy gzip bzip2 zlib 安装protobuf。 wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz tar zxvf protobuf-2.5.0.tar.gz cd protobuf-2.5.0 ./configure make make install 重新编译hadoop wget http://apache.fayea.com/hadoop/common/hadoop-2.8.4/hadoop-2.8.4-src.tar.gz tar zxvf hadoop-2.8.4-src.tar.gz cd hadoop-2.8.4-src mvn package -Pdist,native -DskipTests -Dtar -e cp -r /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-dist/target/hadoop-2.8.4/lib/native . 三、An Ant BuildException has occured: exec returned WZB-MacBook:hadoop-2.8.4-src wangzhibin$ mvn package -Pdist,native -DskipTests -Dtar -e ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 [ERROR] around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml [ERROR] -> [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml 参考：mac下编译Hadoop 2.8.1报错An Ant BuildException has occured: exec returned: 1，排错过程 解决方案：配置环境变量OPENSSL_ROOT_DIR、OPENSSL_INCLUDE_DIR。修改~/.bash_profile # openssl export OPENSSL_ROOT_DIR=/usr/local/Cellar/openssl/1.0.2r export OPENSSL_INCLUDE_DIR=$OPENSSL_ROOT_DIR/include 配置与启动yarn 配置mapred-site.xml cd $HADOOP_HOME/etc/hadoop/ cp mapred-site.xml.template mapred-site.xml vim mapred-site.xml mapreduce.framework.name yarn 配置yarn-site.xml vim yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle yarn启动与停止 启动 cd $HADOOP_HOME ./sbin/start-yarn.sh ./sbin/stop-yarn.sh 浏览器查看：http://localhost:8088 jps查看进程 WZB-MacBook:hadoop wangzhibin$ jps 534 NutstoreGUI 49135 DataNode 49834 ResourceManager 49234 SecondaryNameNode 49973 Jps 67596 49912 NodeManager 49057 NameNode 到此，hadoop单机模式就配置成功了！ 命令与验证 Resource Manager: http://localhost:50070 JobTracker: http://localhost:8088/ Node Specific Info: http://localhost:8042/ Command $ jps $ yarn // For resource management more information than the web interface. $ mapred // Detailed information about jobs Hadoop目录 WZB-MacBook:hadoop wangzhibin$ tree -L 2 . . ├── LICENSE.txt ├── NOTICE.txt ├── README.txt ├── bin │ ├── container-executor │ ├── hadoop │ ├── hadoop.cmd │ ├── hdfs │ ├── hdfs.cmd │ ├── mapred │ ├── mapred.cmd │ ├── rcc │ ├── test-container-executor │ ├── yarn │ └── yarn.cmd ├── etc │ └── hadoop ├── hdfs │ └── tmp ├── include │ ├── Pipes.hh │ ├── SerialUtils.hh │ ├── StringUtils.hh │ ├── TemplateFactory.hh │ └── hdfs.h ├── lib │ ├── native │ └── native-bak ├── libexec │ ├── hadoop-config.cmd │ ├── hadoop-config.sh │ ├── hdfs-config.cmd │ ├── hdfs-config.sh │ ├── httpfs-config.sh │ ├── kms-config.sh │ ├── mapred-config.cmd │ ├── mapred-config.sh │ ├── yarn-config.cmd │ └── yarn-config.sh ├── logs │ ├── SecurityAuth-wangzhibin.audit │ ├── hadoop-wangzhibin-datanode-WZB-MacBook.local.log │ ├── hadoop-wangzhibin-datanode-WZB-MacBook.local.out │ ├── hadoop-wangzhibin-datanode-WZB-MacBook.local.out.1 │ ├── hadoop-wangzhibin-namenode-WZB-MacBook.local.log │ ├── hadoop-wangzhibin-namenode-WZB-MacBook.local.out │ ├── hadoop-wangzhibin-namenode-WZB-MacBook.local.out.1 │ ├── hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.log │ ├── hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out │ ├── hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out.1 │ ├── userlogs │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.log │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.out │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.out.1 │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.out.2 │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.out.3 │ ├── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.log │ ├── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.out │ ├── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.out.1 │ ├── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.out.2 │ └── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.out.3 ├── sbin │ ├── distribute-exclude.sh │ ├── hadoop-daemon.sh │ ├── hadoop-daemons.sh │ ├── hdfs-config.cmd │ ├── hdfs-config.sh │ ├── httpfs.sh │ ├── kms.sh │ ├── mr-jobhistory-daemon.sh │ ├── refresh-namenodes.sh │ ├── slaves.sh │ ├── start-all.cmd │ ├── start-all.sh │ ├── start-balancer.sh │ ├── start-dfs.cmd │ ├── start-dfs.sh │ ├── start-secure-dns.sh │ ├── start-yarn.cmd │ ├── start-yarn.sh │ ├── stop-all.cmd │ ├── stop-all.sh │ ├── stop-balancer.sh │ ├── stop-dfs.cmd │ ├── stop-dfs.sh │ ├── stop-secure-dns.sh │ ├── stop-yarn.cmd │ ├── stop-yarn.sh │ ├── yarn-daemon.sh │ └── yarn-daemons.sh └── share ├── doc └── hadoop 参考资料 Hadoop: Setting up a Single Node Cluster. centos7 hadoop 单机模式安装配置 Hadoop in OSX El-Capitan Installing Hadoop on Mac OS X 10.9.4 macOS上搭建伪分布式Hadoop环境 本地官方API文档 ChangeLog 20190611 | 增加Hadoop目录结构 20190515 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-11 14:50:27 "},"20190517_the-first-mapreduce-program.html":{"url":"20190517_the-first-mapreduce-program.html","title":"第一个MapReduce程序","keywords":"","body":"第一个MapReduce程序 2019-05-17 | 大数据学习之路系列02 已发布博客：腾讯云社区、CSDN博客、语雀。 目标 单词计数是最简单也是最能体现 MapReduce 思想的程序之一，可以称为 MapReduce 版“Hello World”。 单词计数主要完成功能是：统计一系列文本文件中每个单词出现的次数，如下图所示。 准备工作 新建目录 WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/input WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/output WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/input drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 准备文件 WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file1.txt Hello World WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file2.txt Hello Hadoop WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 14:50 /practice/20190517_mr/input -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 14:49 /practice/20190517_mr/input/file1.txt -rw-r--r-- 1 wangzhibin supergroup 13 2019-05-17 14:49 /practice/20190517_mr/input/file2.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 运行例子 在集群上运行 WordCount 程序 备注:以 input 作为输入目录，output 目录作为输出目录。 已经编译好的 WordCount 的 Jar 在“$HADOOP_HOME/share/hadoop/mapreduce/”下面，就是“hadoop-mapreduce-examples-2.8.4.jar”， MapReduce 执行过程显示信息 执行命令： hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 执行过程： WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:39:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:39:20 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558078701666_0002 19/05/17 15:39:20 INFO impl.YarnClientImpl: Submitted application application_1558078701666_0002 19/05/17 15:39:20 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1558078701666_0002/ 19/05/17 15:39:20 INFO mapreduce.Job: Running job: job_1558078701666_0002 19/05/17 15:39:28 INFO mapreduce.Job: Job job_1558078701666_0002 running in uber mode : false 19/05/17 15:39:28 INFO mapreduce.Job: map 0% reduce 0% 19/05/17 15:39:33 INFO mapreduce.Job: map 100% reduce 0% 19/05/17 15:39:39 INFO mapreduce.Job: map 100% reduce 100% 19/05/17 15:39:39 INFO mapreduce.Job: Job job_1558078701666_0002 completed successfully 19/05/17 15:39:39 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=55 FILE: Number of bytes written=474472 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=271 HDFS: Number of bytes written=25 HDFS: Number of read operations=9 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=2 Launched reduce tasks=1 Data-local map tasks=2 Total time spent by all maps in occupied slots (ms)=6213 Total time spent by all reduces in occupied slots (ms)=2848 Total time spent by all map tasks (ms)=6213 Total time spent by all reduce tasks (ms)=2848 Total vcore-milliseconds taken by all map tasks=6213 Total vcore-milliseconds taken by all reduce tasks=2848 Total megabyte-milliseconds taken by all map tasks=6362112 Total megabyte-milliseconds taken by all reduce tasks=2916352 Map-Reduce Framework Map input records=2 Map output records=4 Map output bytes=41 Map output materialized bytes=61 Input split bytes=246 Combine input records=4 Combine output records=4 Reduce input groups=3 Reduce shuffle bytes=61 Reduce input records=4 Reduce output records=3 Spilled Records=8 Shuffled Maps =2 Failed Shuffles=0 Merged Map outputs=2 GC time elapsed (ms)=97 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=603979776 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=25 File Output Format Counters Bytes Written=25 查看结果 WZB-MacBook:hadoop wangzhibin$ hadoop dfs -ls -R /practice/20190517_mr/output/ -rw-r--r-- 1 wangzhibin supergroup 0 2019-05-17 15:39 /practice/20190517_mr/output/_SUCCESS -rw-r--r-- 1 wangzhibin supergroup 25 2019-05-17 15:39 /practice/20190517_mr/output/part-r-00000 WZB-MacBook:hadoop wangzhibin$ hadoop fs -cat /practice/20190517_mr/output/* Hadoop 1 Hello 2 World 1 遇到的坑 问题一：执行到Running job: job_1557977819409_0004的地方就不往下执行了。 WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoopoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:01:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:01:03 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:01:03 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:01:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557977819409_0004 19/05/17 15:01:04 INFO impl.YarnClientImpl: Submitted application application_1557977819409_0004 19/05/17 15:01:04 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1557977819409_0004/ 19/05/17 15:01:04 INFO mapreduce.Job: Running job: job_1557977819409_0004 参考： Hadoop相关总结 Hadoop 运行wordcount任务卡在job running的一种解决办法 hadoop2.7.x运行wordcount程序卡住在INFO mapreduce.Job: Running job:job _1469603958907_0002 Can't run a MapReduce job on hadoop 2.4.0 解决方案：在$HADOOP_HOME/etc/hadoop/yarn-site.xml中增加配置。 yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 最终的yarn-site.xml如下： yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce.shuffle.class org.apache.hadoop.mapred.ShuffleHandler yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 重启yarn： ./sbin/stop-yarn.sh ./sbin/start-yarn.sh ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:02:47 "},"20190517_detailed-hadoop-commands.html":{"url":"20190517_detailed-hadoop-commands.html","title":"Hadoop常用命令详解","keywords":"","body":"Hadoop常用命令详解 大数据学习之路03 已发布博客：腾讯云社区、CSDN博客、语雀。 Hadoop基本命令 version 查看Hadoop版本。 WZB-MacBook:target wangzhibin$ hadoop version Hadoop 2.8.4 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674 Compiled by jdu on 2018-05-08T02:50Z Compiled with protoc 2.5.0 From source with checksum b02a59bb17646783210e979bea443b0 This command was run using /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/share/hadoop/common/hadoop-common-2.8.4.jar HDFS基础命令 命令格式 hadoop fs -cmd ls 列出hdfs文件系统根目录下的目录和文件 hadoop fs -ls / 列出hdfs文件系统所有的目录和文件 hadoop fs -ls -R / mkdir 一级一级的建目录，父目录不存在的话使用这个命令会报错 command: hadoop fs -mkdir eg: WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir /test/20190517 WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 所创建的目录如果父目录不存在就创建该父目录 hadoop fs -mkdir -p put 上传文件。hdfs file的父目录一定要存在，否则命令不会执行 command: hadoop fs -put eg: $ hadoop fs -put tmp/tmp.txt /test $ hadoop fs -ls -R /test 上传目录。hdfs dir 一定要存在，否则命令不会执行 command: hadoop fs -put ... eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put tmp/ /test WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 从键盘读取输入到hdfs file中，按Ctrl+D（Control+D）结束输入。hdfs file不能存在，否则命令不会执行 command: hadoop fs -put - eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put - /test/20190517.tmp.txt hello world WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 10:44 /test/20190517.tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cat 在标准输出中显示文件内容 command: hadoop fs -cat eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -cat /test/20190517.tmp.txt hello world tail 在标准输出中显示文件末尾的1KB数据 command: hadoop fs -tail eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -tail /test/20190517.tmp.txt hello world get 从hdfs中下载文件到本地。local file不能和 hdfs file名字不能相同，否则会提示文件已存在，没有重名的文件会复制到本地 command: hadoop fs -get eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test/20190517.tmp.txt . WZB-MacBook:tmp wangzhibin$ ls 20190517.tmp.txt tmp.txt 拷贝多个文件或目录到本地时，本地要为文件夹路径 command: hadoop fs -get ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test . WZB-MacBook:tmp wangzhibin$ ls -l test/ total 24 drwxr-xr-x 2 wangzhibin staff 64 5 17 10:56 20190517 -rw-r--r-- 1 wangzhibin staff 12 5 17 10:56 20190517.tmp.txt drwxr-xr-x 3 wangzhibin staff 96 5 17 10:56 tmp -rw-r--r-- 1 wangzhibin staff 4662 5 17 10:56 tmp.txt 注意：如果用户不是root， local 路径要为用户文件夹下的路径，否则会出现权限问题。 rm 每次可以删除多个文件或目录 command: hadoop fs -rm ... hadoop fs -rm -r ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -rm /test/20190517.tmp.txt Deleted /test/20190517.tmp.txt WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cp HDFS拷贝文件或文件夹。目标文件或者文件夹不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件还存在。 command: hadoop fs -cp hadoop fs -cp ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -cp /test/tmp.txt /test/20190517/ WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:03 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt mv HDFS移动文件或文件夹。目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件不存在。源路径有多个时，目标路径必须为目录，且必须存在。 command: hadoop fs -mv hadoop fs -mv ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -mv /test/20190517/tmp.20190519.txt /test/tmp WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:04 /test/tmp/tmp.20190519.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 注意：跨文件系统的移动（local到hdfs或者反过来）都是不允许的 count 统计hdfs对应路径下的目录个数，文件个数，文件总计大小 显示为目录个数，文件个数，文件总计大小，输入路径 command: hadoop fs -count eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -count /test 3 4 18648 /test du 显示hdfs对应路径下每个文件夹和文件的大小 command: hadoop fs -du eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du /test 4662 /test/20190517 9324 /test/tmp 4662 /test/tmp.txt 显示hdfs对应路径下所有文件和的大小 command: hadoop fs -du -s eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -s /test 18648 /test 显示hdfs对应路径下每个文件夹和文件的大小,文件的大小用方便阅读的形式表示，例如用64M代替67108864 command: hadoop fs -du -h eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -h /test 4.6 K /test/20190517 9.1 K /test/tmp 4.6 K /test/tmp.txt HDFS高级命令 以下命令参考：hadoop HDFS常用文件操作命令。没有实践。 moveFromLocal hadoop fs -moveFromLocal ... 与put相类似，命令执行后源文件 local src 被删除，也可以从从键盘读取输入到hdfs file中 copyFromLocal hadoop fs -copyFromLocal ... 与put相类似，也可以从从键盘读取输入到hdfs file中 moveToLocal 当前版本中还未实现此命令 copyToLocal hadoop fs -copyToLocal ... 与get相类似 getmerge hadoop fs -getmerge 将hdfs指定目录下所有文件排序后合并到local指定的文件中，文件不存在时会自动创建，文件存在时会覆盖里面的内容 hadoop fs -getmerge -nl 加上nl后，合并到local file中的hdfs文件之间会空出一行 text hadoop fs -text 将文本文件或某些格式的非文本文件通过文本格式输出 setrep hadoop fs -setrep -R 3 改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作 stat hdoop fs -stat [format] 返回对应路径的状态信息 [format]可选参数有：%b（文件大小），%o（Block大小），%n（文件名），%r（副本个数），%y（最后一次修改日期和时间） 可以这样书写hadoop fs -stat %b%o%n ，不过不建议，这样每个字符输出的结果不是太容易分清楚 archive hadoop archive -archiveName name.har -p * 命令中参数name：压缩文件名，自己任意取； ：压缩文件所在的父目录；：要压缩的文件名；：压缩文件存放路径\\示例：hadoop archive -archiveName hadoop.har -p /user 1.txt 2.txt /des* 示例中将hdfs中/user目录下的文件1.txt，2.txt压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下，如果1.txt，2.txt不写就是将/user目录下所有的目录和文件压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下 显示har的内容可以用如下命令： hadoop fs -ls /des/hadoop.jar 显示har压缩的是那些文件可以用如下命令 hadoop fs -ls -R har:///des/hadoop.har 注意：har文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少NameNode和DataNode过多的空间浪费。 balancer hdfs balancer 如果管理员发现某些DataNode保存数据过多，某些DataNode保存数据相对较少，可以使用上述命令手动启动内部的均衡过程 dfsadmin hdfs dfsadmin -help 管理员可以通过dfsadmin管理HDFS，用法可以通过上述命令查看 hdfs dfsadmin -report 显示文件系统的基本数据 hdfs dfsadmin -safemode enter：进入安全模式；leave：离开安全模式；get：获知是否开启安全模式； wait：等待离开安全模式 distcp 用来在两个HDFS之间拷贝数据 MapReduce命令 命令帮助 WZB-MacBook:target wangzhibin$ mapred -help Usage: mapred [--config confdir] [--loglevel loglevel] COMMAND where COMMAND is one of: pipes run a Pipes job job manipulate MapReduce jobs queue get information regarding JobQueues classpath prints the class path needed for running mapreduce subcommands historyserver run job history servers as a standalone daemon distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive archive-logs combine aggregated logs into hadoop archives hsadmin job history server admin interface Most commands print help when invoked w/o parameters. 列出所有任务 WZB-MacBook:target wangzhibin$ mapred job -list all 19/05/20 10:22:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Total jobs:1 JobId State StartTime UserName Queue Priority UsedContainers RsvdContainers UsedMem RsvdMem NeededMem AM info job_1558104288185_0001 SUCCEEDED 1558104322342 wangzhibin default DEFAULT N/A N/A N/A N/A N/A http://WZB-MacBook.local:8088/proxy/application_1558104288185_0001/ 强制停止任务 mapred job -kill 参考资料 1.0.4版本官方文档-Hadoop Shell命令 hadoop HDFS常用文件操作命令 大数据基本组件（Hadoop、HDFS、MapRed、YARN）入门命令 ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:14 "},"20190517_develop-mapreduce-program-with-idea.html":{"url":"20190517_develop-mapreduce-program-with-idea.html","title":"使用IDEA开发MapReduce程序","keywords":"","body":"使用IDEA开发MapReduce程序 2019-05-17 | 大数据学习之路04 环境准备 jdk1.7 intellij idea maven 本地MapReduce程序之WordCount 这里以Hadoop的官方示例程序WordCount为例，演示如何一步步编写程序直到运行。 新建一个Maven工程 使用idea新建一个普通maven项目bigdata-learn-wordcount maven依赖 org.apache.hadoop hadoop-common 2.8.4 org.apache.hadoop hadoop-hdfs 2.8.4 org.apache.hadoop hadoop-mapreduce-client-core 2.8.4 org.apache.hadoop hadoop-client 2.8.4 拷贝Hadoop中的WordCount源码 /** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package com.zhbwang.bigdata.example; import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class WordCount { public static class TokenizerMapper extends Mapper{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } public static class IntSumReducer extends Reducer { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length [...] \"); System.exit(2); } Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); for (int i = 0; i 初始化文件 在工程根目录新建input文件夹，增加两个文件 input - file1.TXT Hello World - file2.txt Hello Hadoop 运行配置 程序执行 执行日志 执行结果 打包到服务器使用hadoop jar命令执行 pom.xml增加打包插件 maven-assembly-plugin 2.2 com.zhbwang.bigdata.example.WordCount jar-with-dependencies make-assembly package single maven打包 $ mvn clean install 得到一个可执行jar包：bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar 使用java -jar执行 在当前可执行jar目录初始化input文件夹 执行以下命令，即可在当前目录生成output文件夹，里面就是执行结果。 java -jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar input output 使用hadoop jar执行 一开始遇到问题了，还以为打包打的不对，换了几个打包插件都不行。 WZB-MacBook:target wangzhibin$ hadoop jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar /practice/20190517_mr/input /practice/20190517_mr/output Exception in thread \"main\" java.io.IOException: Mkdirs failed to create /var/folders/gg/35tlzsrs1kj3c460vh9tvvv40000gn/T/hadoop-unjar2170725475686001105/META-INF/license at org.apache.hadoop.util.RunJar.ensureDirectory(RunJar.java:140) at org.apache.hadoop.util.RunJar.unJar(RunJar.java:109) at org.apache.hadoop.util.RunJar.unJar(RunJar.java:85) at org.apache.hadoop.util.RunJar.run(RunJar.java:222) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) 后来找到几篇文章，发现是Mac的问题，在stackoverflow中找到解释： The issue is that a /tmp/hadoop-xxx/xxx/LICENSE file and a /tmp/hadoop-xxx/xxx/license directory are being created on a case-insensitive file system when unjarring the mahout jobs. 参考资料： Hadoop java.io.IOException: Mkdirs failed to create /some/path Mac下hadoop运行word count的坑 解决方案：删除原来压缩包的META-INF/LICENS即可。 zip -d bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar META-INF/LICENSE jar tvf bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar | grep LICENSE 接下来就可以使用hadoop jar命令执行了。 WZB-MacBook:target wangzhibin$ hadoop jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 22:07:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 22:07:10 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 22:07:10 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 22:07:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558078701666_0004 19/05/17 22:07:11 INFO impl.YarnClientImpl: Submitted application application_1558078701666_0004 19/05/17 22:07:11 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1558078701666_0004/ 19/05/17 22:07:11 INFO mapreduce.Job: Running job: job_1558078701666_0004 查看结果。 WZB-MacBook:target wangzhibin$ hadoop fs -cat /practice/20190517_mr/output/part-r-00000 Hadoop 1 Hello 2 World 1 问题 一、Hadoop 2.x中还有hadoop-core-x.x.jar吗？ 答：2.x系列已经没有hadoop-core的jar包了，取而代之的是 对于Hadoop2.x.x版本，需要引入4个jar： hadoop-common hadoop-hdfs hadoop-mapreduce-client-core hadoop-client jdk.tools（一般需要引入，否则报错） 参考：Hadoop需要的Jar包 参考资料 IDEA 配置Hadoop开发（开发调试） Hadoop入门学习之（二）：Intellij 开发Hadoop环境搭建 Hadoop: Intellij结合Maven本地运行和调试MapReduce程序 (无需搭载Hadoop和HDFS环境) 大数据系列（hadoop） 集群环境搭建 idea 开发设置 Mac下hadoop运行word count的坑 ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:00:43 "},"20190522_principle-and-architecture-analysis-of-hadoop .html":{"url":"20190522_principle-and-architecture-analysis-of-hadoop .html","title":"Hadoop原理与架构解析","keywords":"","body":"2019-05-22 | Hadoop原理与架构解析 2019-05-22 | 大数据学习之路05 Hadoop简介 基本介绍 Hadoop 是 Apache 开源组织的一个分布式计算开源框架，是一个可以更容易开发和运行处理大规模数据的解决方案，它提供了一套分布式系统基础架构，允许使用简单的编程模型跨大型计算机的大型数据集进行分布式处理。 Hadoop架构 Hadoop框架包括以下四个模块： Hadoop Common：这些是其他Hadoop模块所需的Java库和实用程序。这些库提供文件系统和操作系统级抽象，并包含启动Hadoop所需的必要Java文件和脚本。 Hadoop YARN：这是作业调度和集群资源管理的框架。 Hadoop分布式文件系统（HDFS）：提供对应用程序数据的高吞吐量访问的分布式文件系统。 Hadoop MapReduce： 这是基于YARN的大型数据集并行处理系统。 Hadoop 框架中最核心的设计就是：MapReduce 和 HDFS。 MapReduce 的思想是由 Google 的一篇论文所提及而被广为流传的，简单的一句话解释 MapReduce 就是“任务的分解与结果的汇总”。 HDFS 是 Hadoop 分布式文件系统（Hadoop Distributed File System）的缩写，为分布式计算存储提供了底层支持。 HDFS运行原理 HDFS简介 HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。 HDFS有很多特点： 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。为防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其他某几个主机上。 运行在廉价的机器上。 适合大数据的处理。HDFS会将一个完整的大文件平均分块存储到不同计算机上，默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。 流式数据访问，一次写入多次读写，和传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化只能在文件末尾添加 HDFS架构原理 HDFS 架构原理HDFS采用Master/Slave架构。 一个HDFS集群包含一个单独的NameNode和多个DataNode。 NameNode作为Master服务，它负责管理文件系统的命名空间和客户端对文件的访问。NameNode会保存文件系统的具体信息，包括文件信息、文件被分割成具体block块的信息、以及每一个block块归属的DataNode的信息。对于整个集群来说，HDFS通过NameNode对用户提供了一个单一的命名空间。 DataNode作为Slave服务，在集群中可以存在多个。通常每一个DataNode都对应于一个物理节点。DataNode负责管理节点上它们拥有的存储，它将存储划分为多个block块，管理block块信息，同时周期性的将其所有的block块信息发送给NameNode。 下图为HDFS系统架构图，主要有三个角色，Client、NameNode、DataNode。 HDFS的一些关键元素 Block：将文件分块，通常为64M。 NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间。保存整个文件系统的目录信息、文件信息及分块信息，由唯一一台主机专门保存。 SecondaryNameNode：是一个小弟，分担大哥NameNode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给NameNode。（热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。） DataNode：是Slave节点，奴隶，干活的。负责存储Client发来的数据块block；执行数据块的读写操作。 fsimage：元数据镜像文件（文件系统的目录树） edits：元数据的操作日志（针对文件系统做的修改操作记录） HDFS设计重点 HDFS 数据备份HDFS被设计成一个可以在大集群中、跨机器、可靠的存储海量数据的框架。它将所有文件存储成block块组成的序列，除了最后一个block块，所有的block块大小都是一样的。 HDFS中的文件默认规则是write one（一次写、多次读）的，并且严格要求在任何时候只有一个writer。 NameNode全权管理数据块的复制，它周期性地从集群中的每个DataNode接受心跳信号和块状态报告（BlockReport）。接收到心跳信号以为该DataNode工作正常，块状态报告包含了一个该DataNode上所有数据块的列表。 NameNode内存中存储的是=fsimage+edits。SecondaryNameNode负责定时（默认1小时）从NameNode上，获取fsimage和edits来进行合并，然后再发送给NameNode。减少NameNode的工作量。 文件写入 Client向NameNode发起文件写入的请求。 NameNode根据文件大小和文件块配置情况，返回给Client它所管理部分DataNode的信息。 Client将文件划分为多个block块，并根据DataNode的地址信息，按顺序写入到每一个DataNode块中。 以下过程完全参考自（【Hadoop】HDFS的运行原理） 例如：有一个文件FileA，100M大小。Client将FileA写入到HDFS上。 HDFS按默认配置。 HDFS分布在三个机架上Rack1，Rack2，Rack3。 文件写入过程如下： Client将FileA按64M分块。分成两块，block1和Block2; Client向NameNode发送写数据请求，如图蓝色虚线①------>。 NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②--------->。 Block1: host2,host1,host3 Block2: host7,host8,host4 原理： NameNode具有RackAware机架感知功能，这个可以配置。 若Client为DataNode节点，那存储block时，规则为：副本1，同Client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。 若Client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。 Client向DataNode发送block1；发送过程是以流式写入。流式写入过程如下： 将64M的block1按64k的package划分; 然后将第一个package发送给host2; host2接收完后，将第一个package发送给host1，同时Client想host2发送第二个package； host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。 以此类推，如图红线实线所示，直到将block1发送完毕。 host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。 Client收到host2发来的消息后，向NameNode发送消息，说我写完了。这样就真完成了。如图黄色粗实线 发送完block1后，再向host7、host8、host4发送block2，如图蓝色实线所示。 发送完block2后，host7、host8、host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。 Client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。 分析：通过写过程，我们可以了解到 写1T文件，我们需要3T的存储，3T的网络流量贷款。 在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。 挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。 文件读取 当文件读取： Client向NameNode发起文件读取的请求。 NameNode返回文件存储的block块信息、及其block块所在DataNode的信息。 Client读取文件信息。 如图所示，Client要从DataNode上，读取FileA。而FileA由block1和block2组成。读操作流程如下： Client向NameNode发送读请求。 NameNode查看Metadata信息，返回FileA的block的位置。 block1:host2,host1,host3 block2:host7,host8,host4 block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取。 上面例子中，Client位于机架外，那么如果Client位于机架内某个DataNode上，例如,Client是host6。那么读取的时候，遵循的规律是：优选读取本机架上的数据。 问题：如果读取block是按照先后顺序读，是否意味着在不同副本之间的读取是不平均的，没有考虑通过负载策略来提高读效率吗？ 备份数据的存放 备份数据的存放是HDFS可靠性和性能的关键。HDFS采用一种称为rack-aware的策略来决定备份数据的存放。 通过一个称为Rack Awareness的过程，NameNode决定每个DataNode所属rack id。 缺省情况下，一个block块会有三个备份： 一个在NameNode指定的DataNode上 一个在指定DataNode非同一rack的DataNode上 一个在指定DataNode同一rack的DataNode上。 这种策略综合考虑了同一rack失效、以及不同rack之间数据复制性能问题。 副本的选择：为了降低整体的带宽消耗和读取延时，HDFS会尽量读取最近的副本。如果在同一个rack上有一个副本，那么就读该副本。如果一个HDFS集群跨越多个数据中心，那么将首先尝试读本地数据中心的副本。 MapReduce运行原理 MapReduce简介 MapReduce是一种分布式计算模型，由Google提出，主要用于搜索领域，解决海量数据的计算问题。 MapReduce分成两个部分：Map（映射）和Reduce（归纳）。 当你向MapReduce框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map任务，然后分配到不同的节点上去执行，每一个Map任务处理输入数据中的一部分。 当Map任务完成后，它会生成一些中间文件，这些中间文件将会作为Reduce任务的输入数据。Reduce任务的主要目标就是把前面若干个Map的输出汇总并输出。 MapReduce的基本模型和处理思想 大规模数据处理时，MapReduce在三个层面上的基本构思 参考（MapReduce的基本工作原理） 如何对付大数据处理：分而治之 对相互间不具有计算依赖关系的大数据，实现并行最自然的办法就是采取分而治之的策略。 什么样的计算任务可进行并行化计算？ A：不可分拆的计算任务或相互间有依赖关系的数据无法进行并行计算！ 一个大数据若可以分为具有同样计算过程的数据块，并且这些数据块之间不存在数据依赖关系，则提高处理速度的最好办法就是并行计算。 上升到抽象模型：Mapper与Reducer MPI等并行计算方法缺少高层并行编程模型，为了克服这一缺陷，MapReduce借鉴了Lisp函数式语言中的思想，用Map和Reduce两个函数提供了高层的并行编程抽象模型。 关键思想：为大数据处理过程中的两个主要处理操作提供一种抽象机制。Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。 MapReduce借鉴了函数式程序设计语言Lisp中的思想，定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现: map: (k1; v1) → [(k2; v2)]。 输入：键值对(k1; v1)表示的数据 处理：文档数据记录(如文本文件中的行，或数据表格中的行)将以“键值对”形式传入map函数；map函数将处理这些键值对，并以另一种键值对形式输出处理的一组键值对中间结果[(k2; v2)] 输出：键值对[(k2; v2)]表示的一组中间数据 reduce: (k2; [v2]) → [(k3; v3)] 输入： 由map输出的一组键值对[(k2; v2)] 将被进行合并处理将同样主键下的不同数值合并到一个列表[v2]中，故reduce的输入为(k2; [v2]) 处理：对传入的中间结果列表数据进行某种整理或进一步的处理,并产生最终的某种形式的结果输出[(k3; v3)] 。 输出：最终输出结果[(k3; v3)] 示例：假设有4组原始文本数据 Text 1: the weather is good Text 2: today is good Text 3: good weather is good Text 4: today has good weather MapReduce处理方式： 使用4个map节点： map节点1: 输入：(text1, “the weather is good”) 输出：(the, 1), (weather, 1), (is, 1), (good, 1) map节点2: 输入：(text2, “today is good”) 输出：(today, 1), (is, 1), (good, 1) map节点3: 输入：(text3, “good weather is good”) 输出：(good, 1), (weather, 1), (is, 1), (good, 1) map节点4: 输入：(text3, “today has good weather”) 输出：(today, 1), (has, 1), (good, 1), (weather, 1) 使用3个reduce节点： reduce节点1： 输入：(good, 1),(good, 1),(good, 1),(good, 1),(good, 1) 输出：(good, 5) reduce节点2： 输入：(has, 1),(is, 1),(is, 1),(is, 1) 输出：(has, 1),(is, 3) reduce节点3： 输入：(the, 1),(today, 1),(today, 1),(weather, 1),(weather, 1),(weather, 1) 输出：(the, 1),(today, 2),(weather, 3) 上升到构架：统一构架，为程序员隐藏系统层细节 MPI等并行计算方法缺少统一的计算框架支持，程序员需要考虑数据存储、划分、分发、结果收集、错误恢复等诸多细节；为此，MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。 各个map函数对所划分的数据并行处理，从不同的输入数据产生不同的中间结果输出 各个reduce也各自并行计算，各自负责处理不同的中间结果数据集合进行reduce处理之前，必须等到所有的map函数做完，因此，在进入reduce前需要有一个同步障(barrier)；这个阶段也负责对map的中间结果数据进行收集整理(aggregation & shuffle)处理，以便reduce更有效地计算最终结果最终汇总所有reduce的输出结果即可获得最终结果。 MapReduce提供一个统一的计算框架，可完成： 计算任务的划分和调度 数据的分布存储和划分 处理数据与计算任务的同步 结果数据的收集整理(sorting, combining, partitioning,…) 系统通信、负载平衡、计算性能优化处理 处理系统节点出错检测和失效恢复 MapReduce运行流程 MapReduce的物理架构 Map-Reduce的处理过程主要涉及以下四个部分： 客户端Client：用于提交Map-reduce任务job JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件 MapReduce的逻辑运行流程 MapReduce运行按照时间顺序包括五个阶段：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。 输入分片（input split） 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务。 输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组 输入分片（input split）和hdfs的block（块）关系很密切。假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。 map阶段：就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行。 combiner阶段： Combiner是一个本地化的reduce操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作 shuffle阶段 将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。 具体shuffle的过程不介绍了。 reduce阶段：和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。 简单的来说： 有一个待处理的大数据，被划分成大小相同的数据库(如64MB)，以及与此相应的用户作业程序。 系统中有一个负责调度的主节点(JobTracker)，以及数据Map和Reduce工作节点(TaskTracker). 用户作业提交给主节点JobTracker。 主节点为作业程序寻找和配备可用的Map节点，并将程序传送给map节点。 主节点也为作业程序寻找和配备可用的Reduce节点，并将程序传送给Reduce节点。 主节点启动每一个Map节点执行程序，每个Map节点尽可能读取本地或本机架的数据进行计算。(实现代码向数据靠拢，减少集群中数据的通信量)。 每个Map节点处理读取的数据块，并做一些数据整理工作(combining,sorting等)并将数据存储在本地机器上；同时通知主节点计算任务完成并告知主节点中间结果数据的存储位置。 主节点等所有Map节点计算完成后，开始启动Reduce节点运行；Reduce节点从主节点所掌握的中间结果数据位置信息，远程读取这些数据。 Reduce节点计算结果汇总输出到一个结果文件，即获得整个处理结果。 YARN运行原理 YARN简介 Yarn是Hadoop集群的分布式资源管理系统。Hadoop2.0对MapReduce框架做了彻底的设计重构，我们称Hadoop2.0中的MapReduce为MRv2或者Yarn，YARN是为了提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。 原MapReduce框架的不足 Hadoop 原 MapReduce 架构如下： 原 MapReduce 程序的流程及设计思路： 首先用户程序 (JobClient) 提交了一个 job，job 的信息会发送到 Job Tracker 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。 TaskTracker 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。 TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以给新提交的 job 分配运行在哪些机器上。上图虚线箭头就是表示消息的发送 - 接收的过程。 随着分布式系统集群的规模和其工作负荷的增长，原框架的问题逐渐浮出水面，主要的问题集中如下： JobTracker 是 Map-reduce 的集中处理点，存在单点故障。 JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗。业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限。 在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一块，很容易出现 OOM。 在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。 源代码层面分析的时候，会发现代码非常的难读，常常因为一个 class 做了太多的事情，代码量达 3000 多行，造成 class 的任务不清晰，增加 bug 修复和版本维护的难度。 从操作的角度来看，现在的 Hadoop MapReduce 框架在有任何重要的或者不重要的变化 ( 例如 bug 修复，性能提升和特性化 ) 时，都会强制进行系统级别的升级更新。更糟的是，它不管用户的喜好，强制让分布式集群系统的每一个用户端同时更新。这些更新会让用户为了验证他们之前的应用程序是不是适用新的 Hadoop 版本而浪费大量时间。 Yarn/MRv2的产生 为从根本上解决旧 MapReduce 框架的性能瓶颈，促进 Hadoop 框架的更长远发展，从 0.23.0 版本开始，Hadoop 的 MapReduce 框架完全重构，发生了根本的变化。新的 Hadoop MapReduce 框架命名为 MapReduceV2 或者叫 Yarn。 在Yarn中把job的概念换成了application，因为在新的Hadoop2.x中，运行的应用不只是MapReduce了，还有可能是其它应用如一个DAG（有向无环图Directed Acyclic Graph，例如storm应用）。 Yarn的另一个目标就是拓展Hadoop，使得它不仅仅可以支持MapReduce计算，还能很方便的管理诸如Hive、Hbase、Pig、Spark/Shark等应用。各种应用就可以互不干扰的运行在同一个Hadoop系统中，共享整个集群资源。 YARN组件与架构 Yarn主要由以下几个组件组成： ResourceManager：Global（全局）的进程 NodeManager：运行在每个节点上的进程 ApplicationMaster：Application-specific（应用级别）的进程。ApplicationMaster是对运行在Yarn中某个应用的抽象，它其实就是某个类型应用的实例，ApplicationMaster是应用级别的，它的主要功能就是向ResourceManager（全局的）申请计算资源（Containers）并且和NodeManager交互来执行和监控具体的task。 Scheduler：是ResourceManager的一个组件。Scheduler是ResourceManager专门进行资源管理的一个组件，负责分配NodeManager上的Container资源，NodeManager也会不断发送自己Container使用情况给ResourceManager。 Container：节点上一组CPU和内存资源。Container是Yarn对计算机计算资源的抽象，它其实就是一组CPU和内存资源，所有的应用都会运行在Container中。 新的 Hadoop MapReduce 框架（Yarn）架构 YARN执行过程 Application在Yarn中的执行过程，整个执行过程可以总结为三步： 应用程序提交 启动应用的ApplicationMaster实例 ApplicationMaster实例管理应用程序的执行 客户端程序向ResourceManager提交应用并请求一个ApplicationMaster实例 ResourceManager找到可以运行一个Container的NodeManager，并在这个Container中启动ApplicationMaster实例 ApplicationMaster向ResourceManager进行注册，注册之后客户端就可以查询ResourceManager获得自己ApplicationMaster的详细信息，以后就可以和自己的ApplicationMaster直接交互了 在平常的操作过程中，ApplicationMaster根据resource-request协议向ResourceManager发送resource-request请求 当Container被成功分配之后，ApplicationMaster通过向NodeManager发送container-launch-specification信息来启动Container， container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料 应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster 在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，交流的协议也是application-specific协议 一但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，用到所有的Container也归还给系统 。 思考题 还是那个经典的题目，一个10G大小的文件，给定1G大小的内存，如何使用Java程序统计出现次数最多的10个单词及次数 参考资料 Hadoop简介 分布式计算开源框架 Hadoop 介绍 Hadoop-介绍 【Hadoop】HDFS的运行原理 分布式计算框架Hadoop原理及架构全解 Hadoop 原理总结 MapReduce原理与设计思想 MapReduce的基本工作原理 Hadoop 之MapReduce 运行原理全解析 hadoop 学习笔记：mapreduce框架详解 Hadoop核心之HDFS 架构设计 Hadoop核心之MapReduce架构设计 Hadoop Yarn详解 Hadoop 新 MapReduce 框架 Yarn 详解 ChangeLog 20190522 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 17:12:50 "},"20190525_installation-and-use-of-hive.html":{"url":"20190525_installation-and-use-of-hive.html","title":"Hive安装与使用","keywords":"","body":"2019-05-25 | Hive安装与使用 2019-05-25 | 大数据学习之路06 Hive简介 这里先简单介绍，明确Hive的目标是什么。后续会详细介绍Hive架构与原理。 Hive是基于Hadoop的数据仓库工具，可以对存储在HDFS中的文件的数据进行数据整理、查询、分析。 Hive提供了类似与SQL的查询语言——HiveQL，可以通过HQL实现简单的MR统计，Hive将HQL语句转换成MR任务进行执行。 Hive下载 Hive与Hadoop对应关系 截止当前（2019-05-25），Hive最新版本有三种：hive-1.2.2、hive-2.3.5、hive-3.1.1。 Hive官网下载页面说明，hive-2.3.5对应Hadoop版本是2.x.y，hive-3.1.1对应Hadoop版本是3.x.y。 本人安装Hadoop版本是2.8.4，故下载hive-2.3.5。 Hive下载地址 下载地址：http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz WZB-MacBook:50_bigdata wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata WZB-MacBook:50_bigdata wangzhibin$ wget http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz Hive安装配置 Hive安装 解压 WZB-MacBook:50_bigdata wangzhibin$ tar zxvf apache-hive-2.3.5-bin.tar.gz 配置.bash_profile WZB-MacBook:50_bigdata wangzhibin$ vi ~/.bash_profile WZB-MacBook:50_bigdata wangzhibin$ source ~/.bash_profile 增加如下配置： # hive export HIVE_HOME=/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin export PATH=$PATH:$HIVE_HOME/bin 验证是否安装成功 WZB-MacBook:50_bigdata wangzhibin$ hive --version Hive 2.3.5 Git git://HW13934/Users/gates/git/hive -r 76595628ae13b95162e77bba365fe4d2c60b3f29 Compiled by gates on Tue May 7 15:45:09 PDT 2019 From source with checksum c7864fc25abcb9cf7a36953ac6be4665 Hive配置 由于hive是默认将元数据保存在本地内嵌的 Derby 数据库中，但是这种做法缺点也很明显，Derby不支持多会话连接，因此本文将选择mysql作为元数据存储。 需要先安装Mysql，本文不做过多介绍，可以自行百度。 需要下载mysql的jdbc，然后将下载后的jdbc放到hive安装包的lib目录下。 WZB-MacBook:50_bigdata wangzhibin$ wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz WZB-MacBook:50_bigdata wangzhibin$ tar zxvf mysql-connector-java-5.1.47.tar.gz WZB-MacBook:50_bigdata wangzhibin$ cd mysql-connector-java-5.1.47 WZB-MacBook:mysql-connector-java-5.1.47 wangzhibin$ cp mysql-connector-java-5.1.47-bin.jar $HIVE_HOME/lib/ 修改配置hive-site.xml WZB-MacBook:~ wangzhibin$ cd $HIVE_HOME/conf WZB-MacBook:conf wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/conf WZB-MacBook:conf wangzhibin$ cp hive-default.xml.template hive-site.xml WZB-MacBook:conf wangzhibin$ vim hive-site.xml 配置文件如下： javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword mysql javax.jdo.option.ConnectionURL jdbc:mysql://localhost:3306/hive javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver 在mysql中初始化hive的schema（在此之前需要创建mysql下的hive数据库） WZB-MacBook:conf wangzhibin$ cd $HIVE_HOME/bin WZB-MacBook:bin wangzhibin$ schematool -dbType mysql -initSchema hive库中会初始化一些模型表： 到此配置完毕。HDFS中并未初始化数据仓库位置。 Hive使用 创建一个hive测试库 WZB-MacBook:bin wangzhibin$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/lib/hive-common-2.3.5.jar!/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases. hive> hive> create database hive_1; OK Time taken: 4.089 seconds hive> show databases; OK default hive_1 Time taken: 0.123 seconds, Fetched: 2 row(s) hive> 看看HDFS目录发生了什么变化 WZB-MacBook:conf wangzhibin$ hadoop fs -ls -R /user drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse/hive_1.db drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 15:53 /user/wangzhibin 看看mysql下的hive库有什么变化 mysql> use hive; mysql> select * from DBS; +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ | DB_ID | DESC | DB_LOCATION_URI | NAME | OWNER_NAME | OWNER_TYPE | +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ | 1 | Default Hive database | hdfs://localhost:9000/user/hive/warehouse | default | public | ROLE | | 2 | NULL | hdfs://localhost:9000/user/hive/warehouse/hive_1.db | hive_1 | wangzhibin | USER | +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ 2 rows in set (0.00 sec) 创建一个hive测试表 hive> use hive_1; OK Time taken: 3.772 seconds hive> create table hive_01 (id int,name string); OK Time taken: 0.582 seconds hive> show tables; OK hive_01 Time taken: 0.087 seconds, Fetched: 1 row(s) hive> 看看HDFS目录发生了什么变化 WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /user drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:28 /user/hive/warehouse/hive_1.db drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:28 /user/hive/warehouse/hive_1.db/hive_01 drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 15:53 /user/wangzhibin 看看mysql下的hive库有什么变化 mysql> select * from TBLS; +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ | TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER | RETENTION | SD_ID | TBL_NAME | TBL_TYPE | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT | IS_REWRITE_ENABLED | +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ | 1 | 1558780134 | 2 | 0 | wangzhibin | 0 | 1 | hive_01 | MANAGED_TABLE | NULL | NULL | | +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ 1 row in set (0.00 sec) 看一下web上有什么变化。 以上就是hive的简单使用，说白了，hive与mysql的使用差不多；对应于hdfs，hive_1库是hdfs中的一个目录，hive_01表也是一个目录。 参考资料 Hive基础知识介绍 Hive详细介绍及简单应用 hive简介 Hive安装与配置详解 Hive官方文档 Getting Started Guide on the Hive wiki ChangeLog 20190611 | 增加Hive官方文档链接 20190525 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-13 09:52:35 "},"20190527_implementation-of-wordcount-by-hive.html":{"url":"20190527_implementation-of-wordcount-by-hive.html","title":"Hive实现wordcount词频统计","keywords":"","body":"2019-05-27 | Hive实现wordcount词频统计 2019-05-27 | 大数据学习之路07 新建测试文件 WZB-MacBook:tmp wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata/tmp WZB-MacBook:tmp wangzhibin$ vi test.txt 增加内容： hello man what are you doing now my running hello kevin hi man 文件导入到hive 建表并指定文件内容分隔符 hive> use hive_1; OK Time taken: 0.024 seconds hive> create table wc(txt String) row format delimited fields terminated by '\\t'; OK Time taken: 0.715 seconds hive> show tables; OK hive_01 wc Time taken: 0.035 seconds, Fetched: 2 row(s) 导入文件 HDFS初始无数据 WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /user/hive/warehouse/hive_1.db/wc 返回无数据 导入文件 hive> load data local inpath '/Users/wangzhibin/00_dev_suite/50_bigdata/tmp/test.txt' overwrite into table wc; Loading data to table hive_1.wc OK Time taken: 2.235 seconds hive> select * from wc; OK hello man what are you doing now my running hello kevin hi man Time taken: 1.602 seconds, Fetched: 6 row(s) HDFS文件内容 WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /user/hive/warehouse/hive_1.db/wc -rwxr-xr-x 1 wangzhibin supergroup 63 2019-05-27 21:09 /user/hive/warehouse/hive_1.db/wc/test.txt WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -cat /user/hive/warehouse/hive_1.db/wc/test.txt hello man what are you doing now my running hello kevin hi man 使用HQL统计单词 hive> select split(txt,' ') from wc; OK [\"hello\",\"man\"] [\"what\",\"are\",\"you\",\"doing\",\"now\"] [\"my\",\"running\"] [\"hello\"] [\"kevin\"] [\"hi\",\"man\"] Time taken: 0.337 seconds, Fetched: 6 row(s) hive> select explode(split(txt,' ')) from wc; OK hello man what are you doing now my running hello kevin hi man Time taken: 0.094 seconds, Fetched: 13 row(s) hive> select t1.word,count(t1.word) from (select explode(split(txt,' '))word from wc)t1 group by t1.word; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases. Query ID = wangzhibin_20190527214319_1532be66-b6e5-4603-83a9-dc4c7d6ec466 Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks not specified. Defaulting to jobconf value of: 1 In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer= In order to limit the maximum number of reducers: set hive.exec.reducers.max= In order to set a constant number of reducers: set mapreduce.job.reduces= Starting Job = job_1558964487152_0004, Tracking URL = http://WZB-MacBook.local:8088/proxy/application_1558964487152_0004/ Kill Command = /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/bin/hadoop job -kill job_1558964487152_0004 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1 2019-05-27 21:43:25,976 Stage-1 map = 0%, reduce = 0% 2019-05-27 21:43:31,173 Stage-1 map = 100%, reduce = 0% 2019-05-27 21:43:36,365 Stage-1 map = 100%, reduce = 100% Ended Job = job_1558964487152_0004 MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 HDFS Read: 8923 HDFS Write: 294 SUCCESS Total MapReduce CPU Time Spent: 0 msec OK are 1 doing 1 hello 2 hi 1 kevin 1 man 2 my 1 now 1 running 1 what 1 you 1 Time taken: 17.562 seconds, Fetched: 11 row(s) 小插曲：[执行结果未出来的原因是：没有启动yarn] 总结 split--------------------------列变数组 explode------------------------数组拆分成多行 group by和count----------------对行分组后求各行出现的次数 参考资料： Hive实现wordcount词频统计 ChangeLog 20190527 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:09:01 "},"20190529_architecture-and-principle-of-hive.html":{"url":"20190529_architecture-and-principle-of-hive.html","title":"Hive原理详解","keywords":"","body":"2019-05-29 | Hive原理详解 2019-05-29 | 大数据学习之路08 Hive概述 数据仓库的概念 首先要来看一下数据库与数据仓库的区别。 数据库：传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。 数据仓库：数据仓库系统的主要应用主要是OLAP（On-Line Analytical Processing），支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 主要区别如下： 数据库偏重数据的业务处理（transaction），属于OLTP（Online transaction processing）层面，后者着重于分析，可能会重点面向某个行业，属于OLAP（Online analytical processing）层面。 数据库一般叫“业务型数据库”，数据仓库被称为“分析型数据库”。数据库常采用行式存储，而数据仓库常采用列式存储，数据结构有利于查询和分析。 前者的用户数量大（主要是业务人员），既要执行“读”操作也要执行“写”操作，每次写的量不大，但是对时间敏感。后者的用户数量小（主要是决策人员），一般只需要执行读操作，每次读取的数据量很大，对反应时间不那么敏感。 把所需要的数据从业务型数据库导入分析型数据仓库的过程，称为ETL（Extract-Transform-Load，“抽取-转换-加载”）。 数据库用到的工具主要有MySQL, Oracle, MS SQLServer等，数据仓库用到的工具主要有Hive, AWSRedshift, Green Plum, SAP HANA等。 参考： 数据库 与 数据仓库的本质区别是什么？ \"数据库\" vs. \"数据仓库\": 区别与联系 Hive简介 Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取、转化、加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。 Hive是一个构建于Hadoop顶层的数据仓库工具，可以查询和管理PB级别的分布式数据。 支持大规模数据存储、分析，具有良好的可扩展性 某种程度上可以看作是用户编程接口，本身不存储和处理数据。 依赖分布式文件系统HDFS存储数据。 依赖分布式并行计算模型MapReduce处理数据。 定义了简单的类似SQL 的查询语言——HiveQL。 用户可以通过编写的HiveQL语句运行MapReduce任务。 可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到Hadoop平台上。 是一个可以提供有效、合理、直观组织和使用数据的分析工具。 参考：Hive技术原理解析 Hive与传统数据库的区别 Hive适用场景 Hive不适用于大规模数据集实现低延迟快速查询。 Hive 构建在基于静态批处理的Hadoop 之上，Hadoop 通常都有较高的延迟并且在作业提交和调度的时候需要大量的开销。因此，Hive 并不能够在大规模数据集上实现低延迟快速的查询，例如，Hive 在几百MB 的数据集上执行查询一般有分钟级的时间延迟。 Hive 并不适合那些需要低延迟的应用，例如，联机事务处理（OLTP）。Hive 查询操作过程严格遵守Hadoop MapReduce 的作业执行模型，Hive 将用户的HiveQL 语句通过解释器转换为MapReduce 作业提交到Hadoop 集群上，Hadoop 监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，Hive 并不提供实时的查询和基于行级的数据更新操作。Hive 的最佳使用场合是大数据集的批处理作业，例如，网络日志分析。 Hive适用于以下场景： 数据挖掘：用户行为分析；兴趣分区；区域展示； 非实时分析：日志分析；文本分析。 数据汇总：每天/每周用户点击数，流量统计。 数据仓库：数据抽取，加载，转换（ETL）。 Hive功能与架构 Hive与Hadoop Hive的执行入口是Driver，执行的SQL语句首先提交到Drive驱动，然后调用compiler解释驱动，最终解释成MapReduce任务去执行。 Hive与Hadoop生态系统中其他组件的关系 Hive依赖于HDFS 存储数据 Hive依赖于MapReduce 处理数据 在某些场景下Pig可以作为Hive的替代工具 HBase 提供数据的实时访问 Hive系统架构 （图片来源：Design - Apache Hive） Hive的工作原理简单的说就是一个查询引擎，接收到一个SQL，后续工作包括： 词法分析/语法分析 使用antlr将SQL语句解析成抽象语法树(AST) 语义分析 从Metastore获取模式信息，验证SQL语句中队表名，列名，以及数据类型的检查和隐式转换，以及Hive提供的函数和用户自定义的函数(UDF/UAF) 逻辑计划生成 生成逻辑计划--算子树 逻辑计划优化 对算子树进行优化，包括列剪枝，分区剪枝，谓词下推等 物理计划生成 将逻辑计划生成包含由MapReduce任务组成的DAG的物理计划 物理计划执行 将DAG发送到Hadoop集群进行执行 （图片来源：http://infolab.stanford.edu/~ragho/hive-icde2010.pdf ） Hive的数据模型 Hive的数据存储在HDFS上，基本存储单位是表或者分区，Hive内部把表或者分区称作SD，即Storage Descriptor。一个SD通常是一个HDFS路径，或者其它文件系统路径。SD的元数据信息存储在Hive MetaStore中，如文件路径，文件格式，列，数据类型，分隔符。Hive默认的分格符有三种，分别是\\^A、\\^B和\\^C，即ASCii码的1、2和3，分别用于分隔列，分隔列中的数组元素，和元素Key-Value对中的Key和Value。 分区：数据表可以按照某个字段的值划分分区。 每个分区是一个目录。 分区数量不固定。 分区下可再有分区或者桶。 桶：数据可以根据桶的方式将不同数据放入不同的桶中。 每个桶是一个文件。 建表时指定桶个数，桶内可排序。 数据按照某个字段的值Hash后放入某个桶中。 Hive可以创建托管表和外部表： 默认创建托管表，Hiva会将数据移动到数据仓库的目录。 创建外部表，这时Hiva会到仓库目录以外的位置访问数据。 如果所有处理都由Hive完成，建议使用托管表。 如果要用Hive和其他工具来处理同一个数据集，建议使用外部表。 Hive工作原理 SQL语句转换成MapReduce作业的基本原理 join的实现原理 group by实现原理 Hive中SQL查询转换成MapReduce作用的过程 当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作： 驱动模块接收该命令或查询编译器 对该命令或查询进行解析编译 由优化器对该命令或查询进行优化计算 该命令或查询通过执行器进行执行 第1步：由Hive驱动模块中的编译器对用户输入的SQL语言进行词法和语法解析，将SQL语句转化为抽象语法树的形式。 第2步：抽象语法树的结构仍很复杂，不方便直接翻译为MapReduce算法程序，因此，把抽象语法书转化为查询块。 第3步：把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符。 第4步：重写逻辑查询计划，进行优化，合并多余操作，减少MapReduce任务数量。 第5步：将逻辑操作符转换成需要执行的具体MapReduce任务。 第6步：对生成的MapReduce任务进行优化，生成最终的MapReduce任务执行计划。 第7步：由Hive驱动模块中的执行器，对最终的MapReduce任务进行执行输出。 几点说明： 当启动MapReduce程序时，Hive本身是不会生成MapReduce算法程序的。 需要通过一个表示“Job执行计划”的XML文件驱动执行内置的、原生的Mapper和Reducer模块。 Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在JobTracker所在的管理节点上执行。 通常在大型集群上，会有专门的网关机来部署Hive工具。网关机的作用主要是远程操作和管理节点上的JobTracker通信来执行任务。 数据文件通常存储在HDFS上，HDFS由名称节点管理。 Hive 的数据类型 Hive 的数据存储支持 HDFS 的一些文件格式，比如 CSV, Sequence File, Avro, RC File, ORC, Parquet。也支持访问 HBase。 Hive 支持原子和复杂数据类型，原子数据类型包括：数据值、布尔类型、字符串类型等，复杂的类型包括：Array、Map和Struct。其中Array和Map和java中的Array和Map是相似的，Struct和C语言中的Struct相似。 Create table test( col1 Array, col2 Map, col3 Struct ); 参考资料 Design-Apache Hive Hive原理及查询优化 Hive技术原理 Hive技术原理解析 Hive 工作原理详解 如何通俗地理解Hive的工作原理？ ChangeLog 20190609 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 17:01:55 "},"20190609_commands-of-hive.html":{"url":"20190609_commands-of-hive.html","title":"Hive常用命令","keywords":"","body":"Hive 基本命令 连接 Hive shell 连接 Hive WZB-MacBook:hadoop wangzhibin$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/Cellar_w/raw/apache-hive-2.3.5-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/Cellar_w/raw/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in jar:file:/usr/local/Cellar_w/raw/apache-hive-2.3.5-bin/lib/hive-common-2.3.5.jar!/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases. hive> 退出 Hive hive> quit; hive> quit; Mon Jun 10 11:21:55 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:55 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:55 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:55 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:56 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:56 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:56 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:56 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. WZB-MacBook:hadoop wangzhibin$ 注意：hive > exit; 会影响之前的使用，所以需要下一句 kill 掉 hadoop 的进程：hine > hadoop job -kill [jobid] 数据库相关 创建数据库 hive> create database test_01; OK Time taken: 5.708 seconds 如果数据库已经存在就会抛出一个错误信息，使用如下语句可以避免抛出错误信息： hive> create database test_01; FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database test_01 already exists hive> create database if not exists test_01; OK Time taken: 0.012 seconds 查看数据库 hive> show databases; OK default hive_1 test_01 Time taken: 0.146 seconds, Fetched: 3 row(s) 如果数据库比较多的话，也可以用正则表达式来查看： hive> show databases like 'h.*'; OK hive_1 Time taken: 0.228 seconds, Fetched: 1 row(s) 使用数据库 hive> use test_01; OK Time taken: 0.027 seconds 查看数据库的描述及路径 hive> describe database test_01; OK test_01 hdfs://localhost:9900/user/hive/warehouse/test_01.db wangzhibin USER Time taken: 0.017 seconds, Fetched: 1 row(s) 修改数据库的路径（没有尝试） hive> creat database database_name location '路径'; 删除库 hive> drop database if exists database_name; --删除空的数据库 hive> drop database if exists database_name cascade; --先删除数据库中的表再删除数据库； hive> show databases; OK default hive_1 test_01 Time taken: 0.299 seconds, Fetched: 3 row(s) hive> drop database if exists test_01 cascade; OK Time taken: 0.387 seconds hive> show databases; OK default hive_1 Time taken: 0.016 seconds, Fetched: 2 row(s) 表相关 创建表 hive> create table test(key string); OK Time taken: 0.875 seconds 创建一个新表，结构与其他一样 hive> create table test_01 like test; OK Time taken: 0.394 seconds 分区表与数据加载 创建分区表 hive> create table logs(ts bigint,line string) partitioned by (dt String,country String) row format delimited fields terminated by ','; OK Time taken: 0.073 seconds 数据加载 load data local inpath '/home/wangzb/file1' into table logs partition (dt='2001-01-01',country='GB'); 数据加载的两种方式 hive>load data inpath '/root/inner_table.dat' into table t1; 移动hdfs中数据到t1表中 hive>load data local inpath '/root/inner_table.dat' into table t1;上传本地数据到hdfs中 有 local 的速度明显比没有 local 慢。 说明： 首先，创建分区表的时候，需要通过关键字 partitioned by (dt String,country String) 声明该表是分区表，并且是按照字段 dt、country 进行分区的； 其次，向分区表导入数据的时候，要通过关键字 partition (dt='2001-01-01',country='GB') 显示声明数据要导入到表的哪个分区中。 所谓分区，这是将满足某些条件的记录打包，做个记号，在查询时提高效率，相当于按文件夹对文件进行分类，文件夹名可类比分区字段。 这个分区字段形式上存在于数据表中，在查询时会显示到客户端上，但并不真正在存储在数据表文件中，是所谓伪列。所以，千万不要以为是对属性表中真正存在的列按照属性值的异同进行分区。比如上面的分区依据的列 dt、country 并不真正的存在于数据表中，是我们为了方便管理添加的一个伪列，这个列的值也是我们人为规定的，不是从数据表中读取之后根据值的不同将其分区。 我们并不能按照某个数据表中真实存在的列，如 ts 来分区。 可参见示例：Hive实现wordcount词频统计 命令：创建分桶 create table t_bluk(id string, name string) clustered by(id) sort by (id) into 4 buckets; 解析：clustered by(id) 意思是根据id分成4个桶，并且桶内按照id排序。 上述命令执行后，将会在相应的hdfs文件目录下创建四个子目录，如： 可能遇到的问题：当使用命令“insert into t_buck select * from other”时，出现t_buck目录下并没有四个子目录，只有一个子目录，需要如下操作： 设置如下变量： #设置变量, 设置分桶为true, 设置reduce数量是分桶的数量个数 set hive.enforce.bucketing = true; set mapreduce.job.reduces=4; 使用“insert ... select ...”命令向t_buck插入数据才会最终生成四个分区。 额外说明：insert into t_buck select id,name from t_p distribute by (id) sort by (id);。distribute by (id)指定分区字段； sort by (id) 指定排序字段。当排序和分桶的字段相同的时候可以使用 distribute by (sno) sort by (sno asc) 或Cluster by(字段)。cluster by等同于分桶+排序(sort) 分区和分桶的区别： 分区依据的是伪列，分桶则是相对分区进行更细粒度的划分。 分桶将整个数据内容按照某列属性值的hash值进行区分，如要按照name属性分为3个桶，就是对name属性值的hash值对3取摸，按照取模结果对数据分桶。如取模结果为0的数据记录存放到一个文件，取模为1的数据存放到一个文件，取模为2的数据存放到一个文件。 与分区不同的是，分区依据的不是真实数据表文件中的列，而是我们指定的伪列，但是分桶是依据数据表中真实的列而不是伪列。所以在指定分区依据的列的时候要指定列的类型，因为在数据表文件中不存在这个列，相当于新建一个列。而分桶依据的是表中已经存在的列，这个列的数据类型显然是已知的，所以不需要指定列的类型。 参考：Hive基本命令解析 查看所有表 hive> show tables; OK logs test test_01 Time taken: 0.304 seconds, Fetched: 3 row(s) 查看表的结构与路径 hive> describe test; OK key string Time taken: 0.048 seconds, Fetched: 1 row(s) hive> describe logs; OK ts bigint line string dt string country string # Partition Information # col_name data_type comment dt string country string Time taken: 0.11 seconds, Fetched: 10 row(s) 更新表名称 hive> alter table test rename to test_02; OK Time taken: 0.555 seconds hive> show tables; OK logs test_01 test_02 Time taken: 0.024 seconds, Fetched: 3 row(s) 添加新一列 hive> describe test_01; OK key string Time taken: 0.059 seconds, Fetched: 1 row(s) hive> alter table test_01 add columns(value string comment 'a comment'); OK Time taken: 0.097 seconds hive> describe test_01; OK key string value string a comment Time taken: 0.046 seconds, Fetched: 2 row(s) 删除表 hive> drop table test_01; --或者 drop table if exists test_01; OK Time taken: 0.429 seconds hive> show tables; OK logs test_02 Time taken: 0.022 seconds, Fetched: 2 row(s) 删除表，保留结构 hive> truncate table tableName; Hive其他命令 查询当前linux文件夹下的文件 hive> !ls /usr; bin lib libexec local sbin share standalone 查询当前hdfs文件系统下的文件 hive> dfs -ls -R /user; drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive drwxr-xr-x - wangzhibin supergroup 0 2019-06-10 15:10 /user/hive/warehouse drwxr-xr-x - wangzhibin supergroup 0 2019-05-27 21:06 /user/hive/warehouse/hive_1.db drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:28 /user/hive/warehouse/hive_1.db/hive_01 drwxr-xr-x - wangzhibin supergroup 0 2019-05-27 21:09 /user/hive/warehouse/hive_1.db/wc -rwxr-xr-x 1 wangzhibin supergroup 63 2019-05-27 21:09 /user/hive/warehouse/hive_1.db/wc/test.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 15:53 /user/wangzhibin 高级命令 加载本地数据 Hive不支持行级插入操作、更新操作和删除操作，也不支持事务，那么往表里面装数据的唯一的途径就是使用一种「大量」的数据装载操作，或者仅仅将文件写入到正确的目录下面，即以load的方式加载到建立好的表中，且数据一旦导入就不可以修改，加载的目标可以是一个表，或者分区，如果表包含分区，必须指定每一个分区名。 使用overwrite关键字，加载本地数据，同时给分区信息： hive>load data local inpath '${env:HOME}/目录' overwrite into table table_name partition (ds='2018-05-05')； 目标表（或者分区）中的内容（如果有）会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中，如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 将查询结果插入hive表 hive> insert overwrite table tab_name [partition(par1=val1,par2=val2)] select_statement1 from from_statement; hive> insert into table tab_name [partition(par1=val1,par2=val2)] select_statement1 from from_statement; 将查询结果写入HDFS文件系统 hive> insert overwrite [local] directory directory1 select ... from ...; 参考资料 Hive基本命令解析 Hive shell 基本命令 Hive入门及常用指令 ChangeLog 20190610 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-10 15:38:01 "},"20190611_installation-and-use-of-sqoop.html":{"url":"20190611_installation-and-use-of-sqoop.html","title":"Sqoop安装与使用","keywords":"","body":"Sqoop安装与使用 Sqoop简介 Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。 导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统； 导出数据：从Hadoop的文件系统中导出数据到关系数据库 Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。 本文针对的是Sqoop1，不涉及到Sqoop2，两者有很大区别。 Sqoop下载与安装 下载Sqoop 下载页面下有两个链接，使用sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz，包含hadoop支持。不要用sqoop-1.4.7.tar.gz。 WZB-MacBook:raw wangzhibin$ wget http://mirror.bit.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 解压Sqoop安装文件 WZB-MacBook:raw wangzhibin$ tar zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz WZB-MacBook:Cellar_w wangzhibin$ cd .. WZB-MacBook:Cellar_w wangzhibin$ ln -s raw/sqoop-1.4.7.bin__hadoop-2.6.0 sqoop WZB-MacBook:Cellar_w wangzhibin$ ls -l lrwxr-xr-x 1 wangzhibin wheel 33 6 11 14:53 sqoop -> raw/sqoop-1.4.7.bin__hadoop-2.6.0 配置环境变量 # sqoop export SQOOP_HOME=/usr/local/Cellar_w/sqoop export PATH=$PATH:$SQOOP_HOME/bin Sqoop 配置文件修改。 进入 $SQOOP_HOME/conf 目录下，将 sqoop-env-template.sh 复制一份，并取名为 sqoop-env.sh WZB-MacBook:sqoop wangzhibin$ cd $SQOOP_HOME/conf WZB-MacBook:conf wangzhibin$ cp sqoop-env-template.sh sqoop-env.sh WZB-MacBook:conf wangzhibin$ ls oraoop-site-template.xml sqoop-env-template.cmd sqoop-env-template.sh sqoop-env.sh sqoop-site-template.xml sqoop-site.xml WZB-MacBook:conf wangzhibin$ 在 sqoop-env.sh 文件末尾加入配置： export HADOOP_COMMON_HOME=/usr/local/Cellar_w/hadoop export HADOOP_MAPRED_HOME=/usr/local/Cellar_w/hadoop export HIVE_HOME=/usr/local/Cellar_w/hive # export HBASE_HOME=/home/hadoop/hbase-1.2.2 MySQL驱动包 把 MySQL 的驱动包上传到 Sqoop 的 lib 目录下。之前在 Hive 安装过程中用到过 MySQL 的驱动包，直接使用。 WZB-MacBook:lib wangzhibin$ cp /usr/local/Cellar_w/hive/lib/mysql-connector-java-5.1.47-bin.jar . Sqoop使用 Sqoop Help WZB-MacBook:sqoop wangzhibin$ sqoop help Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/11 15:36:18 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 usage: sqoop COMMAND [ARGS] Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version information See 'sqoop help COMMAND' for information on a specific command. WZB-MacBook:sqoop wangzhibin$ 查看 MySQL 中的数据库。 执行如下命令，连接mysql看有多少个数据库。 WZB-MacBook:sqoop wangzhibin$ sqoop list-databases --connect jdbc:mysql://localhost:3306?characterEncoding=UTF-8 --username root --password 'mysql' Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/11 15:39:34 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 19/06/11 15:39:34 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/11 15:39:34 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. Tue Jun 11 15:39:34 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. information_schema hive my_summary_v1.0 mysql performance_schema sys test_hdfs_import 查看 MySQL 有多少个表 WZB-MacBook:sqoop wangzhibin$ sqoop list-tables --connect jdbc:mysql://localhost:3306/test_hdfs_import?characterEncoding=UTF-8 --username root --password 'mysql' Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/11 15:50:36 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 19/06/11 15:50:36 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/11 15:50:36 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. Tue Jun 11 15:50:36 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. role user 把 MySQL 中的表导入 HDFS 中 前提：一定要启动hdfs和yarn。 MySQL数据库user ​ 导入HDFS。 WZB-MacBook:sqoop wangzhibin$ sqoop import -m 1 --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password mysql --table user --target-dir /user/sqoop/test_mysql_import Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/11 15:55:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 19/06/11 15:55:16 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/11 15:55:17 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 19/06/11 15:55:17 INFO tool.CodeGenTool: Beginning code generation Tue Jun 11 15:55:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/11 15:55:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/11 15:55:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/11 15:55:17 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/Cellar_w/hadoop 注: /tmp/sqoop-wangzhibin/compile/8fa026c1f508000a44160a00979d17d3/user.java使用或覆盖了已过时的 API。 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。 19/06/11 15:55:19 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-wangzhibin/compile/8fa026c1f508000a44160a00979d17d3/user.jar 19/06/11 15:55:19 WARN manager.MySQLManager: It looks like you are importing from mysql. 19/06/11 15:55:19 WARN manager.MySQLManager: This transfer can be faster! Use the --direct 19/06/11 15:55:19 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path. 19/06/11 15:55:19 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql) 19/06/11 15:55:19 INFO mapreduce.ImportJobBase: Beginning import of user 19/06/11 15:55:19 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 19/06/11 15:55:20 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 19/06/11 15:55:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Tue Jun 11 15:55:23 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/11 15:55:23 INFO db.DBInputFormat: Using read commited transaction isolation 19/06/11 15:55:24 INFO mapreduce.JobSubmitter: number of splits:1 19/06/11 15:55:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1560136240717_0001 19/06/11 15:55:25 INFO impl.YarnClientImpl: Submitted application application_1560136240717_0001 19/06/11 15:55:25 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1560136240717_0001/ 19/06/11 15:55:25 INFO mapreduce.Job: Running job: job_1560136240717_0001 19/06/11 15:55:37 INFO mapreduce.Job: Job job_1560136240717_0001 running in uber mode : false 19/06/11 15:55:37 INFO mapreduce.Job: map 0% reduce 0% 19/06/11 15:55:42 INFO mapreduce.Job: map 100% reduce 0% 19/06/11 15:55:43 INFO mapreduce.Job: Job job_1560136240717_0001 completed successfully 19/06/11 15:55:43 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=178561 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=87 HDFS: Number of bytes written=48 HDFS: Number of read operations=4 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Other local map tasks=1 Total time spent by all maps in occupied slots (ms)=2995 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=2995 Total vcore-milliseconds taken by all map tasks=2995 Total megabyte-milliseconds taken by all map tasks=3066880 Map-Reduce Framework Map input records=2 Map output records=2 Input split bytes=87 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=21 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=201326592 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=48 19/06/11 15:55:43 INFO mapreduce.ImportJobBase: Transferred 48 bytes in 23.6248 seconds (2.0318 bytes/sec) 19/06/11 15:55:43 INFO mapreduce.ImportJobBase: Retrieved 2 records. 导入结果 WZB-MacBook:sqoop wangzhibin$ hadoop fs -ls -R /user/sqoop drwxr-xr-x - wangzhibin supergroup 0 2019-06-11 15:55 /user/sqoop drwxr-xr-x - wangzhibin supergroup 0 2019-06-11 15:55 /user/sqoop/test_mysql_import -rw-r--r-- 1 wangzhibin supergroup 0 2019-06-11 15:55 /user/sqoop/test_mysql_import/_SUCCESS -rw-r--r-- 1 wangzhibin supergroup 48 2019-06-11 15:55 /user/sqoop/test_mysql_import/part-m-00000 WZB-MacBook:sqoop wangzhibin$ hadoop fs -cat /user/sqoop/test_mysql_import/part-m-00000 1,wangzhibin,wangzhibin 2,testuser,测试用户 将 MySQL 中的数据表导入到Hive中 创建 Hive 表 test_user 字段与MySQL中一致。 use hive_1; drop table if exists hive_1.test_user ; create table hive_1.test_user( id int, name string, desc string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' ; 导入关系表到HIVE sqoop import \\ --connect jdbc:mysql://localhost:3306/test_hdfs_import \\ --username root \\ --password mysql \\ --table user \\ --fields-terminated-by '\\t' \\ --delete-target-dir \\ --hive-import \\ --hive-database hive_1 \\ --hive-table test_user \\ -m 1 执行结果： WZB-MacBook:lib wangzhibin$ sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password mysql --table user --fields-terminated-by '\\t' --delete-target-dir --hive-import --hive-database hive_1 --hive-table test_user -m 1 Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/13 10:46:50 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/Cellar_w/raw/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/Cellar_w/raw/apache-hive-2.3.5-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 19/06/13 10:46:50 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/13 10:46:50 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 19/06/13 10:46:50 INFO tool.CodeGenTool: Beginning code generation Thu Jun 13 10:46:50 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:46:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 10:46:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 10:46:50 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/Cellar_w/hadoop 注: /tmp/sqoop-wangzhibin/compile/93f56e3a0bd9ae4384d9c3fc01ef8e1b/user.java使用或覆盖了已过时的 API。 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。 19/06/13 10:46:53 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-wangzhibin/compile/93f56e3a0bd9ae4384d9c3fc01ef8e1b/user.jar 19/06/13 10:46:54 INFO tool.ImportTool: Destination directory user deleted. 19/06/13 10:46:54 WARN manager.MySQLManager: It looks like you are importing from mysql. 19/06/13 10:46:54 WARN manager.MySQLManager: This transfer can be faster! Use the --direct 19/06/13 10:46:54 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path. 19/06/13 10:46:54 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql) 19/06/13 10:46:54 INFO mapreduce.ImportJobBase: Beginning import of user 19/06/13 10:46:54 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 19/06/13 10:46:54 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 19/06/13 10:46:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Thu Jun 13 10:46:57 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:46:57 INFO db.DBInputFormat: Using read commited transaction isolation 19/06/13 10:46:57 INFO mapreduce.JobSubmitter: number of splits:1 19/06/13 10:46:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1560136240717_0009 19/06/13 10:46:57 INFO impl.YarnClientImpl: Submitted application application_1560136240717_0009 19/06/13 10:46:57 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1560136240717_0009/ 19/06/13 10:46:57 INFO mapreduce.Job: Running job: job_1560136240717_0009 19/06/13 10:47:05 INFO mapreduce.Job: Job job_1560136240717_0009 running in uber mode : false 19/06/13 10:47:05 INFO mapreduce.Job: map 0% reduce 0% 19/06/13 10:47:10 INFO mapreduce.Job: map 100% reduce 0% 19/06/13 10:47:11 INFO mapreduce.Job: Job job_1560136240717_0009 completed successfully 19/06/13 10:47:12 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=183612 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=87 HDFS: Number of bytes written=48 HDFS: Number of read operations=4 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Other local map tasks=1 Total time spent by all maps in occupied slots (ms)=3032 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=3032 Total vcore-milliseconds taken by all map tasks=3032 Total megabyte-milliseconds taken by all map tasks=3104768 Map-Reduce Framework Map input records=2 Map output records=2 Input split bytes=87 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=26 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=201326592 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=48 19/06/13 10:47:12 INFO mapreduce.ImportJobBase: Transferred 48 bytes in 17.5847 seconds (2.7296 bytes/sec) 19/06/13 10:47:12 INFO mapreduce.ImportJobBase: Retrieved 2 records. 19/06/13 10:47:12 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user Thu Jun 13 10:47:12 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:47:12 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 10:47:12 INFO hive.HiveImport: Loading uploaded data into Hive 19/06/13 10:47:12 INFO conf.HiveConf: Found configuration file file:/usr/local/Cellar_w/hive/conf/hive-site.xml Logging initialized using configuration in jar:file:/usr/local/Cellar_w/raw/sqoop-1.4.7.bin__hadoop-2.6.0/lib/hive-exec-2.3.5.jar!/hive-log4j2.properties Async: true 19/06/13 10:47:14 INFO SessionState: Logging initialized using configuration in jar:file:/usr/local/Cellar_w/raw/sqoop-1.4.7.bin__hadoop-2.6.0/lib/hive-exec-2.3.5.jar!/hive-log4j2.properties Async: true 19/06/13 10:47:15 INFO session.SessionState: Created HDFS directory: /tmp/hive/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:15 INFO session.SessionState: Created local directory: /var/folders/gg/35tlzsrs1kj3c460vh9tvvv40000gn/T/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:15 INFO session.SessionState: Created HDFS directory: /tmp/hive/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36/_tmp_space.db 19/06/13 10:47:15 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:15 INFO session.SessionState: Updating thread name to 7041efcf-5e5a-4c48-a381-89bd445e7b36 main 19/06/13 10:47:15 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:15 INFO ql.Driver: Compiling command(queryId=wangzhibin_20190613104715_4762c8b4-54b9-4399-9ab7-acd0c1b732c1): CREATE TABLE IF NOT EXISTS `hive_1`.`test_user` ( `ID` INT, `USER_NAME` STRING, `USER_DESC` STRING) COMMENT 'Imported by sqoop on 2019/06/13 10:47:12' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\011' LINES TERMINATED BY '\\012' STORED AS TEXTFILE 19/06/13 10:47:17 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:17 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:17 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored 19/06/13 10:47:17 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored Thu Jun 13 10:47:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:47:18 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\" Thu Jun 13 10:47:20 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:20 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:20 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:20 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:47:22 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:22 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:22 INFO metastore.HiveMetaStore: Added admin role in metastore 19/06/13 10:47:22 INFO metastore.HiveMetaStore: Added public role in metastore 19/06/13 10:47:22 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty 19/06/13 10:47:22 INFO metastore.HiveMetaStore: 0: get_all_functions 19/06/13 10:47:22 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_all_functions 19/06/13 10:47:22 INFO parse.CalcitePlanner: Starting Semantic Analysis 19/06/13 10:47:22 INFO parse.CalcitePlanner: Creating table hive_1.test_user position=27 19/06/13 10:47:22 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:22 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO ql.Driver: Semantic Analysis Completed 19/06/13 10:47:23 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null) 19/06/13 10:47:23 INFO ql.Driver: Completed compiling command(queryId=wangzhibin_20190613104715_4762c8b4-54b9-4399-9ab7-acd0c1b732c1); Time taken: 7.893 seconds 19/06/13 10:47:23 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager 19/06/13 10:47:23 INFO ql.Driver: Executing command(queryId=wangzhibin_20190613104715_4762c8b4-54b9-4399-9ab7-acd0c1b732c1): CREATE TABLE IF NOT EXISTS `hive_1`.`test_user` ( `ID` INT, `USER_NAME` STRING, `USER_DESC` STRING) COMMENT 'Imported by sqoop on 2019/06/13 10:47:12' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\011' LINES TERMINATED BY '\\012' STORED AS TEXTFILE 19/06/13 10:47:23 INFO sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7041efcf-5e5a-4c48-a381-89bd445e7b36, clientType=HIVECLI] 19/06/13 10:47:23 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory. 19/06/13 10:47:23 INFO hive.metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore 19/06/13 10:47:23 INFO ql.Driver: Completed executing command(queryId=wangzhibin_20190613104715_4762c8b4-54b9-4399-9ab7-acd0c1b732c1); Time taken: 0.058 seconds OK 19/06/13 10:47:23 INFO ql.Driver: OK Time taken: 7.959 seconds 19/06/13 10:47:23 INFO CliDriver: Time taken: 7.959 seconds 19/06/13 10:47:23 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:23 INFO session.SessionState: Resetting thread name to main 19/06/13 10:47:23 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:23 INFO session.SessionState: Updating thread name to 7041efcf-5e5a-4c48-a381-89bd445e7b36 main 19/06/13 10:47:23 INFO ql.Driver: Compiling command(queryId=wangzhibin_20190613104723_983989d1-e1b0-42b5-b587-1c922b71ee1b): LOAD DATA INPATH 'hdfs://localhost:9900/user/wangzhibin/user' INTO TABLE `hive_1`.`test_user` 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:23 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:23 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:23 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO ql.Driver: Semantic Analysis Completed 19/06/13 10:47:23 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null) 19/06/13 10:47:23 INFO ql.Driver: Completed compiling command(queryId=wangzhibin_20190613104723_983989d1-e1b0-42b5-b587-1c922b71ee1b); Time taken: 0.374 seconds 19/06/13 10:47:23 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager 19/06/13 10:47:23 INFO ql.Driver: Executing command(queryId=wangzhibin_20190613104723_983989d1-e1b0-42b5-b587-1c922b71ee1b): LOAD DATA INPATH 'hdfs://localhost:9900/user/wangzhibin/user' INTO TABLE `hive_1`.`test_user` 19/06/13 10:47:23 INFO ql.Driver: Starting task [Stage-0:MOVE] in serial mode 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore Loading data to table hive_1.test_user 19/06/13 10:47:23 INFO exec.Task: Loading data to table hive_1.test_user from hdfs://localhost:9900/user/wangzhibin/user 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:23 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:23 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:23 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: alter_table: db=hive_1 tbl=test_user newtbl=test_user 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=alter_table: db=hive_1 tbl=test_user newtbl=test_user 19/06/13 10:47:24 INFO ql.Driver: Starting task [Stage-1:STATS] in serial mode 19/06/13 10:47:24 INFO exec.StatsTask: Executing stats task 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:24 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:24 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:24 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: alter_table: db=hive_1 tbl=test_user newtbl=test_user 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=alter_table: db=hive_1 tbl=test_user newtbl=test_user 19/06/13 10:47:24 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:24 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:24 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:24 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:24 INFO hive.log: Updating table stats fast for test_user 19/06/13 10:47:24 INFO hive.log: Updated size of table test_user to 96 19/06/13 10:47:24 INFO exec.StatsTask: Table hive_1.test_user stats: [numFiles=2, numRows=0, totalSize=96, rawDataSize=0] 19/06/13 10:47:24 INFO ql.Driver: Completed executing command(queryId=wangzhibin_20190613104723_983989d1-e1b0-42b5-b587-1c922b71ee1b); Time taken: 0.947 seconds OK 19/06/13 10:47:24 INFO ql.Driver: OK Time taken: 1.321 seconds 19/06/13 10:47:24 INFO CliDriver: Time taken: 1.321 seconds 19/06/13 10:47:24 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:24 INFO session.SessionState: Resetting thread name to main 19/06/13 10:47:24 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:24 INFO session.SessionState: Deleted directory: /tmp/hive/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36 on fs with scheme hdfs 19/06/13 10:47:24 INFO session.SessionState: Deleted directory: /var/folders/gg/35tlzsrs1kj3c460vh9tvvv40000gn/T/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36 on fs with scheme file 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO hive.HiveImport: Hive import complete. 19/06/13 10:47:24 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory. 查看Hive表结果 WZB-MacBook:security wangzhibin$ hive hive> use hive_1; OK Time taken: 4.181 seconds hive> show tables; OK hive_01 test_user wc Time taken: 0.143 seconds, Fetched: 3 row(s) hive> select * from test_user; OK 1 wangzhibin wangzhibin 2 testuser 测试用户 Time taken: 1.594 seconds, Fetched: 2 row(s) 导入 Hive 的过程 现将MySQL数据导入到HDFS的 /user/wangzhibin/[tablename] 中 再从 /user/wangzhibin/[tablename] 通过load加载到Hive制定数据表中。 删除 /user/wangzhibin/[tablename] 将 HDFS 上的数据导出到数据库中 HDFS文件 WZB-MacBook:sqoop wangzhibin$ hadoop fs -cat /user/sqoop/test_mysql_export/file.txt 10 test_wzb 20 test_wzb1 21 test_wzb2 Sqoop导出 WZB-MacBook:sqoop wangzhibin$ sqoop export --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password mysql --export-dir /user/sqoop/test_mysql_export --table user -m 1 --fields-terminated-by ' ' Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/13 08:36:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 19/06/13 08:36:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/13 08:36:51 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 19/06/13 08:36:51 INFO tool.CodeGenTool: Beginning code generation Thu Jun 13 08:36:51 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 08:36:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 08:36:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 08:36:51 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/Cellar_w/hadoop 注: /tmp/sqoop-wangzhibin/compile/d065816cee026699216afea62bb3193e/user.java使用或覆盖了已过时的 API。 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。 19/06/13 08:36:55 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-wangzhibin/compile/d065816cee026699216afea62bb3193e/user.jar 19/06/13 08:36:55 INFO mapreduce.ExportJobBase: Beginning export of user 19/06/13 08:36:55 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 19/06/13 08:36:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative 19/06/13 08:36:55 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative 19/06/13 08:36:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 19/06/13 08:36:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/06/13 08:37:00 INFO input.FileInputFormat: Total input files to process : 1 19/06/13 08:37:00 INFO input.FileInputFormat: Total input files to process : 1 19/06/13 08:37:00 INFO mapreduce.JobSubmitter: number of splits:1 19/06/13 08:37:00 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative 19/06/13 08:37:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1560136240717_0002 19/06/13 08:37:01 INFO impl.YarnClientImpl: Submitted application application_1560136240717_0002 19/06/13 08:37:01 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1560136240717_0002/ 19/06/13 08:37:01 INFO mapreduce.Job: Running job: job_1560136240717_0002 19/06/13 08:37:11 INFO mapreduce.Job: Job job_1560136240717_0002 running in uber mode : false 19/06/13 08:37:11 INFO mapreduce.Job: map 0% reduce 0% 19/06/13 08:37:16 INFO mapreduce.Job: map 100% reduce 0% 19/06/13 08:37:16 INFO mapreduce.Job: Job job_1560136240717_0002 completed successfully 19/06/13 08:37:17 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=178209 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=181 HDFS: Number of bytes written=0 HDFS: Number of read operations=4 HDFS: Number of large read operations=0 HDFS: Number of write operations=0 Job Counters Launched map tasks=1 Data-local map tasks=1 Total time spent by all maps in occupied slots (ms)=2859 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=2859 Total vcore-milliseconds taken by all map tasks=2859 Total megabyte-milliseconds taken by all map tasks=2927616 Map-Reduce Framework Map input records=3 Map output records=3 Input split bytes=140 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=22 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=201326592 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=0 19/06/13 08:37:17 INFO mapreduce.ExportJobBase: Transferred 181 bytes in 21.1273 seconds (8.5671 bytes/sec) 19/06/13 08:37:17 INFO mapreduce.ExportJobBase: Exported 3 records. 导出结果 其他命令 数据库中的数据导入到HDFS上 sqoop import --connect jdbc:mysql://localhost:3306/itcast --username root --password 123 --table trade_detail --columns 'id, account, income, expenses' 指定输出路径、指定数据分隔符 sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password 123 --table trade_detail --target-dir '/sqoop/td' --fields-terminated-by '\\t' 指定Map数量 -m sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password 123 --table trade_detail --target-dir '/sqoop/td1' --fields-terminated-by '\\t' -m 2 增加where条件 注意：条件必须用引号引起来 sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password 123 --table trade_detail --where 'id>3' --target-dir '/sqoop/td2' 增加query语句(使用 \\ 将语句换行) sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password 123 \\ --query 'SELECT * FROM trade_detail where id > 2 AND $CONDITIONS' --split-by trade_detail.id --target-dir '/sqoop/td3' 注意： 如果使用 --query 这个命令的时候，需要注意的是 where 后面的参数，AND $CONDITIONS 这个参数必须加上 单引号与双引号的区别，如果--query 后面使用的是双引号，那么需要 $CONDITIONS 前加上\\ 即\\$CONDITIONS 如果设置map数量为1个时即-m 1，不用加上--split-by ${tablename.column}，否则需要加上 Sqoop遇到的坑 sqoop Could not initialize class org.apache.derby.jdbc.AutoloadedDriver40 解决方法：将 $HIVE_HOME/lib 中的 derby-x.x.jar 拷贝到 $SQOOP_HOME/lib。 java.security.AccessControlException: access denied (\"javax.management.MBeanTrustPermission\" \"register\") 解决方法： vim /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home/jre/lib/java.policy 新增语句： grant { // JMX Java Management eXtensions permission javax.management.MBeanTrustPermission \"register\"; }; tool.ImportTool: Import failed: java.io.IOException: Hive CliDriver exited with status=1 解决方法： cp $HIVE_HOME/lib/libthrift*.jar $SQOOP_HOME/lib java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.readerFor(Ljava/lang/Class;)Lcom/fasterxml/jackson/databind/ObjectReader; 原因：Sqoop的jackson版本号与Hive中的jackson版本号不一致。 解决方法： WZB-MacBook:lib wangzhibin$ mv jackson-* ../bak/ WZB-MacBook:lib wangzhibin$ cp /usr/local/Cellar_w/hive/lib/jackson-* . 参考资料 官方文档 Sqoop最佳实践 sqoop部署及使用 Sqoop安装 Sqoop安装和简单使用 Hadoop集群中sqoop的安装使用 sqoop数据导入hive 遇到的问题 Sqoop 工作原理 ChangeLog 20190613 | 完善Sqoop从MySQL导入到Hive的部分 20190611 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-13 11:06:50 "},"20180712_commands-of-linux.html":{"url":"20180712_commands-of-linux.html","title":"Linux常用命令","keywords":"","body":"技术 | Linux常用命令 全文检索 find / -print -type f | xargs grep '/var/lib/psql/data' 查看端口占用情况 lsof -i:端口号 netstat -tunlp|grep 端口号 这两种方式都可以查看指定端口被哪个进程占用的情况。 查看当前占用内存较多的程序 ps aux --sort -rss | more 命令说明： ps aux: 列出目前所有的正在内存当中的程序。 a显示终端上的所有进程,包括其他用户地进程(有的进程没有终端)。 -a 显示所有终端机下执行的进程。 -u 以用户为主的格式来显示进程状况。 -x 显示所有进程，不以终端机来区分。 a会包括其他用户(否则只有用户本身); x会包括其他终端; aux就可以包括内存所有; 命令结果说明： USER：该 process 属于那个使用者账号的 PID ：该 process 的进程号 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 Linux 查看CPU信息，机器型号，内存等信息 系统 # uname -a # 查看内核/操作系统/CPU信息 # lsb_release -a # 查看操作系统版本 (适用于所有的linux，包括Redhat、SuSE、Debian等发行版，但是在debian下要安装lsb) # cat /etc/issue | cat /etc/redhat-release # 查看CentOS操作系统版本。 # cat /proc/cpuinfo # 查看CPU信息 # hostname # 查看计算机名 # lspci -tv # 列出所有PCI设备 # lsusb -tv # 列出所有USB设备 # lsmod # 列出加载的内核模块 # env # 查看环境变量 资源 # free -m # 查看内存使用量和交换区使用量 # df -h # 查看各分区使用情况 # du -sh # 查看指定目录的大小 # grep MemTotal /proc/meminfo # 查看内存总量 # grep MemFree /proc/meminfo # 查看空闲内存量 # uptime # 查看系统运行时间、用户数、负载 # cat /proc/loadavg # 查看系统负载 磁盘和分区 # mount | column -t # 查看挂接的分区状态 # fdisk -l # 查看所有分区 # swapon -s # 查看所有交换分区 # hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) # dmesg | grep IDE # 查看启动时IDE设备检测状况 网络 # ifconfig # 查看所有网络接口的属性 # iptables -L # 查看防火墙设置 # route -n # 查看路由表 # netstat -lntp # 查看所有监听端口 # netstat -antp # 查看所有已经建立的连接 # netstat -s # 查看网络统计信息 进程 # ps -ef # 查看所有进程 # top # 实时显示进程状态 # jps # 查看java进程 用户 # w # 查看活动用户 # id # 查看指定用户信息 # last # 查看用户登录日志 # cut -d: -f1 /etc/passwd # 查看系统所有用户 # cut -d: -f1 /etc/group # 查看系统所有组 # crontab -l # 查看当前用户的计划任务 服务 # chkconfig --list # 列出所有系统服务 # chkconfig --list | grep on # 列出所有启动的系统服务 程序 # rpm -qa # 查看所有安装的软件包 查看CPU信息（型号） # cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 8 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz (看到有8个逻辑CPU, 也知道了CPU型号) # cat /proc/cpuinfo | grep physical | uniq -c 4 physical id : 0 4 physical id : 1 (说明实际上是两颗4核的CPU) # getconf LONG_BIT 32 (说明当前CPU运行在32bit模式下, 但不代表CPU不支持64bit) # cat /proc/cpuinfo | grep flags | grep ' lm ' | wc -l 8 (结果大于0, 说明支持64bit计算. lm指long mode, 支持lm则是64bit) 再完整看cpu详细信息, 不过大部分我们都不关心而已. # dmidecode | grep 'Processor Information' 查看内存信息 # cat /proc/meminfo # uname -a Linux euis1 2.6.9-55.ELsmp #1 SMP Fri Apr 20 17:03:35 EDT 2007 i686 i686 i386 GNU/Linux (查看当前操作系统内核信息) # cat /etc/issue | grep Linux Red Hat Enterprise Linux AS release 4 (Nahant Update 5) (查看当前操作系统发行版信息) 查看机器型号 # dmidecode | grep \"Product Name\" 查看网卡信息 # dmesg | grep -i eth 设置网络信息（以CentOS为例） ifconfig查看当前网卡名称与信息 [root@node1 ~]# ifconfig eth0 Link encap:Ethernet HWaddr 00:0C:29:A4:9D:C2 inet addr:192.168.129.137 Bcast:192.168.129.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fea4:9dc2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:908629128 errors:0 dropped:0 overruns:0 frame:0 TX packets:820212029 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:101322481671 (94.3 GiB) TX bytes:109980885936 (102.4 GiB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:90328695 errors:0 dropped:0 overruns:0 frame:0 TX packets:90328695 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:6989862905 (6.5 GiB) TX bytes:6989862905 (6.5 GiB) 设置网络信息 vi /etc/sysconfig/network-scripts/ifcfg-eth0 内容示例： DEVICE=eth0 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=none DEFROUTE=yes IPV4_FAILURE_FATAL=yes IPV6INIT=no NAME=\"System eth0\" HWADDR=00:0C:29:A4:9D:C2 IPADDR=192.168.129.137 PREFIX=24 GATEWAY=192.168.129.10 UUID=5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03 LAST_CONNECT=1516781563 DNS1=202.102.192.68 重新网卡 service network restart Linux基本操作（以CentOS 7.2为例） 设置主机名 //永久性的修改主机名称，重启后能保持修改后的。 hostnamectl set-hostname xxx 或者修改/etc/hostname。 创建用户并添加sudo权限 #添加用户组dev groupadd dev #添加libb用户，并归属于dev组（增加-m 自动创建目录） useradd -m -g dev libb #给tydic用户改密码 passwd libb #给已有的用户增加工作组 gpasswd -a user group #查看组以及组员 cat /etc/group 添加sudo权限 #sudoers 文件添加可写权限 chmod -v u+w /etc/sudoers #打开sudoers文件添加 #表示dev组下所有用户都可以执行sudo，且不用密码 %dev ALL=(ALL) NOPASSWD: ALL #表示libb用户可以执行sudo，且不用密码 libb ALL=(ALL) NOPASSWD: ALL #表示libb用户可以执行sudo libb ALL=(ALL) ALL #最后取消sudoers 文件可写权限 chmod -v u-w /etc/sudoers 关闭防火墙 #firewall-cmd --state // 查看防火墙状态 # systemctl stop firewalld.service #停止firewall # systemctl disable firewalld.service #禁止firewall开机启动 ChangeLog 20190618 | 增加「Linux基本操作命令」 20190612 | 增加「查看当前占用内存较多的程序」 20190605 | 增加「查看端口占用情况」 20180712 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-18 18:58:42 "},"20190612_excessive-memory-of-springboot-program.html":{"url":"20190612_excessive-memory-of-springboot-program.html","title":"SpringBoot程序内存占用过高解决方法","keywords":"","body":"SpringBoot程序内存占用过高解决方法 问题 使用top查看服务器内存占用情况，发现交换内存也已经被占用很多了，说明正在运行的程序占用内存非常高。 使用ps aux --sort -rss命令查看各个程序占用内存数如下，发现使用SpringBoot打包的程序运行占用内存达到2G以上，而实际程序包大小大约40M左右。 初步判断是JVM设置问题。 关于jvm配置 参考 IntelliJ IDEA设置JVM运行参数 有如下描述: 设置JVM内存的参数有四个： -Xmx Java Heap最大值，默认值为物理内存的1/4，最佳设值应该视物理内存大小及计算机内其他内存开销而定； -Xms Java Heap初始值，Server端JVM最好将-Xms和-Xmx设为相同值，开发测试机JVM可以保留默认值； -Xmn Java Heap Young区大小，不熟悉最好保留默认值； -Xss 每个线程的Stack大小，不熟悉最好保留默认值； 参考 java--jvm启动的参数 有如下描述： 一般用到最多的是： -Xms512m 设置JVM促使内存为512m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 -Xmx512m ，设置JVM最大可用内存为512M。 -Xmn200m：设置年轻代大小为200M。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 -Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。(实际发现-Xss128k设置无法启动SpringBoot程序，并提示至少需要228k。) JVM参数设置示例： java -Xms64m #JVM启动时的初始堆大小 -Xmx128m #最大堆大小 -Xmn64m #年轻代的大小，其余的空间是老年代 -XX:MaxMetaspaceSize=128m # -XX:CompressedClassSpaceSize=64m #使用 -XX：CompressedClassSpaceSize 设置为压缩类空间保留的最大内存。 -Xss256k #线程 -XX:InitialCodeCacheSize=4m # -XX:ReservedCodeCacheSize=8m # 这是由 JIT（即时）编译器编译为本地代码的本机代码（如JNI）或 Java 方法的空间 -XX:MaxDirectMemorySize=16m -jar app.jar 参考启动脚本 java -Xms512m -Xmx512m -Xmn200m -Xss500k -jar dap-service-manager-0.0.1.jar --server.port=9200 实际效果：能将程序占用内存数减少至600M所有。 参考资料 Spring cloud开发内存占用过高解决方法 ChangeLog 20190612 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-12 14:45:01 "},"20190619_Install-the-virtual-machine-remotely-using-vmfusion.html":{"url":"20190619_Install-the-virtual-machine-remotely-using-vmfusion.html","title":"使用vmfusion远程安装虚拟机","keywords":"","body":"使用vmfusion远程安装虚拟机 使用VmFusion连接远程服务器 选择「在远程服务器上创建虚拟机」 输入远程地址、用户名、密码，点击连接即可。 连接完成后，即展示该服务器上的所有虚拟机。 创建虚拟机 点击「创建新的虚拟机」，选择「localhost.localdomain()」，选择存储盘，点击「继续」。 选择硬件版本，默认即可。 选择网络，默认即可。 选择操作系统「Linux」-「CentOS 4/5/6/7(64位)」。 选择固件类型，默认即可。 配置虚拟磁盘。 修改虚拟机名称，然后点击「自定义设置」。 修改处理和内存。 点击「添加设备」-「CD/DVD驱动器」。 选择主机中的ISO文件。（如果没有请参考上传ISO文件到主机） 或者选择「CD/DVD（IDE）」，加载本地ISO文件。 开始安装系统 选择安装过程中的语言 初始化安装信息——安装位置。 初始化安装信息——网络和主机名 开始安装 设置Root密码 创建用户 等待安装完成即可。 上传ISO文件到主机 服务器安装的是Vmware ESXi服务器，并不是普通的Linux服务器，访问https://192.168.129.95/ui/，使用用户名密码即可登录管理服务器。 选择「存储」-「datastore1」-「数据存储浏览器」，即可上传下载文件到主机了。 参考资料 ChangeLog 20190619 | 新建文档。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-19 19:02:28 "},"20190426_installation-of-python-on-macos.html":{"url":"20190426_installation-of-python-on-macos.html","title":"MacOS中Python环境搭建","keywords":"","body":"2019-04-26 | Mac OS搭建Python开发环境 安装Python Brew方式安装 $ brew install python3 Warning: python 3.7.3 is already installed, it's just not linked You can use `brew link python` to link this version. 问题：发现/usr/local/下没有路径/usr/local/Frameworks。需要新建该路径，并修改权限 $ brew install python3 Error: An unexpected error occurred during the `brew link` step The formula built, but is not symlinked into /usr/local Permission denied @ dir_s_mkdir - /usr/local/Frameworks Error: Permission denied @ dir_s_mkdir - /usr/local/Frameworks 解决： $ sudo mkdir /usr/local/Frameworks $ sudo chown $(whoami):admin /usr/local/Frameworks brew link python3 ``` $ brew link python3 Linking /usr/local/Cellar/python/3.7.3... 20 symlinks created ``` which python3 ``` $ which python3 /usr/local/bin/python3 ``` 一个非常重要的问题 来源：https://www.jianshu.com/p/892a14bdda4d Mac OS自身其实已经带有Python，版本为2.7.X，这个Python主要用于支持系统文件和XCode，所以我们在安装新的Python版本时候最好不要影响这部分。 这里就会出现一个十分困扰的问题，我们按照上述步骤安装好了自己所需要Python版本，目前我们一般都会选择安装Python 3.X版本，在安装好了之后，我们输入以下命令 python --version或者python 发现所示内容仍然是2.7.X版本的Python，这是因为我们使用python这个命令，系统仍然会调用默认的Python版本(即系统版本)，网上很多教程会让我们修改配置文件或者$PATH变量将系统默认Python版本切换至我们安装的版本，但是个人感觉没有多大必要，毕竟系统的东西能不改最好不要改。 所以我们选择一个比较简单的办法，就是当我们需要使用自己安装的Python版本时(即之前安装的3.X版本)，直接使用python3作为命令即可。相同的命令为： 终端输入以下命令，查看Python安装位置which python3，终端输入以下命令，查看Python当前版本python3 --version，终端输入以下命令，进入Python交互模式python3 安装pip3 参考：https://oldtang.com/351.html https://pip.readthedocs.io/en/stable/installing/ 如果通过 homebrew 安装 python3，那么 pip3 会同时安装。所以建议直接通过 homebrew 安装 python3： brew install python3 如果你已经通过其他渠道安装了 python3 但是尚未安装 pip3，那么需要通过以下步骤实现安装： curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py cd cd /usr/local/bin/ ln -s ../Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/pip3 pip3 安装完成后，可以查看一下 pip3 的版本： pip3 -V 使用 pip3 安装第三方库 这个就很简单了，直接使用 pip3 install 就行了，比如： pip3 install flask 自己想装什么就装什么即可。 Mac中Python开发IDE环境搭建 参考：https://blog.csdn.net/a464057216/article/details/55652179 https://www.readern.com/configure-vscode-for-python3-on-mac.html 安装Vscode 略 下载Python插件 配置PythonPath 新建Python文件test.py msg = \"Hello World\" print(msg) 执行python脚本 学习资料 《机器学习从认知到实践》源码：https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints 参考资料 Mac OS搭建Python开发环境 brew link python3出错 Permissions issue when linking python3 ChangeLog 2019-06-10 | 修改文档样式，转移到GitBook。 20190426 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-10 16:03:34 "},"20190613_installation-of-nginx-on-centos.html":{"url":"20190613_installation-of-nginx-on-centos.html","title":"CentOS 6 安装Nginx指南","keywords":"","body":"CentOS 6 安装Nginx Nginx下载 wget http://nginx.org/download/nginx-1.16.0.tar.gz 安装依赖包 # yum -y install gcc gcc-c++ automake autoconf libtool make # yum -y install openssl openssl-devel # yum -y install pcre pcre-devel # yum -y install zlib zlib-devel Nginx安装 将nginx解压并移动到/usr/local目录下 # tar zxvf nginx-1.16.0.tar.gz # mv nginx-1.16.0 /usr/local/ 进入nginx目录 # cd /usr/local/nginx-1.16.0/ 依赖包 # yum -y install gcc automake autoconf libtool make configure [root@node1 nginx-1.16.0]# ./configure ... Configuration summary + using system PCRE library + OpenSSL library is not used + using system zlib library nginx path prefix: \"/usr/local/nginx\" nginx binary file: \"/usr/local/nginx/sbin/nginx\" nginx modules path: \"/usr/local/nginx/modules\" nginx configuration prefix: \"/usr/local/nginx/conf\" nginx configuration file: \"/usr/local/nginx/conf/nginx.conf\" nginx pid file: \"/usr/local/nginx/logs/nginx.pid\" nginx error log file: \"/usr/local/nginx/logs/error.log\" nginx http access log file: \"/usr/local/nginx/logs/access.log\" nginx http client request body temporary files: \"client_body_temp\" nginx http proxy temporary files: \"proxy_temp\" nginx http fastcgi temporary files: \"fastcgi_temp\" nginx http uwsgi temporary files: \"uwsgi_temp\" nginx http scgi temporary files: \"scgi_temp\" make # make # make install 启动 # /usr/local/nginx/sbin/nginx # /usr/local/nginx/sbin/nginx -s reload 安装成功，在浏览器访问http://192.168.129.137，展示页面： 参考资料 CentOS安装Nginx ChangeLog 20190613 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-18 18:54:27 "},"20180603_introduction-of-markdown.html":{"url":"20180603_introduction-of-markdown.html","title":"初步认识Markdown","keywords":"","body":"初步认识markdown 为什么是Markdown 文本编辑比较 无格式文本：简洁、不依赖工具 富文本：美观，重点突出 Markdown与富文本编辑器异同： 作用一致： 使用者输入纯文字，通过编辑器的处理，使其拥有一份样式，最终得到带格式的文档。 使用区别： 富文本编辑器「所见即所得」 Markdown是一种标记语言，手动切换预览与编辑模式。 Markdown同时兼具无格式文本与富文本的优势。 什么是Markdown Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。 —— 维基百科 Markdown 是一种轻量级的「标记语言」，创始人为约翰·格鲁伯。Markdown 的创始人 John Gruber 这样定义： \"Markdown\" is two things: (1) a plain text formatting syntax; (2) a software tool, that converts the plain text formatting to others. 也就是说Markdown首先意味着是一套语法规则，其次代表了编辑器，把纯文本转换为排版效果的文字。 Markdown语法演进 CommonMark GFM(GitHub Flavored Markdown) 其他语法：PHP Markdown Extra、Maruku、kramdown、RDiscount、Redcarpet、MultiMarkdown 原有的 Markdown 语法的功能稍显不足，Github Flavored Markdown 在前面所说的语法的三个方面都做出了相应的增强。 比如： 标准Markdown要在一行的最后加两个空格符才表示换行，否则是不换行的；但是GFM则没有此要求。 支持把列表变成带勾选框的任务列表 在对段落的处理方面，对原有代码块进行了增强，可以制定不同的语言类型对代码进行语法高亮。 GFM的修改参考：GFM修改，GFM语法参考：GFM语法 语法特点： 用简洁的语法代替排版，其常用的标记符号不超过十个， 相对于更为复杂的 HTML 标记语言来说，Markdown 十分的轻量 一旦熟悉这种语法规则，会有沉浸式编辑的效果。 其他增强型Markdown语法：MultiMarkdown Markdown优势 书写过程流畅 富文本编辑器编辑文字时是两个不连续的动作，输入文字时双手放在键盘上，编辑文字则需要视线和手离开输入框和键盘，去寻找和点击功能按钮。 Markdown 的「书写流畅」就体现在将这两个动作合成一个输入字符的动作。 格式不随编辑器而改变，导出与分享方便 Markdown 格式保持的文件本质上仍是一份纯文本。 书写错误容易发现。 比如Word中，用空格、分页来控制排版，容易出错。而Markdown没有不需要考虑几个空格的问题，如果有几个词语想加粗，没有渲染成功，就说明写错了。 Markdown 的局限性 什么时候不该用 Markdown？ Markdown 无法对「段落」进行灵活处理。比如：文本位置 Markdown 对非纯文本元素的排版能力很差。比如：图文混排 Markdown一开始就定位为「文字输入工具」，不适合对排版格式自定义程度较高的文档进行排版。 Markdown适用场景 网络环境下的写作 利用了 Markdown 「写作即排版」的特点，Markdown 可以让使用者专心于文章书写，而非排版。 文档协作 团队成员间可以自由选用自己喜欢的操作系统和编辑器工具来进行写作，而不局限于 Word 或者 Google Docs等只支持富文本编辑的软件。 文档的展示方式不仅仅是在编辑器中，你可以随时把文档转换成网页，任何时候任何人都可以方便地查看。 利用它「纯文本格式」的优势，用 Markdown 来文档协作会比其他工具更自由。 基础语法 标题 markdown代码： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 ### 文字格式 **This is bold text** *This text is italicized* ~~This was mistaken text~~ **This text is _extremely_ important** 区块引用 尼采说： Was mich nicht umbringt, macht mich stärker. markdown代码： #### 尼采说： > Was mich nicht umbringt, macht mich **stärker**. 列表 无序列表 George Washington John Adams Thomas Jefferson George Washington John Adams Thomas Jefferson 有序列表 James Madison James Monroe John Quincy Adams 嵌套列表 First list item First nested list item Second nested list item markdown代码： #### 无序列表 - George Washington - John Adams - Thomas Jefferson * George Washington * John Adams * Thomas Jefferson #### 有序列表 1. James Madison 2. James Monroe 3. John Quincy Adams #### 嵌套列表 1. First list item - First nested list item - Second nested list item 代码 行内代码 Use git status to list all new or modified files that haven't yet been committed. 代码块 @requires_authorization def somefunc(param1='', param2=0): '''A docstring''' if param1 > param2: # interesting print 'Greater' return (param2 - param1 + 1) or None class SomeClass: pass >>> message = '''interpreter ... prompt''' markdown代码： #### 行内代码 Use `git status` to list all new or modified files that haven't yet been committed. #### 代码块 ​```python @requires_authorization def somefunc(param1='', param2=0): '''A docstring''' if param1 > param2: # interesting print 'Greater' return (param2 - param1 + 1) or None class SomeClass: pass >>> message = '''interpreter ... prompt''' ​`` ` 分割线 这是分割线 这也是分割线 markdown代码： 这是分割线 --- 这也是分割线 *** 链接 网络链接 This site was built using GitHub Pages. This site was built using GitHub Pages. 相对链接 Test.md 图片链接 markdown代码： #### 网络链接 This site was built using [GitHub Pages](https://pages.github.com/). This site was built using [GitHub Pages][1]. #### 相对链接 [Test.md](./test.md) #### 图片链接 ![markdown](http://pic.iloc.cn/2019-06-05-I-love-markdown-syntax-language.png) 脚注 脚注demo 参考1 markdown代码： 脚注[^demo] 参考[^1] 高级语法 注：可能会有渲染不支持的情况。 表格 Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 markdown代码： #### 表格 | Item | Value | Qty | | :------- | -------: | :--: | | Computer | 1600 USD | 5 | | Phone | 12 USD | 12 | | Pipe | 1 USD | 234 | 目录 [TOC] markdown代码： #### 目录 [TOC] 待办事项 使用 - [ ] 和 - [x] 语法可以创建复选框，实现 todo-list 等功能。例如： [x] 已完成事项 [ ] 待办事项1 [ ] 待办事项2 markdown代码： #### 待办事项 使用 `- [ ]` 和 `- [x]` 语法可以创建复选框，实现 todo-list 等功能。例如： - [x] 已完成事项 - [ ] 待办事项1 - [ ] 待办事项2 公式 $$E=mc^2$$ 可以创建行内公式，例如 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。或者块级公式： $$x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ markdown代码： #### 公式 $$E=mc^2$$ 可以创建行内公式，例如 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。或者块级公式： $$x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ 流程图 st=>start: Start op=>operation: Your Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op markdown代码： #### 流程图 ​```flow st=>start: Start op=>operation: Your Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op ​`` ` 序列图 Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks! markdown代码： #### 序列图 ​```sequence Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks! ​`` ` 工具推荐 MWeb（推荐，收费，Mac/IOS） Typora（推荐，免费。全平台） 特点： WYSIWYG（What You See Is What You Get） 表格编辑功能增强 插入图片 代码和数学公式输入 支持显示目录大纲 下载：https://www.typora.io/ 介绍：https://sspai.com/post/30292 参考资料 印象笔记 Markdown 入门指南：https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php Markdown 完全入门（上）：https://sspai.com/post/36610 Markdown 完全入门（下）：https://sspai.com/post/36682 Markdown教程：http://www.markdown.cn/ 创始人 John Gruber 的 Markdown 语法说明：https://daringfireball.net/projects/markdown/syntax Github Flavored Markdown语法：https://help.github.com/articles/basic-writing-and-formatting-syntax/ 印象笔记 Markdown 入门指南：https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php Markdown简易入门教程：https://blog.huihut.com/2017/01/25/MarkdownTutorial/ Typora介绍：https://sspai.com/post/30292 ChangeLog 20190606 | 推荐工具增加MWeb，去掉小书匠、有道云笔记等。 20190606 | 从Evernote转移到GitBook 20180603 | 完成第一版，并在小组内分享 demo. 这是一个脚注。 ↩ 1. 这也是一个脚注。 ↩ Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 19:53:02 "},"20190214_git-code-transfer-to-gitee.html":{"url":"20190214_git-code-transfer-to-gitee.html","title":"Git代码转移到码云相关命令","keywords":"","body":"Git代码转移到码云相关命令 查看当前用户（global）配置 git config --global --list 生成公钥 ssh-keygen -t rsa -C \"youremail@example.com\" 查看本地代码的远程仓库地址 git remote -v 校验本机是否能连接gitee ssh -T git@gitee.com 重新设置远程仓库地址 git remote set-url origin git@gitee.com:xxx/xxxx.git OR git remote set-url origin https://gitee.com/xxxx/xxxx.git git remote -v 第一次pull出现问题的处理方法 git pull --allow-unrelated-histories 代码的换行符与windows不一致的情况，处理方法： git config --global core.autocrlf input ChangeLog 20190606 | 从Evernote迁移到GitBook 20190214 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:27 "},"20190603_doduwiki-installation-on-macos.html":{"url":"20190603_doduwiki-installation-on-macos.html","title":"DokuWiki安装小记","keywords":"","body":"DokuWiki安装小记 2019-06-03 以下为MacOS平台的DokuWiki安装，其他平台类似。 什么是DokuWiki DokuWiki 是一个使用，用途多样的开源 Wiki 软件，并且不需要数据库。 比较wiki 安装准备工作 安装nginx。 安装php。参考MacOS下安装PHP与nginx配置 安装DokuWiki 安装过程参考：DokuWiki with Mac OS X and Apache 下载DokuWiki。可以在官网直接下载，我下载的是2019-05-31 snapshot版本，放到目录/usr/local/Cellar_w/raw下。 WZB-MacBook:raw wangzhibin$ pwd /usr/local/Cellar_w/raw WZB-MacBook:Cellar_w wangzhibin$ wget https://download.dokuwiki.org/src/dokuwiki/dokuwiki-stable.tgz 解压DokuWiki安装包，并创建软连接。 WZB-MacBook:raw wangzhibin$ tar zxvf splitbrain-dokuwiki-upstream-0.0.20091252c-7022-g2e6e11a.tar.gz WZB-MacBook:raw wangzhibin$ cd .. WZB-MacBook:Cellar_w wangzhibin$ pwd /usr/local/Cellar_w WZB-MacBook:Cellar_w wangzhibin$ ln -s raw/splitbrain-dokuwiki-2e6e11a/ dokuwiki 配置nginx代理 打开nginx.conf，在http块添加一行include servers/*.conf;（默认已经存在）。MacOS使用brew安装nginx的配置文件目录在/usr/local/etc/nginx/。 在/usr/local/etc/nginx/servesr下创建dokuwiki.conf。 增加以下内容： server { listen 8090; root /usr/local/Cellar_w/dokuwiki; server_name localhost; index index.php index.html doku.php; location ~ ^/(data|conf|bin|inc) { return 404; } location ~ ^/lib.*\\.(gif|png|ico|jpg)$ { expires 31d; } #location / { # try_files $uri $uri/ @dokuwiki; #} location @dokuwiki { rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&id=$2 last; rewrite ^/tag/(.*) /doku.php?id=tag:$1&do=showtag&tag=tag:$1 last; rewrite ^/(.*) /doku.php?id=$1&$args last; } location ~ .+\\.php($|/) { include fastcgi_params; set $real_script_name $uri; set $path_info \"/\"; if ($fastcgi_script_name ~ \"^(.+\\.php)(/.+)$\") { set $real_script_name $1; set $path_info $2; } fastcgi_param SCRIPT_FILENAME $document_root$real_script_name; fastcgi_param SCRIPT_NAME $real_script_name; fastcgi_param PATH_INFO $path_info; fastcgi_pass 127.0.0.1:9000;#监听9000端口 fastcgi_index index.php; } } 重新启动nginx，下面两种命令都可以。 sudo brew services restart nginx /usr/local/bin/nginx -s reload 在浏览器中打开http://localhost:8090，即可访问dokuWiki首页了。 使用DokuWiki 初始化安装与ACL启用 中文文件名乱码问题 在界面中搜索“测试页面” 点击红色的“:测试页面”，即可创建中文词条“测试页面”。随便写个内容，保存。 在后台pages目录下该词条名称乱码。 下面就开始修改源码，使得中文词条名称正常。 在dokuwiki/conf/local.php文件最后一行加上。（如果conf目录仅发现local.php.dist文件，这是没有install的缘故。） $conf[ 'fnencode' ] = 'utf-8'; #注意分号不能少。 再次尝试创建中文词条 发现pages目录正常。 Doku主题 官方主题排行 官方主题按照受欢迎程度排名靠前的如下： 主题安装 以主题Breeze为例。 下载主题Breeze Template，解压到lib/tpl/目录下，命名为breeze。 在conf/local.php文件中增加： $conf['template']='breeze'; // 配置的是tpl下主题目录名。 推荐主题 推荐的几个主题。在官网主题区可以搜索到。 * bootstrap3 * vector (强烈推荐) * sprintdoc * breeze * white 修改vector主题样式 修改侧边栏的宽度 修改lib/tpl/vector/static/3rd/vector/main-ltr.css中的样式： div#panel { ... width: 13em; ... } ... /* Content */ div#content { margin-left: 13em; ... } ... /* Footer */ div#footer { margin-left: 13em; ... } ... /* Navigation Containers */ #left-navigation { ... left: 13em; ... } 更换logo 使用Vector主题时，logo位置为：background-image:url(/lib/tpl/vector/static/3rd/dokuwiki/logo.png); 替换即可。 DokuWiki插件 侧边栏插件indexmenu 安装indexmenu插件 在sidebar页面增加配置，左侧会自动出现全部页面导航。 indexmenu使用 仅显示:wiki里面的内容 默认展开到两层 仅展现wiki下的，并且去掉toc、右键菜单。 Vector主题下不能展示sidebar的修改方法，在lib/tpl/vector/conf/default.php中修改： //$conf[\"vector_navigation_location\"] = \":wiki:navigation\"; //page/article used to store the navigation $conf[\"vector_navigation_location\"] = \":sidebar\"; //page/article used to store the navigation 增加排序 新增页面插件Add New Page 在扩展管理器中搜索“Add New Page”，安装此插件。 在sidebar页面下方增加，表示仅在wiki命名空间下增加页面。 ------------------------ false 有了page新增管理，那么命名空间怎么新增呢？ 直接在浏览器输入http://localhost:8091/doku.php?id=wiki:test:新目录:新文件，即可创建。 如何删除文件？ 进入编辑页面，文章内容全部删除后，保存，该文章就被删除了。与此同时，如果命名空间下没有文章，命名空间也被删除了 移动插件move 在扩展管理器中搜索“move”，安装此插件。 使用方法，在管理界面，会出现“页面移动/重命名...”的工具，可以进入管理界面。 贡献插件authorstats 安装authorstats插件。 在文章中增加 保存后即可查看 评论区插件discussion 插件：plugin:discussion 用法：~~DISCUSSION~~，插入该语句到 wiki 中，即可在 wiki 内容后添加评论区。 配置：管理->配置管理->Discussion，比较有用的配置： 订阅评论区 通知版主有新评论 允许未注册用户评论 可通过 wiki 语法评论 其他可直接安装使用的插件 编辑器支持markdown语法插件：plugin:markdowku 编辑器支持直接粘贴图片：plugin:imgpaste 导出Word文件plugin:OpenOffice.org 附录 local.php文件内容 sidebar.txt文件内容 {{indexmenu>:wiki#2|js#vista.png notoc nomenu tsort}} --------- {{NEWPAGE>wiki}} start内容底部包括贡献内容 . 参考资料 dokuwiki 搭建 官方插件 用 Dokuwiki 管理小团队知识 dokuwiki安装使用教程（支持中文、editor.md、粘贴上传图片） ChangeLog 20190603 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 12:09:45 "},"20190513_fast-connection-of-ssh-with-iterm.html":{"url":"20190513_fast-connection-of-ssh-with-iterm.html","title":"iTerm2快速SSH连接并保存密码","keywords":"","body":"2019-05-13 | iTerm2快速SSH连接并保存密码 2019-05-15 发布博客。 背景 Mac自带terminal，以及比较好用的iTerm2命令行工具，都缺乏一个功能，就是远程SSH连接，无法保存密码。 一种方法是将本机的ssh_key放到远程服务器中实现无密码登录。这种方法在很多情况下无法实现，因为远程服务器大多是客户的。 本文介绍一个简单、轻量级的命令行工具——sshpass，通过它我们能够向命令提示符本身提供密码（非交互式密码验证），这样就可以实现自动连接远程服务器，而且能自动执行远程命令。 安装sshpass 下载sshpass：https://sourceforge.net/projects/sshpass/files/ 进入 sshpass目录 运行【./configure】 运行【sudo make install】 运行【sshpass 】 来测试是否安装成功 sshpass使用 Usage: sshpass [-f|-d|-p|-e] [-hV] command parameters -f filename Take password to use from file -d number Use number as file descriptor for getting password -p password Provide password as argument (security unwise) -e Password is passed as env-var \"SSHPASS\" With no parameters - password will be taken from stdin -h Show help (this screen) -V Print version information At most one of -f, -d, -p or -e should be used 使用用户名和密码登录到远程Linux ssh服务器（10.42.0.1），并检查文件系统磁盘使用情况，如图所示。 $ sshpass -p 'my_pass_here' ssh aaronkilik@10.42.0.1 'df -h' 也可以使用sshpass 通过scp传输文件或者rsync备份/同步文件，如下所示： ------- Transfer Files Using SCP ------- $ scp -r /var/www/html/example.com --rsh=\"sshpass -p 'my_pass_here' ssh -l aaronkilik\" 10.42.0.1:/var/www/html ------- Backup or Sync Files Using Rsync ------- $ rsync --rsh=\"sshpass -p 'my_pass_here' ssh -l aaronkilik\" 10.42.0.1:/data/backup/ /backup/ iTerm2集成sshpass实现快速SSH连接 打开iTerm2的Profiles菜单，进入Profiles设置。 点击Edit Profiles。 增加SSH连接。 Name：名称 Tags：分组或者标签名称 Title：设置窗口名称 Command：/usr/local/bin/sshpass -p 'xxxx' ssh root@192.168.129.116 快速连接 参考资料 sshpass：一个很棒的免交互SSH登录工具，但不要用在生产服务器上 iTerm2 保存ssh用户名密码 ChangeLog 20190609 | 转移到GitBook 20190515 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 14:19:17 "},"20190603_macos_brew_update_too_slow.html":{"url":"20190603_macos_brew_update_too_slow.html","title":"MacOS的brew update很慢问题解决方案","keywords":"","body":"2019-06-03 | MacOS的brew update很慢问题解决方案 替换为中科大源 替换brew.git cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.git 替换homebrew-core.git cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git 替换Homebrew Bottles源 就是在~/.bashrc或者~/.bash_profile文件末尾加 export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles 这两个文件可以自己创建，~/.bashrc和~/.bash_profile都可以 切换回官方源： 重置brew.git cd \"$(brew --repo)\" git remote set-url origin https://github.com/Homebrew/brew.git 重置homebrew-core cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://github.com/Homebrew/homebrew-core.git 替换为清华源 brew清华源 参考资料 Homebrew国内源设置与常用命令 ChangeLog 20190603 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:11:27 "},"20190603_php_installation_on_macos.html":{"url":"20190603_php_installation_on_macos.html","title":"MacOS下安装PHP与nginx配置","keywords":"","body":"2019-06-03 | PHP安装与nginx配置 MacOS安装PHP MacOS系统自带PHP的，不过还是建议自己安装。 使用brew安装PHP # brew update # brew install php@7.2 ... ... To have launchd start php@7.2 now and restart at login: brew services start php@7.2 Or, if you don't want/need a background service you can just run: php-fpm 注：如何brew update很慢的话，可以参考MacOS的brew update很慢问题解决方案进行设置。 修改配置文件 执行命令： sudo cp /private/etc/php-fpm.conf.default /private/etc/php-fpm.conf sudo cp /private/etc/php-fpm.d/www.conf.default /private/etc/php-fpm.d/www.conf 找到/private/etc/目录下的 php-fpm 文件 /private/etc/php-fpm.conf 找到24行的 error_log ，改为（整行替换，注意 ‘;’ 和空格，就是要把‘;’也删除掉） error_log = /usr/local/var/log/php-fpm.log 否则 php-fpm 时会报错： ERROR: failed to open error_log (/usr/var/log/php-fpm.log): No such file or directory 启动PHP 如果希望后台自动启动，执行以下命令： brew services start php@7.2 如果只是本次执行，直接运行： php-fpm 配置Nginx运行php 打开nginx.conf ，在http块添加一行include servers/*.conf;。MacOS使用brew安装nginx的配置文件目录在/usr/local/etc/nginx/。 在/usr/local/etc/nginx/servesr下创建xxx.conf。我这里创建的是dokuwiki.conf 增加以下内容，尤其注意location ~ .+\\.php($|/)部分。 server { listen 8090; root /usr/local/Cellar_w/dokuwiki; server_name localhost; index index.php index.html doku.php; location ~ ^/(data|conf|bin|inc) { return 404; } location ~ ^/lib.*\\.(gif|png|ico|jpg)$ { expires 31d; } #location / { # try_files $uri $uri/ @dokuwiki; #} location @dokuwiki { rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&id=$2 last; rewrite ^/tag/(.*) /doku.php?id=tag:$1&do=showtag&tag=tag:$1 last; rewrite ^/(.*) /doku.php?id=$1&$args last; } location ~ .+\\.php($|/) { include fastcgi_params; set $real_script_name $uri; set $path_info \"/\"; if ($fastcgi_script_name ~ \"^(.+\\.php)(/.+)$\") { set $real_script_name $1; set $path_info $2; } fastcgi_param SCRIPT_FILENAME $document_root$real_script_name; fastcgi_param SCRIPT_NAME $real_script_name; fastcgi_param PATH_INFO $path_info; fastcgi_pass 127.0.0.1:9000;#监听9000端口 fastcgi_index index.php; } } 重新启动nginx，下面两种命令都可以。 sudo brew services restart nginx /usr/local/bin/nginx -s reload 遇到的坑 问题一：Nginx代理PHP，报错Connection reset by peer nginx.log中错误信息如下： [error] 85581#0: *4 kevent() reported about an closed connection (54: Connection reset by peer) while reading response header from upstream, client: 127.0.0.1, server: localhost, request: \"GET / HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"localhost:8088\" 出错的原因是php-fpm未启动。 解决：sudo php-fpm 问题二、使用sudo php-fpm会报路径出错问题 可能是你配置路径那里没有修改，具体可以看上面的 修改配置文件章节。 问题二：No pool defined [root@localhost etc]# service php-fpm start Starting php-fpm [28-Nov-2016 17:13:23] WARNING: Nothing matches the include pattern ‘/usr/local/php/etc/php-fpm.d/*.conf’ from /usr/local/php/etc/php-fpm.conf at line 125. [28-Nov-2016 17:13:23] ERROR: No pool defined. at least one pool section must be specified in config file [28-Nov-2016 17:13:23] ERROR: failed to post process the configuration [28-Nov-2016 17:13:23] ERROR: FPM initialization failed 解决方法: 进入PHP安装目录/etc/php-fpm.d cp www.conf.default www.conf CentOS 6 安装 PHP 检查当前安装的PHP包 # yum list installed | grep php 如果有安装的PHP包，先删除他们。 # yum remove php.x86_64 php-cli.x86_64 php-common.x86_64 php-gd.x86_64 php-ldap.x86_64 php-mbstring.x86_64 php-mcrypt.x86_64 php-mysql.x86_64 php-pdo.x86_64 配置yum源 追加CentOS 6.5的epel及remi源。 # rpm -Uvh http://ftp.iij.ad.jp/pub/linux/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm # rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm 安装PHP 复制下面命令请勿换行执行。先复制到文本中，编辑成一行，在执行。 yum install --enablerepo=remi --enablerepo=remi-php56 php php-bcmath php-opcache php-devel php-mbstring php-mcrypt php-mysqlnd php-gd php-xml php-memcache php-redis php-fpm php-mysql php-common php-mssql 安装检查 [root@node1 ~]# php -version PHP 5.6.40 (cli) (built: May 28 2019 10:55:13) Copyright (c) 1997-2016 The PHP Group Zend Engine v2.6.0, Copyright (c) 1998-2016 Zend Technologies with Zend OPcache v7.0.6-dev, Copyright (c) 1999-2016, by Zend Technologies 配置php.ini文件，关闭php信息头。 vi etc/php.ini expose_php = Off // 关闭php信息 service php-fpm restart 设置开机启动。 chkconfig php-fpm --level 2345 on CentOS 6 安装 epel 源出现的问题 问题： Error: Cannot retrieve metalink for repository: epel. Please verify its path and try again 原因：/etc/yum.repos.d/epel.repo 配置文件中源地址没有生效 解决： vim /etc/yum.repos.d/epel.repo 修改： [epel] name=Extra Packages for Enterprise Linux 6 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/6/$basearch mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&arch=$basearch failovermethod=priority enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 修改成： [epel] .. baseurl=http://download.fedoraproject.org/pub/epel/6/$basearch #mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&arch=$basearch ... 保存退出后，清理源 yum clean all 参考资料 MACOS下安装PHP运行环境 Mac Nginx+php环境配置，看我就够了 在 macOS High Sierra 10.13 搭建 PHP 开发环境 php-fpm:No pool defined解决方法 centos 6.8 yum安装 PHP 5.6 安装epel源后，报错Error: Cannot retrieve metalink for repository: epel. Please verify its path.. ChangeLog 20190617 | 增加CenOS 6 安装 PHP 文档。 20190603 | 创建文档。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-17 16:32:57 "},"20180904_popular-app-in-MacOS.html":{"url":"20180904_popular-app-in-MacOS.html","title":"MacOS中有哪些比较好的软件","keywords":"","body":"软件 | MacOS中有哪些比较好的软件 2018-09-04 购机 Mac常用软件 印象笔记/Evernote QQ/QQ音乐 Parallels Desktop for Mac V14：Mac下最省心的虚拟机。 https://www.newasp.net/soft/454880.html http://down-www.newasp.net/pcdown/soft/mac/parallelsdesktop14crack.dmg https://www.xp510.com/xiazai/ossoft/mactools/45226.html parallels client ： mac下最好用的windows远程桌面连接软件。 shadowsockX-NG：mac下的翻墙工具，需要自备ss账号。 google chrome：不解释 cornerstone：Mac下的svn工具。暂时没有找到理想的，这个还算不错。 坚果云：全平台云同步服务 keepassXC 2.3.4：keepass密码管理工具 microsoft offic for mac/wps for mac：Mac下支持Office真的很差，只能用这两个了，WPS在2018年底才出来，还需要优化。 foxmail 微云（主要是为了下载mac软件） FUSE for macOS 3.8.2：Mac下读写MTFS神器 https://github.com/osxfuse/osxfuse/releases Omni系列 Mac效率软件 cheatsheet：https://www.mediaatelier.com/CheatSheet/ atext alfred 3 （破解需要xcode command line） betterTouchTools（还需要研究一下怎么用。） 可以设置三指点击，=快捷键command+W，关闭标签页。 Keyboard Maestro： 如何用自动化神器 Keyboard Maestro，高效做读书笔记？（附教程） 让你的Mac成为超高效率的工作工具(Keyboard Maestro 和 Alfred的整合) AppCleaner：轻量级软件卸载工具 Dr. clean pro：Mac下的系统优化软件。 the unarchiver：Mac下的压缩与解压缩软件 iStat Menus_6.20 mac系统监控神器，可以显示电池剩余时间。 从finder到终端的小插件：cdto https://www.gracecode.com/posts/3081.html https://github.com/jbtule/cdto Magnet：可以快速实现窗口并排 AirServer：iPad投屏到Mac PopClip：快捷菜单，可以实现各种快捷操作。不太好的一点，就是每次选中都会有一个Command+c的操作，在Excel里面用的时候，就不爽了。 Typeeto/1Keyboard：可以实现mac键盘充当iphone、ipad蓝牙键盘。神器。 Bartender：Mac任务栏精简工具，可以把不常用的图标收缩起来。 iText：一款从图片中识别文字、并翻译的工具。先安装1.2.8版本，然后使用appstore升级。 开发/编辑器 axure：原型设计 navicat：数据库管理 typaro：最好用的Markdown编辑器 pandoc，为了能导出word。https://github.com/jgm/pandoc/releases/tag/2.2.3.2 macdown，与Typora配合，可以复制完整带格式的文本到邮件。轻量。但是不能拷贝图片。 idea xcode command line jdk7/8 maven https://www.jianshu.com/p/191685a33786 安装brew，主要是为了把原生的svn1.9降级到1.8，兼容cornerstone3.0.3 https://www.jianshu.com/p/4e80b42823d5 svn降级 ：https://note.devework.com/mac/mac-svn.html nodejs https://nodejs.org/en/#download，官网下载安装 Node.js v10.10.0 to /usr/local/bin/node npm v6.4.1 to /usr/local/bin/npm 安装cnpm：http://npm.taobao.org # npm install -g cnpm --registry=https://registry.npm.taobao.org Visual Studio Code：轻量开发编辑器 FortiClient：SSLVPN客户端 PostMan：http测试工具 FileZilla：FTP、SFTP客户端 nginx：brew install nginx 影音工具 Xnip：长图截图工具 Irvue：自动切换壁纸 看图：腾讯出品的图片查看器 IINA：好用的视频播放器 Final Cut Pro：不解释 Movavi Screen Capture Studio 5：视频编辑 Kap：轻量的录屏软件 Resize Master：图片缩放工具 LICEcap：gip录屏软件。 windows虚拟机安装软件 360zip qq(主要是为了远程桌面) microsoft office microsoft project visio powerdesigner 16 VPN客户端 PanDown客户端：PanDownload 2.0.1，不限速，百度云下载加速器。下载： https://www.gdaily.org/15944/pandownload https://pandownload.com 哪些已经尝试了但放弃了 DEVONThink：https://sspai.com/post/44648 因为对中文的支持不太好，中文搜索与中文格式编码的支持不太好。 占用资源太大，大约2G内存。 使用Idex方式，无法与finder中的文件同步 2019-01-24 折腾了几个小时，发现用不起来，还是放弃。 Path Finder 因为没有什么特色，也没有比原生的finder更好用。放弃！ XtraFinder（启用SIP：https://www.jianshu.com/p/a47e96645d88） 因为需要启用SIP，怕影响原生系统，放弃了。 电池使用记录 2018-09-05 工作使用场景（开PD虚拟机） 日期 开始时间 结束时间 开始电量 结束电量 使用时间 2018-09-05 09:00 11:30 100% 55% 2.5小时 2018-09-05 13:30 15:00 50% 30% 1.5小时 2018-09-05 15:00 16:00 30% 10% 1小时 2018-09-05 16:00 16:23 10% 5% 0.5小时 Mac快捷键使用 MacOS command + Q 关闭APP command + Q 锁屏 mac中delete键的5种用法： 第一种：按 delete 键，实现 Windows 键盘上退格键的功能，也就是删除光标之前的一个字符（默认）； 第二种：按 fn+delete 键，删除光标之后的一个字符； 第三种：按 option+delete 键，删除光标之前的一个单词（英文有效）； 第四种：结合第二种，按住fn+option+delete，删除光标之后的一个单词； 第五种：选中文件后按 command+delete，删除掉该文件。 QQ command + M 最小化qq shift + command + W 打开联系人（自定义） shift + command + G 搜索联系人（自定义） ctrl + command + A 截图 ctrl + command + F 搜索联系人 Evernote 最好用的快捷键：https://www.ifanr.com/app/643581 command + J 启动快捷搜索框 ctrl + command + N 快捷笔记面板 印象笔记的markdown仅适合用来写文章。不能用快捷键返回和前进 返回和前进的快捷键：command + [ 返回 / command + ] 前进 删除线 ： ctrl + command + K ⌘ N 新記事 ⌥ ⌘ S 显示/隐藏侧边栏 全局快捷键 Ctrl ⌘ E 在 Evernote 中搜尋 Ctrl ⌥ ⌘ N 新記事視窗 Ctrl ⌘ N 快速記事 Safari浏览器切换标签快捷键（参考：https://sspai.com/post/30902，《Safari for OS X 你不可不知的 10 个快捷键》） 从左向右切换标签的快捷键是： Control + tab. 反向切换标签则是： Control + shift + tab. Option + tab/command + shift + tab 标签切换。 command + w 关闭当前标签 command + z 重新打开 command + [ 上一页 command + ] 下一页 command + shift + b 显示隐藏收藏栏。 command + shift + L 打开侧边栏 command + 单击链接，实现在后台打开新标签页。 command + R 刷新 Mac 自动操作 APP 设置 全局 command + \\ 打开finder shift + command + E 打开Evernote shift + command + W 打开QQ * Keyboard Maestro快捷键设置 option+command为打开Application的快捷键前缀 option+数字为最常用的文件夹与文件快捷键 Mac小技巧 如何从finder中文件夹进入terminal？ 打开Finder中的服务偏好设置，勾选快捷键tab下新建位于文件夹位置的终端窗口。 从finder到终端的小插件 cdto 查看Mac电量剩余使用时间 打开活动监视器-能耗标签，可以查看。 使用iStat Menus_6.20 mac系统监控神器，可以显示电池剩余时间。 安装jdk 安装目录：/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/bin/java 多版本jdk切换：https://blog.csdn.net/tianxiawuzhei/article/details/48263789 输入全角空格 按Shift+Option(Alt)+B组合键打开符号选择框，在底部选中“符号”选项，第六个就是。 参考：https://www.jianshu.com/p/d0ffea021315 mac上右键菜单新增文件夹打开方式 https://www.jianshu.com/p/d5f21b8b79e8 Mac下的【预览】工具增强插件： https://www.jianshu.com/p/9b9e53db22c9 Alfred使用 http://www.alfredworkflow.com https://sspai.com/post/44624 与文件相关的命令 find in open > 常用的一些自定义搜索配置。 百度：https://www.baidu.com/s?ie=utf-8&f=8&wd={query} 简书：http://www.jianshu.com/search?utf8=%E2%9C%93&q={query} 淘宝：http://s.taobao.com/search?oe=utf-8&f=8&q={query} 京东：https://search.jd.com/Search?keyword={query}&enc=utf-8&wq={query} 微信文章：http://weixin.sogou.com/weixin?type=2&query={query} stackoverflow：http://www.stackoverflow.com/search?q={query} github：https://github.com/search?utf8=%E2%9C%93&q={query} maven：http://mvnrepository.com/search?q={query} Android API Search：https://developer.android.com/reference/classes.html#q={query} Alfred + Evernote：https://www.jianshu.com/p/604d7519b6ed https://www.alfredforum.com/topic/840-evernote-workflow-9-beta-3/?tab=comments#comment-4055 http://alfredworkflow.com/ 推荐软件 https://github.com/hzlzh/Best-App/blob/master/README.md ChangeLog 20190609 | 转移到GitBook 20180904 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-11 15:26:28 "}}