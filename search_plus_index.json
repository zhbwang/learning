{"./":{"url":"./","title":"前言","keywords":"","body":"前言 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-04-03 20:00:24 "},"20190703SuperiorImformationSrc.html":{"url":"20190703SuperiorImformationSrc.html","title":"优质信息来源","keywords":"","body":"优质的信息来源 技术博客 ThoughtWorks洞见 前端 | 张鑫旭的个人主页 » 张鑫旭-鑫空间-鑫生活 工作与学习方法 電腦玩物 ChangeLog 20190704 | 增加「电脑玩物」 20190703 | 创建文档，持续补充 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-04 21:33:50 "},"20190622EfficientLife.html":{"url":"20190622EfficientLife.html","title":"如何实现高效人生——打造自己的效率体系","keywords":"","body":"如何实现高效人生——打造自己的效率体系 从2011年开始，经历了仅八年的不断尝试与迭代，逐渐打造出一套完整的效率体系。 这套体系包括：时间管理、反思复盘、输入管理、输出管理、精力管理等。 效率体系的每一个部分都不是孤立存在的，而是一个彼此协同、互相补充的一个完整体系，本文先从时间记录说起，后续会逐步完善。 时间记录 为什么要时间记录 时间记录的鼻祖 柳比歇夫。 时间记录方法论 时间记录实践 工具选择 工具并不是最重要的，每个人使用工具都不一样，没有方法论的指导，工具仅仅是一个工具，不能与你产生化学反应，也就不能产生非同寻常的影响。 我对工具的选择，基于几个原则： 工具要精简，不要太多，尽量能找到一个在多个方面都有用的工具。 要简单，使用起来要方便能够通过各种快捷键高效使用 全平台同步，随时随地查看。 我在时间记录方面主要用到的工具 滴答清单 与 Numbers。 滴答清单 是一款任务管理工具，我用滴答清单主要场景是未来确定日程的安排、当天的工作安排、每天的时间记录，主要用到的是滴答清单中的日历功能（滴答清单日历功能需要付费）。 Numbers 是 Mac 中类似 Excel 的表格工具。年初的时候，我使用 Numbers 设计了一个目标管理与时间统计分析的系统，具体如何设计的后续再说，这里先介绍使用 Numbers 如何进行时间统计分析。 时间记录准备工作 时间规划 时间记录有两个最主要的目的： 一是看看自己的时间是如何利用的，哪些时间是自己的高效时间，哪些时间是自己最容易分心的时候，哪些事情花费的时间多，在工作和学习上投入了多少时间，一天有多少时间是浪费的。 二是看看与自己的时间规划比较是否有出入，一件事预计2个小时完成，实际到底用了多少时间；原来计划完成什么事，实际是否安排时间去做了，还是因为种种原因拖延了。 一开始，可以只是记录自己的时间，看看时间的使用情况。当逐步认识到自己的时间是怎么利用的，就要开始进行时间规划了。 建议按周来规划，比如说学习一个技术，在某个项目上面花费多少时间，提前规划。 每周可以按照60小时进行规划，工作上面花费多少时间，计划与总结花费多少时间，学习花费多少时间等等。具体分类一会再说。 当然60小时这个时间每个人也不一样，可以通过一周的时间记录来分析，看看每天大约有多少时间是可以利用的。 时间记录分类 接下来说一下，时间记录如何分类。 刚刚我们知道，时间记录是用于分析时间分配是否合理，那么时间应该分配到什么地方，可以有很多分类方法，可以按照工作、学习、生活来分，也可以按照项目来分。 下面我推荐一个平衡生活的分类方法： 职场工作——在工作上花费的时间，主要是需要自己发挥才能，为公司或者他人产生价值的事情。 技能提升——与工作相关的提升，包括自己的工作能力、眼界，比如学习技术知识、了解行业趋势等。 个人成长——知识、能力、眼界、心灵的成长，都是个人成长，制定计划与反思总结也属于个人成长。 自我实现——也许与工作无关，但是能发挥你的天赋，实现你价值的事！比如练习书法等。 个人健康——身体、心理健康方面，比如太极拳、跑步、爬楼梯、冥想等。 家庭与生活——有哪些事情是跟家人一起做的。 人际沟通——那些不是亲人的朋友，交流碰撞，这些都是宝贵的财富。 休闲与娱乐——比如看电影、听音乐等。 浪费时间——纯粹的浪费时间，比如玩抖音、玩电子游戏、看一些无聊的网页等。 开始时间记录 时间记录规范 先看一个例子，这是我写效率系统方面的文章时，在滴答清单「日历」中的时间记录，属于「个人成长」清单。 写作 | 效率系统总结，时间记录部分——60分钟(09:45=10:45) 滴答清单导出摘要格式为： - [6月22日] 写作 | 效率系统总结，时间记录部分——60分钟(09:45=10:45) 这样，一个标准的时间记录包括六个元素： 6月22日：每一条时间记录的日期。 写作 ：时间记录的标签。有几个关键的标签可以用来做习惯分析。比如「自我实现」中的「书法」、「太极拳」；「个人成长」中的「听书」、「阅读」、「笔记」、「写作」都可以用来做习惯分析。「休闲与娱乐」中的「电视」、「电影」等。 效率系统总结，时间记录部分：每个事项的内容。 60分钟：每件事所花费的时间，以「分钟」为单位。 09:45=10:45：起止时间。 个人成长：事项所属大类。 可以看到一个时间记录中的各个元素都可以通过确定的符号分割。 为什么要这么设计？接下来会说到，通过「滴答清单」中的「摘要」功能可以导出一整天的时间记录，可以直接拷贝到 Numbers 工具中，并通过符号分割来识别每个元素，进行统计分析。 睡眠的时间是通过标签「睡眠」来完成，并放置在「个人健康」分类中： 睡眠 | 睡眠时间不足(00:30=06:30) # 滴答清档摘要 - [6月22日] 睡眠 | 睡眠不足(00:30=06:30) 可以看到，这里面没有增加花费的时间，在 Numbers 中对「睡眠」有特殊处理，根据「睡眠」、「午休」标签单独分析，获取「早起时间」、「睡眠时长」。 PS： 为了更方便的记录，我还设计了两种其他记录规范，不属于时间范畴，但是对我来说也比较重要，也是在「滴答清单」中记录，导入到 Numbers 系统中。分别是健康状态记录、心理状态记录。 运动数据记录： # 填写 健康 | 活动能量513；步数5014；站立14；爬楼17；锻炼33；体重68.1== # 滴答清单摘要： - [6月22日] 健康 | 活动能量513；步数5014；站立14；爬楼17；锻炼33；体重68.1== 健康状态记录： # 填写 状态 | 天气3；健康3；自信3；情绪3== # 滴答清单摘要 - [6月22日] 状态 | 天气3；健康3；自信3；情绪3== 填写按照以下顺序： - 天气：1雾霾2雨雪3阴天4多云5天气晴空气好 - 健康：1身体重病2轻微感冒3一般4良好5有精神 - 自信：1极度不自信2不太自信3一般4自信5做了一件让我非常自信的事情 - 情绪：1非常不好2情绪低落3一般4情绪饱满5情绪高涨 滴答清单记录 首先在「滴答清单」中创建清单类型，如上介绍的九中类型。 选择「日历」，就会展示一天的所有时间段，这就是时间记录区。 点击任何时间段，弹出事件记录框，就可以记录时间了。时间记录格子的最小单位是 30 分钟，通过拖动格子的上下边缘，可以调整时间范围，调整的最小单位是 15 分钟。 一天结束以后，可以在「滴答清单」的「摘要功能」查看今天的时间记录表。 时间统计分析 接下来就可以基于滴答清单的摘要拷贝到 Numbers 中进行时间分析了，这里我使用 Numbers 中的公式来进行内容识别的。 首先将摘要拷贝到 Numbers 中，只需要拷贝到「行动名称」栏。可以看到 Numbers 的表格中已经能把很多信息都自动识别出来了。 只要确定了时间记录规范，识别都比较简单，比如识别「行动类别」，使用的公式如下： =IF(ISBLANK(E2),\"\",MID(E2,FIND(\"\",E2,开始位置)−FIND(\" 分类时间汇总统计 接下来，可以针对当天进行汇总统计了。通过不同的行动分类，累加当天所花费的时间，可以统计不同类型总的时间花费情况；也可以根据指定标签进行习惯方面的分析。 习惯分析。根据不同的标签来进行习惯分析，比如「书法」、「阅读」等。 早起分析。如果早起时间在规定的时间之前，就算是早起了。 睡眠时间统计。根据早起时间与晚睡时间以及午休时间，可以计算出总的睡眠时长。 不同类型的时间统计。根据不同「行动分类」，统计出各个类型的时间汇总情况。 同样，我们再对本周、本月的时间进行汇总统计。只要将本周每天、本月每周的统计数据进行汇总，就可以自动得到汇总数据。 项目时间汇总统计 上述说的是针对不同分类的时间汇总统计，但这还远远不够。我们还需要知道哪些计划要做的事情，有没有花时间去做，实际上有没有投入足够的时间。 这就需要有一个规划的工作。 这部分的分析，需要先对本月进行一个总的规划，列举出本月想做的一些项目。 时间记录复盘总结 参考资料 ChangeLog 20190622 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-26 13:56:05 "},"20190606BuildKnowledgeBase.html":{"url":"20190606BuildKnowledgeBase.html","title":"如何建立自己的知识库-未完待续","keywords":"","body":"如何建立自己的知识库 知识库建立原则 使用知识的三层模型构建自己的知识库：卡片、文件、项目。 知识库建立最终极的目的是生成一套知识体系，提升生成新知识的能力，完成知识创造的伟大历程。 知识库类型。 知识库包括三大类：工作类、学习类、生活类 工作类包括一切公司内部公共知识与产品文档、项目文档等。 学习类包括一切通过学习获取的知识，包括技术学习、阅读书籍、听书、浏览文章等。这些都是已有的形式知识，需要通过学习总结不断内化为自己的暗隐知识，从而生成新的知识。 生活类主要是记录生活的重要事情，比如：买车买房的过程、日记等。 知识库操作指南 知识库的建立少不了工具的支持，工具的选择必须要精简，减少工具使用过程中不断做选择的精力耗散。 收集过程：使用Evernote、滴答清单进行收集。使用WorkFlowy也可以进行快速记录。 卡片类：所有的卡片全部记录在WorkFlowy中。 文件类：文件的目的是为了分享，所以每一篇文章都需要具备成果意识。用Markdown来写，使用MWeb工具。 学习与生活类文件的组织逻辑：放在 gitbook.zhbwang.com 域名文件夹中。 文件层级尽量不超过两层。比如学习类文件全部放在一个文件夹 wiki@learning 中；生活类文件全部放在一个文件夹 wiki@life 中。这种方法有一个很大的好处，就是所有文章都在一个文件夹，能形成合力。 在文件夹中新建 drafts 文件夹，临时草稿区。根目录中就是已经定稿的。 工作类的文件暂时还没有想好。 gitbook.zhbwang.com 域名文件夹使用坚果云存储。 项目类：项目是一个成果，比如一本书，一个产品，或者一个完成的产品文档。如果说把所有资料都丢掉，只能保留很小的一部分，那么能保留的部分就是这个项目类的成果了。所以项目类的成果一定要精简，并且有结构。 在文件类，已经将一篇一篇的文章写好了，那么项目就要使用README或者SUMMARY文件驱动，将文章形成有结构的一本书，或者一个体系。 这部分也同样使用 GitBook 完成。 在另一个目录新建 GitBook 文件夹，将生活类文件夹、学习类文件夹建立一个软连接到 GitBook 文件夹，写个脚本，自动编译为 html，本机中的 nginx 代理，通过 http://localhost 即可访问。 可以使用 Fluid 将 http://localhost 网址转换为APP的形式，可以很方便的打开。 下一步思考：工作类文件应该如何处理？ 参考资料 ChangeLog 20190606 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-29 08:46:03 "},"20190625WikiUserGuide.html":{"url":"20190625WikiUserGuide.html","title":"知识库使用手册","keywords":"","body":"部门知识库使用手册——用户版 部门知识库旨在团队协作中的信息传递、知识共享与知识沉淀。为了大家更方便的使用知识库，本文将介绍部门知识库系统的使用方法，为大家提供全方位的知识库使用指南。 走马观花看知识库 先来一个系统全揽图： 导航区。知识库目录导航，可以快速进入一个文档。导航区还有一个增加页面的功能，由此出增加的页面默认是放在 99_临时区 里面。 工具区。工具包括： 可打印版本：打印预览 反向链接：查看本文档的反向链接 最近更改：查看整个知识库的最近更新情况 媒体管理器：查看整个知识库的图片库 永久链接：查看当前文档版本的永久链接。 引用此文：查看知识库中引用当前文档的其他文档。 用户信息。登录后的用户信息与操作。 文档操作区。可以针对当前文档进行创建、编辑、查看修订记录等操作。可以发表讨论。 公告区。文档库全局公告。 文档正文。包括正文与目录。 历史足迹。登录用户的浏览足迹。 最后更改。包括文档最近更新时间与更新用户。 迫不及待创建文档了 您可以通过两种方式来创建文档：导航栏创建、地址栏创建 导航栏创建 在知识库系统左侧导航栏下方有增加页面输入框，可以直接在这里输入文档名称，点击 增加页面，即可进入文档编辑器，编辑文档完成后，点击 保存 按钮即可增加文档。 注意：通过这种方式创建的文档默认放在 99_临时区 目录中。 地址栏创建 如果您需要在其他目录创建文档，这里将介绍一种更高级的方法——地址栏创建。 我们先看一下一个文档的地址是什么样的。 http://192.168.129.137:6200/doku.php?id=dicwiki:99_临时区:示例 这是一个示例文档的URL地址。包括两大部分： http://192.168.129.137:6200/doku.php 所有文档的地址都是这样的，只是参数 id 不一样。 id=dicwiki:99_临时区:示例，文档唯一标识。标识 dicwiki/99_临时区 目录下的名字叫做 示例 的文档。 如果你希望在 11_产品区 创建一个文档 产品开发环境说明，你就可以在浏览器地址栏输入： http://192.168.129.137:6200/doku.php?id=dicwiki:11_产品区:产品开发环境说明 这个时候页面会提示：该主题尚不存在，你可以点击文档操作区中的 创建 按钮，即可进入编辑区，输入文档内容，点击保存即可。 文档中引用图片 编辑文档时，图片的引入比较简单，通过截图工具截图后，在编辑器中 Ctrl + V 即可实现图片的自动上传与引入。 上传后图片链接如下： {{:dicwiki:01_新手入门:pasted:20190625-101334.png}} 此时图片大小为原始大小，容易造成文档变形，推荐改为 # 居中，宽度600，点击图片查看原始图片。 {{ :dicwiki:01_新手入门:pasted:20190625-101334.png?direct&600 }} 其他语法说明： # 原始大小，不推荐，会造成文档样式变形。 {{:dicwiki:01_新手入门:pasted:20190625-101334.png}} # 前面加空格、后面加空格表示居中对齐。 {{ :dicwiki:01_新手入门:pasted:20190625-101334.png?direct&600 }} # 前面加空格、后面不加空格，表示图片右对齐 {{ :dicwiki:01_新手入门:pasted:20190625-101334.png?direct&600}} # 前面与后面都不加空格，表示图片左对齐 {{:dicwiki:01_新手入门:pasted:20190625-101334.png?direct&600}} # 前面不加空格，后面加空格，表示图片嵌入到文字中 {{:dicwiki:01_新手入门:pasted:20190625-101334.png?direct&600 }} 当然你也可以通过媒体库引入图片，通过选择大小、位置、嵌入方式，可以自动生成图片引用。 上传文档附件 文档附件的上传与引用，比较复杂。您必须先点击编辑器中的媒体管理器按钮，打开媒体管理器。 选择文件上传后， 点击文档接口自动插入到编辑器中。 {{ :dicwiki:01_新手入门:pasted:20190625-101356.png?600 |}} 增加文档评论 建议在文档最后增加允许评论代码： ~~DISCUSSION~~ 保存文档后，会在文档正文下方出现评论栏，其他同事就可以在这里跟你一起讨论啦~ 评论区也支持Markdown语法。 创建文档注意事项 文档必须按照目录规范放在指定目录，如果需要移到其他目录，可以联系管理员操作。 文档名称必须要能让其他同事一看就明白，文档名称不宜太长。 文档编辑推荐使用Markdown语法，部分高级功能可以使用dokuwiki语法。 文档编写务必按照[知识库文档规范]编写，增加ChangeLog说明。 文档不完善，我要补充！ 无论是谁创建了文档，任何人都有完善补充文档的责任，如果您发现其他人的文档有遗漏，或者自己有更好的想法，您完全可以对该文档进行编辑。 您有两种方式可以编辑文档。 点击文档操作区的 编辑 按钮对整个文档进行编辑 点击文档正文右侧的 编辑 按钮，可以有针对性的对某一段进行编辑 很重要：编辑文档一定要在文档 ChangeLog 部分增加更新时间、姓名、描述。 文档过时了，要删掉？ 删除文档的操作比较简单，只要编辑文档，把文档内容全部删除，改文档也删除了。 如果当前目录也没有文件了，那么当前目录也就删除了。 谁动了我的文档？ 点击文档操作区的 修订记录，即可查看该文档的修订记录，可以看到谁在什么时候修改了文档。 选中两个版本，还能对比文档修改内容。 有没有更方便的文档写作工具？ 不推荐直接在编辑器中编辑文档，毕竟属于在线编辑，一旦浏览器关闭，曾经编辑过的内容就化为乌有，想哭都来不及。 所以推荐两个非常好的Markdown写作工具： Typora （支持Windows/Mac） MWeb （仅支持Mac） ChangeLog 20190625 | 汪志宾 | 增加图片链接详细说明 20190625 | 汪志宾 | 新建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-03 19:22:56 "},"20190625WikiDocSpec.html":{"url":"20190625WikiDocSpec.html","title":"知识库文档规范","keywords":"","body":"知识库文档撰写指南 知识库文档作为团队协作时传递信息的主要媒介，撰写时应当以有助于团队其他成员顺利使用为目的，以便其他成员能依据文档顺利开展工作，减少沟通成本。 知识库文档会经由多位同事补充完善，故需共同遵守约定的规范，本文将介绍知识库文档撰写指南，供大家在创建、修改文档时参考，方便团队协作。 本文将包括三个部分： 知识库文档类型，在本知识库中可以写哪些文档。 知识库文档结构，一篇好的文档所包含的几个关键篇章。 知识库文档命名规范。 文档类型 知识库的目的是为了方便团队之间的信息传递、知识共享以及团队知识沉淀。故在知识库中创建文档必须以有利于上述目的为原则。 以下试列举常见的知识库文档类型： 新手入门。包括知识库使用说明、部门新员工需要了解的规章制度、OA使用指南、新员工在开发产品时所需的工具与环境搭建、文档写作指南、通知公告等。 产品文档。产品介绍文案、产品文档、产品演示环境、开发环境信息等。 团队协作文档。各团队内部的协作相关文档。 技术文档。在产品研发、学习探索过程中学习的技术知识、遇到的技术问题，都可以记录下来与大家分享。 通识文档。工作中所用到的通用能力学习心得、经验分享等。 临时文档。如果你有好的想法，或者文档不属于上述任何一类，可以放到临时区，以便共同完善文档类型。 注：建议不要完全照抄网络上的文章，如果参考了网络文章，至少要有自己的心得与思考，并附上网络链接。 文档结构 本文档结构大部分参考了 协作文档撰写指南。可随实际情况持续更新。 知识库文档结构一般包括五大部分： 头部说明。文档背景、适用情境等有助于队友正确使用本文档的信息。 正文。文案、规约、纪要、技术、文档等内容的正文 NEED 。要达成文档内容所驱动事务的目的，需要谁做什么（若无可忽略）。也不一定汇总到一个板块，在正文中及时说明也 OK。 参考资料。有助于队友更好地使用本文档的资料链接 ChangeLog。记录文档修订过程 头部说明 常规要求 一般来说，应该在文档第一行就明确文档的性质：文案？规约？记要？策划？ 技术？心得？... 不同性质的文档头部说明有不同的侧重点，但总体来说，需要给出文档的背景信息，比如： 简要说明背景要素和提出正文的原因，即给出文档的 5W1H ，比如本文第一段头部说明。 说明方式可以是简要描述，也可以链接相关 Wiki 文档。 以便文档使用者知道原因、进展，根据已有内容获取足够信息以判断文档内容是否 OK ，并给出建设性建议。 各性质文档头部说明侧重点 若正文是规约/手册，头部说明一般为 适用人员 适用情境/环境配置建议等 若正文是纪要，头部说明一般为 背景，也就是为什么会有这次沟通 参与者，以便其他看到文档的人知晓这些是谁达成的共识，了解谁（不在现场）不一定知晓 沟通发生的时间时长地点，以便知晓是在哪里发生的 录音链接（我们公司沟通录音一般上传到公用百度云账号） 若正文是技术文档，头部说明一般为 技术引入场景，本技术能解决什么问题 遇到的问题 技术的一句话介绍。 本技术文档适用的环境 ... 若正文是产品设计，头部说明一般为 需求来源与时间。 功能描述 需求场景说明。 ... 若正文是通用能力实践心得，头部说明一般为 背景，为什么会有这样一次实践 适用人员 总之，一定避免没有任何背景信息介绍，一开篇直接是正文的情况，尤其需要发给别人修订、审核的文案。毕竟这样的无头文档容易让看到的人摸不着头脑，不清楚这个文档是否适合自己/当下情况使用，甚至在无正确理解的情况下进行下一步操作。 文档正文 展开文档正文，该怎么写就怎么写吧。 分段落写，每一段落一个标题，避免长篇大论，没有小标题。 如果是形成了新的规约，一般要及时更新同步到相关规约文档中。 文档持续更新，删除过期的文档，保留最新共识 NEED 列出需要谁配合做什么来完善这个文档/推动这个事情。通常列成 checklist 的方式。 checklist 注意描述清楚 who/when/what，并在微信群或项目管理平台等协作界面知会对方 尽可能给出成果链接地址。比如SVN地址。 可以由挖坑的人给出链接地址，以便填坑人顺利填坑 也可由填坑人按照团队共识上传成果给出地址，以便其他同伴使用该成果 若没有，可不写 参考资料 列出文档中涉及到的相关资料出处，以便相关参考资料内容变化后看到该文档的同事也能及时看到最新版本。 ChangeLog 记录修订过程，一般为 日期+人员+动作 。举例： 20190625 | 汪志宾 | 新建文档 请一定注明创建人，以便日后有疑问时方便追踪源头求解清楚 使用倒序，以便一眼看到最近更新 命名规范 以上主要为协作时创建文档的内容建议，不过命名更是关键：它看似简单，实际体现了对文档在知识库体系中的理解深度。还得慢慢琢磨。 好的命名能让队友一眼识别文件内容需要时及时调用。所以创建文档命名的时候，首先要确定一个基本原则，就是问自己一个问题： 这样命名，我队友能快速调取他要的文件吗，一年/三年/五年后还能吗？ 接下来提几点关于命名的具体规约： 追求不必改动 尽量选用基本层次范畴的词汇。比如： 在CentOS中安装Docker VS Docker安装指南 如果文档限定比较多，把文档限定在Docker、CentOS之间。那么如果要在Mac 、Windows中安装Docker，或者在CentOS 7 中安装与在 CentOS 6 中安装呢？是不是还要再创建一个文档。 如果一开始，就以「Docker安装指南」命名，在各种环境安装的过程都可以放在这个文档里面。 同一主题内容持续积累到同一指定文档。 协作越久文档越多，不想信息过载，最好一个场景只保留唯一可用文档，大家都只更新这唯一可用文档。否则内部必会混乱，或维护者不知还有其它版本需要更新，或使用者所用并非新版，烦恼不断。 此时，提前长远考虑你创建的文件/文档解决什么问题、怎么命名更为重要。 比如关于SpringBoot的开发常见问题解决都放在一个文档，而不是SpringBoot内存问题单独一个文档，SpringBoot配置问题单独一个文档。 当然如果一个主题内容积累比较多，可以考虑是否需要拆分，一旦拆分，就涉及到需要创建一个主题索引文档，将该主题相关文档链接都汇集到一起。 请勿出现「@」「#」「:」「.」等符号 传递链接的微信/企业微信等 IM 对话窗口比较特殊，无法将 @ 符号后头的内容自动识别为链接。所以，为了不给别人添麻烦 —— 比如把链接复制到浏览器中才能打开，请你命名文件时尽量不带 @、# 等特殊字符。 可以使用「_」来间隔。 不建议命名中同时出现 「@」 和 「.」。有可能会被当成邮箱地址。 尽可能增加页面间的连接，不生产孤悬页面 目前正在试用阶段，每个文档的页面都会在左侧导航栏展示，如果文档积累比较多，建议每个文档都通过链接关联到其他文档，左侧导航栏仅保留关键的索引文档。 参考资料 协作文档撰写指南 团队文档命名指南 ChangeLog 20190625 | 汪志宾 | 新建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-26 11:24:47 "},"20190515HadoopQuickStart.html":{"url":"20190515HadoopQuickStart.html","title":"让Hadoop在MacOS上跑起来","keywords":"","body":"让Hadoop在MacOS上跑起来 大数据学习之路系列01 本安装文档是在MacOS中安装单机版Hadoop。 安装目录 WZB-MacBook:50_bigdata wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata 准备工作 JDK Mac安装JDK的过程略，参考：MAC下安装多版本JDK和切换几种方式 WZB-MacBook:50_bigdata wangzhibin$ java -version java version \"1.7.0_80\" Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) WZB-MacBook:50_bigdata wangzhibin$ echo $JAVA_HOME /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 下载Hadoop brew install wget wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/core/hadoop-2.8.4/hadoop-2.8.4.tar.gz WZB-MacBook:50_bigdata wangzhibin$ tar -zxvf hadoop-2.8.4.tar.gz 安装与配置Hadoop 修改JDK配置 WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hadoop-env.sh export JAVA_HOME=${JAVA_HOME}改为 export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 验证Hadoop WZB-MacBook:hadoop-2.8.4 wangzhibin$ bin/hadoop Usage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar run a jar file note: please use \"yarn jar\" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive classpath prints the class path needed to get the credential interact with credential providers Hadoop jar and the required libraries daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settings Most commands print help when invoked w/o parameters. 单机模式执行 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar grep input output 'dfs[a-z.]+' $ cat output/* 1 dfsadmin 配置core-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ mkdir -p hdfs/tmp WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/core-site.xml 增加如下配置： hadoop.tmp.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp Abase for other temporary directories. fs.defaultFS hdfs://localhost:9000 配置hdfs-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hdfs-site.xml 增加如下配置： dfs.replication 1 dfs.namenode.name.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name dfs.datanode.data.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/data 启动与停止Hadoop 配置.bash_profile # set hadoop export HADOOP_HOME=/Users/wangzhibin/00_dev_suite/50_bigdata/hadoop export PATH=$PATH:$HADOOP_HOME/bin 第一次启动hdfs需要格式化 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./bin/hdfs namenode -format ... 19/05/15 22:30:47 INFO common.Storage: Storage directory /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name has been successfully formatted. ... 启动HDFS ./sbin/start-dfs.sh 停止HDFS ./sbin/stop-dfs.sh HDFS启动状态查看 HDFS 状态：http://localhost:50070/dfshealth.html#tab-overview Secordary NameNode 状态：http://localhost:50090/status.html 本地官方文档：API文档 验证HDFS 简单的验证hadoop命令： $ hadoop fs -mkdir /test WZB-MacBook:hadoop wangzhibin$ hadoop fs -ls / Found 1 items drwxr-xr-x - wangzhibin supergroup 0 2019-05-16 11:26 /test 启动时遇到的坑 一、sh: connect to host localhost port 22: Connection refused 此时可能会出现如下错误。是因为没有配置ssh免密登录。 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:38:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: ssh: connect to host localhost port 22: Connection refused localhost: ssh: connect to host localhost port 22: Connection refused Starting secondary namenodes [0.0.0.0] 0.0.0.0: ssh: connect to host 0.0.0.0 port 22: Connection refused 19/05/15 22:38:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 采用如下方法解决： 1）解决方法是选择系统偏好设置->选择共享->点击远程登录 2）设置免密登录 $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys $ ssh localhost 二、Unable to load native-hadoop library for your platform WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:50:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: starting namenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-namenode-WZB-MacBook.local.out localhost: starting datanode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-datanode-WZB-MacBook.local.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out 19/05/15 22:50:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 参考： 官方-Native Libraries Guide Mac OSX 下 Hadoop 使用本地库提高效率 Hadoop native libraries: Installation on Mac Osx 解决方案：重新编译hadoop，将编译后的hadoop-dist/target/hadoop-2.8.4/lib/native替换$HADOOP_HOME/lib/native。 安装基础组件 $ brew install gcc autoconf automake libtool cmake snappy gzip bzip2 zlib 安装protobuf。 wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz tar zxvf protobuf-2.5.0.tar.gz cd protobuf-2.5.0 ./configure make make install 重新编译hadoop wget http://apache.fayea.com/hadoop/common/hadoop-2.8.4/hadoop-2.8.4-src.tar.gz tar zxvf hadoop-2.8.4-src.tar.gz cd hadoop-2.8.4-src mvn package -Pdist,native -DskipTests -Dtar -e cp -r /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-dist/target/hadoop-2.8.4/lib/native . 三、An Ant BuildException has occured: exec returned WZB-MacBook:hadoop-2.8.4-src wangzhibin$ mvn package -Pdist,native -DskipTests -Dtar -e ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 [ERROR] around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml [ERROR] -> [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml 参考：mac下编译Hadoop 2.8.1报错An Ant BuildException has occured: exec returned: 1，排错过程 解决方案：配置环境变量OPENSSL_ROOT_DIR、OPENSSL_INCLUDE_DIR。修改~/.bash_profile # openssl export OPENSSL_ROOT_DIR=/usr/local/Cellar/openssl/1.0.2r export OPENSSL_INCLUDE_DIR=$OPENSSL_ROOT_DIR/include 配置与启动yarn 配置mapred-site.xml cd $HADOOP_HOME/etc/hadoop/ cp mapred-site.xml.template mapred-site.xml vim mapred-site.xml mapreduce.framework.name yarn 配置yarn-site.xml vim yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle yarn启动与停止 启动 cd $HADOOP_HOME ./sbin/start-yarn.sh ./sbin/stop-yarn.sh 浏览器查看：http://localhost:8088 jps查看进程 WZB-MacBook:hadoop wangzhibin$ jps 534 NutstoreGUI 49135 DataNode 49834 ResourceManager 49234 SecondaryNameNode 49973 Jps 67596 49912 NodeManager 49057 NameNode 到此，hadoop单机模式就配置成功了！ 命令与验证 Resource Manager: http://localhost:50070 JobTracker: http://localhost:8088/ Node Specific Info: http://localhost:8042/ Command $ jps $ yarn // For resource management more information than the web interface. $ mapred // Detailed information about jobs Hadoop目录 WZB-MacBook:hadoop wangzhibin$ tree -L 2 . . ├── LICENSE.txt ├── NOTICE.txt ├── README.txt ├── bin │ ├── container-executor │ ├── hadoop │ ├── hadoop.cmd │ ├── hdfs │ ├── hdfs.cmd │ ├── mapred │ ├── mapred.cmd │ ├── rcc │ ├── test-container-executor │ ├── yarn │ └── yarn.cmd ├── etc │ └── hadoop ├── hdfs │ └── tmp ├── include │ ├── Pipes.hh │ ├── SerialUtils.hh │ ├── StringUtils.hh │ ├── TemplateFactory.hh │ └── hdfs.h ├── lib │ ├── native │ └── native-bak ├── libexec │ ├── hadoop-config.cmd │ ├── hadoop-config.sh │ ├── hdfs-config.cmd │ ├── hdfs-config.sh │ ├── httpfs-config.sh │ ├── kms-config.sh │ ├── mapred-config.cmd │ ├── mapred-config.sh │ ├── yarn-config.cmd │ └── yarn-config.sh ├── logs │ ├── SecurityAuth-wangzhibin.audit │ ├── hadoop-wangzhibin-datanode-WZB-MacBook.local.log │ ├── hadoop-wangzhibin-datanode-WZB-MacBook.local.out │ ├── hadoop-wangzhibin-datanode-WZB-MacBook.local.out.1 │ ├── hadoop-wangzhibin-namenode-WZB-MacBook.local.log │ ├── hadoop-wangzhibin-namenode-WZB-MacBook.local.out │ ├── hadoop-wangzhibin-namenode-WZB-MacBook.local.out.1 │ ├── hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.log │ ├── hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out │ ├── hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out.1 │ ├── userlogs │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.log │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.out │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.out.1 │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.out.2 │ ├── yarn-wangzhibin-nodemanager-WZB-MacBook.local.out.3 │ ├── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.log │ ├── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.out │ ├── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.out.1 │ ├── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.out.2 │ └── yarn-wangzhibin-resourcemanager-WZB-MacBook.local.out.3 ├── sbin │ ├── distribute-exclude.sh │ ├── hadoop-daemon.sh │ ├── hadoop-daemons.sh │ ├── hdfs-config.cmd │ ├── hdfs-config.sh │ ├── httpfs.sh │ ├── kms.sh │ ├── mr-jobhistory-daemon.sh │ ├── refresh-namenodes.sh │ ├── slaves.sh │ ├── start-all.cmd │ ├── start-all.sh │ ├── start-balancer.sh │ ├── start-dfs.cmd │ ├── start-dfs.sh │ ├── start-secure-dns.sh │ ├── start-yarn.cmd │ ├── start-yarn.sh │ ├── stop-all.cmd │ ├── stop-all.sh │ ├── stop-balancer.sh │ ├── stop-dfs.cmd │ ├── stop-dfs.sh │ ├── stop-secure-dns.sh │ ├── stop-yarn.cmd │ ├── stop-yarn.sh │ ├── yarn-daemon.sh │ └── yarn-daemons.sh └── share ├── doc └── hadoop 参考资料 Hadoop: Setting up a Single Node Cluster. centos7 hadoop 单机模式安装配置 Hadoop in OSX El-Capitan Installing Hadoop on Mac OS X 10.9.4 macOS上搭建伪分布式Hadoop环境 本地官方API文档 ChangeLog 20190611 | 增加Hadoop目录结构 20190515 | 创建文档，已发布博客：腾讯云社区、CSDN博客、语雀。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-03 18:45:05 "},"20190517FirstMapreduceProgram.html":{"url":"20190517FirstMapreduceProgram.html","title":"第一个MapReduce程序","keywords":"","body":"第一个MapReduce程序 大数据学习之路系列02 目标 单词计数是最简单也是最能体现 MapReduce 思想的程序之一，可以称为 MapReduce 版“Hello World”。 单词计数主要完成功能是：统计一系列文本文件中每个单词出现的次数，如下图所示。 准备工作 新建目录 WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/input WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/output WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/input drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 准备文件 WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file1.txt Hello World WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file2.txt Hello Hadoop WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 14:50 /practice/20190517_mr/input -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 14:49 /practice/20190517_mr/input/file1.txt -rw-r--r-- 1 wangzhibin supergroup 13 2019-05-17 14:49 /practice/20190517_mr/input/file2.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 运行例子 在集群上运行 WordCount 程序 备注:以 input 作为输入目录，output 目录作为输出目录。 已经编译好的 WordCount 的 Jar 在“$HADOOP_HOME/share/hadoop/mapreduce/”下面，就是“hadoop-mapreduce-examples-2.8.4.jar”， MapReduce 执行过程显示信息 执行命令： hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 执行过程： WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:39:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:39:20 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558078701666_0002 19/05/17 15:39:20 INFO impl.YarnClientImpl: Submitted application application_1558078701666_0002 19/05/17 15:39:20 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1558078701666_0002/ 19/05/17 15:39:20 INFO mapreduce.Job: Running job: job_1558078701666_0002 19/05/17 15:39:28 INFO mapreduce.Job: Job job_1558078701666_0002 running in uber mode : false 19/05/17 15:39:28 INFO mapreduce.Job: map 0% reduce 0% 19/05/17 15:39:33 INFO mapreduce.Job: map 100% reduce 0% 19/05/17 15:39:39 INFO mapreduce.Job: map 100% reduce 100% 19/05/17 15:39:39 INFO mapreduce.Job: Job job_1558078701666_0002 completed successfully 19/05/17 15:39:39 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=55 FILE: Number of bytes written=474472 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=271 HDFS: Number of bytes written=25 HDFS: Number of read operations=9 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=2 Launched reduce tasks=1 Data-local map tasks=2 Total time spent by all maps in occupied slots (ms)=6213 Total time spent by all reduces in occupied slots (ms)=2848 Total time spent by all map tasks (ms)=6213 Total time spent by all reduce tasks (ms)=2848 Total vcore-milliseconds taken by all map tasks=6213 Total vcore-milliseconds taken by all reduce tasks=2848 Total megabyte-milliseconds taken by all map tasks=6362112 Total megabyte-milliseconds taken by all reduce tasks=2916352 Map-Reduce Framework Map input records=2 Map output records=4 Map output bytes=41 Map output materialized bytes=61 Input split bytes=246 Combine input records=4 Combine output records=4 Reduce input groups=3 Reduce shuffle bytes=61 Reduce input records=4 Reduce output records=3 Spilled Records=8 Shuffled Maps =2 Failed Shuffles=0 Merged Map outputs=2 GC time elapsed (ms)=97 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=603979776 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=25 File Output Format Counters Bytes Written=25 查看结果 WZB-MacBook:hadoop wangzhibin$ hadoop dfs -ls -R /practice/20190517_mr/output/ -rw-r--r-- 1 wangzhibin supergroup 0 2019-05-17 15:39 /practice/20190517_mr/output/_SUCCESS -rw-r--r-- 1 wangzhibin supergroup 25 2019-05-17 15:39 /practice/20190517_mr/output/part-r-00000 WZB-MacBook:hadoop wangzhibin$ hadoop fs -cat /practice/20190517_mr/output/* Hadoop 1 Hello 2 World 1 遇到的坑 问题一：执行到Running job: job_1557977819409_0004的地方就不往下执行了。 WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoopoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:01:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:01:03 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:01:03 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:01:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557977819409_0004 19/05/17 15:01:04 INFO impl.YarnClientImpl: Submitted application application_1557977819409_0004 19/05/17 15:01:04 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1557977819409_0004/ 19/05/17 15:01:04 INFO mapreduce.Job: Running job: job_1557977819409_0004 参考： Hadoop相关总结 Hadoop 运行wordcount任务卡在job running的一种解决办法 hadoop2.7.x运行wordcount程序卡住在INFO mapreduce.Job: Running job:job _1469603958907_0002 Can't run a MapReduce job on hadoop 2.4.0 解决方案：在$HADOOP_HOME/etc/hadoop/yarn-site.xml中增加配置。 yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 最终的yarn-site.xml如下： yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce.shuffle.class org.apache.hadoop.mapred.ShuffleHandler yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 重启yarn： ./sbin/stop-yarn.sh ./sbin/start-yarn.sh ChangeLog 20190517 | 创建文档，已发布博客：腾讯云社区、CSDN博客、语雀。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-03 18:45:55 "},"20190517HadoopCommands.html":{"url":"20190517HadoopCommands.html","title":"Hadoop常用命令详解","keywords":"","body":"Hadoop常用命令详解 大数据学习之路03 Hadoop基本命令 version 查看Hadoop版本。 WZB-MacBook:target wangzhibin$ hadoop version Hadoop 2.8.4 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674 Compiled by jdu on 2018-05-08T02:50Z Compiled with protoc 2.5.0 From source with checksum b02a59bb17646783210e979bea443b0 This command was run using /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/share/hadoop/common/hadoop-common-2.8.4.jar HDFS基础命令 命令格式 hadoop fs -cmd ls 列出hdfs文件系统根目录下的目录和文件 hadoop fs -ls / 列出hdfs文件系统所有的目录和文件 hadoop fs -ls -R / mkdir 一级一级的建目录，父目录不存在的话使用这个命令会报错 command: hadoop fs -mkdir eg: WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir /test/20190517 WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 所创建的目录如果父目录不存在就创建该父目录 hadoop fs -mkdir -p put 上传文件。hdfs file的父目录一定要存在，否则命令不会执行 command: hadoop fs -put eg: $ hadoop fs -put tmp/tmp.txt /test $ hadoop fs -ls -R /test 上传目录。hdfs dir 一定要存在，否则命令不会执行 command: hadoop fs -put ... eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put tmp/ /test WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 从键盘读取输入到hdfs file中，按Ctrl+D（Control+D）结束输入。hdfs file不能存在，否则命令不会执行 command: hadoop fs -put - eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put - /test/20190517.tmp.txt hello world WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 10:44 /test/20190517.tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cat 在标准输出中显示文件内容 command: hadoop fs -cat eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -cat /test/20190517.tmp.txt hello world tail 在标准输出中显示文件末尾的1KB数据 command: hadoop fs -tail eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -tail /test/20190517.tmp.txt hello world get 从hdfs中下载文件到本地。local file不能和 hdfs file名字不能相同，否则会提示文件已存在，没有重名的文件会复制到本地 command: hadoop fs -get eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test/20190517.tmp.txt . WZB-MacBook:tmp wangzhibin$ ls 20190517.tmp.txt tmp.txt 拷贝多个文件或目录到本地时，本地要为文件夹路径 command: hadoop fs -get ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test . WZB-MacBook:tmp wangzhibin$ ls -l test/ total 24 drwxr-xr-x 2 wangzhibin staff 64 5 17 10:56 20190517 -rw-r--r-- 1 wangzhibin staff 12 5 17 10:56 20190517.tmp.txt drwxr-xr-x 3 wangzhibin staff 96 5 17 10:56 tmp -rw-r--r-- 1 wangzhibin staff 4662 5 17 10:56 tmp.txt 注意：如果用户不是root， local 路径要为用户文件夹下的路径，否则会出现权限问题。 rm 每次可以删除多个文件或目录 command: hadoop fs -rm ... hadoop fs -rm -r ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -rm /test/20190517.tmp.txt Deleted /test/20190517.tmp.txt WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cp HDFS拷贝文件或文件夹。目标文件或者文件夹不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件还存在。 command: hadoop fs -cp hadoop fs -cp ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -cp /test/tmp.txt /test/20190517/ WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:03 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt mv HDFS移动文件或文件夹。目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件不存在。源路径有多个时，目标路径必须为目录，且必须存在。 command: hadoop fs -mv hadoop fs -mv ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -mv /test/20190517/tmp.20190519.txt /test/tmp WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:04 /test/tmp/tmp.20190519.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 注意：跨文件系统的移动（local到hdfs或者反过来）都是不允许的 count 统计hdfs对应路径下的目录个数，文件个数，文件总计大小 显示为目录个数，文件个数，文件总计大小，输入路径 command: hadoop fs -count eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -count /test 3 4 18648 /test du 显示hdfs对应路径下每个文件夹和文件的大小 command: hadoop fs -du eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du /test 4662 /test/20190517 9324 /test/tmp 4662 /test/tmp.txt 显示hdfs对应路径下所有文件和的大小 command: hadoop fs -du -s eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -s /test 18648 /test 显示hdfs对应路径下每个文件夹和文件的大小,文件的大小用方便阅读的形式表示，例如用64M代替67108864 command: hadoop fs -du -h eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -h /test 4.6 K /test/20190517 9.1 K /test/tmp 4.6 K /test/tmp.txt HDFS高级命令 以下命令参考：hadoop HDFS常用文件操作命令。没有实践。 moveFromLocal hadoop fs -moveFromLocal ... 与put相类似，命令执行后源文件 local src 被删除，也可以从从键盘读取输入到hdfs file中 copyFromLocal hadoop fs -copyFromLocal ... 与put相类似，也可以从从键盘读取输入到hdfs file中 moveToLocal 当前版本中还未实现此命令 copyToLocal hadoop fs -copyToLocal ... 与get相类似 getmerge hadoop fs -getmerge 将hdfs指定目录下所有文件排序后合并到local指定的文件中，文件不存在时会自动创建，文件存在时会覆盖里面的内容 hadoop fs -getmerge -nl 加上nl后，合并到local file中的hdfs文件之间会空出一行 text hadoop fs -text 将文本文件或某些格式的非文本文件通过文本格式输出 setrep hadoop fs -setrep -R 3 改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作 stat hdoop fs -stat [format] 返回对应路径的状态信息 [format]可选参数有：%b（文件大小），%o（Block大小），%n（文件名），%r（副本个数），%y（最后一次修改日期和时间） 可以这样书写hadoop fs -stat %b%o%n ，不过不建议，这样每个字符输出的结果不是太容易分清楚 archive hadoop archive -archiveName name.har -p * 命令中参数name：压缩文件名，自己任意取； ：压缩文件所在的父目录；：要压缩的文件名；：压缩文件存放路径\\示例：hadoop archive -archiveName hadoop.har -p /user 1.txt 2.txt /des* 示例中将hdfs中/user目录下的文件1.txt，2.txt压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下，如果1.txt，2.txt不写就是将/user目录下所有的目录和文件压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下 显示har的内容可以用如下命令： hadoop fs -ls /des/hadoop.jar 显示har压缩的是那些文件可以用如下命令 hadoop fs -ls -R har:///des/hadoop.har 注意：har文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少NameNode和DataNode过多的空间浪费。 balancer hdfs balancer 如果管理员发现某些DataNode保存数据过多，某些DataNode保存数据相对较少，可以使用上述命令手动启动内部的均衡过程 dfsadmin hdfs dfsadmin -help 管理员可以通过dfsadmin管理HDFS，用法可以通过上述命令查看 hdfs dfsadmin -report 显示文件系统的基本数据 hdfs dfsadmin -safemode enter：进入安全模式；leave：离开安全模式；get：获知是否开启安全模式； wait：等待离开安全模式 distcp 用来在两个HDFS之间拷贝数据 MapReduce命令 命令帮助 WZB-MacBook:target wangzhibin$ mapred -help Usage: mapred [--config confdir] [--loglevel loglevel] COMMAND where COMMAND is one of: pipes run a Pipes job job manipulate MapReduce jobs queue get information regarding JobQueues classpath prints the class path needed for running mapreduce subcommands historyserver run job history servers as a standalone daemon distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive archive-logs combine aggregated logs into hadoop archives hsadmin job history server admin interface Most commands print help when invoked w/o parameters. 列出所有任务 WZB-MacBook:target wangzhibin$ mapred job -list all 19/05/20 10:22:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Total jobs:1 JobId State StartTime UserName Queue Priority UsedContainers RsvdContainers UsedMem RsvdMem NeededMem AM info job_1558104288185_0001 SUCCEEDED 1558104322342 wangzhibin default DEFAULT N/A N/A N/A N/A N/A http://WZB-MacBook.local:8088/proxy/application_1558104288185_0001/ 强制停止任务 mapred job -kill 参考资料 1.0.4版本官方文档-Hadoop Shell命令 hadoop HDFS常用文件操作命令 大数据基本组件（Hadoop、HDFS、MapRed、YARN）入门命令 ChangeLog 20190517 | 创建文档，已发布博客：腾讯云社区、CSDN博客、语雀。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-03 18:48:30 "},"20190517MapreduceProgramWithIdea.html":{"url":"20190517MapreduceProgramWithIdea.html","title":"使用IDEA开发MapReduce程序","keywords":"","body":"使用IDEA开发MapReduce程序 2019-05-17 | 大数据学习之路04 环境准备 jdk1.7 intellij idea maven 本地MapReduce程序之WordCount 这里以Hadoop的官方示例程序WordCount为例，演示如何一步步编写程序直到运行。 新建一个Maven工程 使用idea新建一个普通maven项目bigdata-learn-wordcount maven依赖 org.apache.hadoop hadoop-common 2.8.4 org.apache.hadoop hadoop-hdfs 2.8.4 org.apache.hadoop hadoop-mapreduce-client-core 2.8.4 org.apache.hadoop hadoop-client 2.8.4 拷贝Hadoop中的WordCount源码 /** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package com.zhbwang.bigdata.example; import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class WordCount { public static class TokenizerMapper extends Mapper{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } public static class IntSumReducer extends Reducer { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length [...] \"); System.exit(2); } Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); for (int i = 0; i 初始化文件 在工程根目录新建input文件夹，增加两个文件 input - file1.TXT Hello World - file2.txt Hello Hadoop 运行配置 程序执行 执行日志 执行结果 打包到服务器使用hadoop jar命令执行 pom.xml增加打包插件 maven-assembly-plugin 2.2 com.zhbwang.bigdata.example.WordCount jar-with-dependencies make-assembly package single maven打包 $ mvn clean install 得到一个可执行jar包：bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar 使用java -jar执行 在当前可执行jar目录初始化input文件夹 执行以下命令，即可在当前目录生成output文件夹，里面就是执行结果。 java -jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar input output 使用hadoop jar执行 一开始遇到问题了，还以为打包打的不对，换了几个打包插件都不行。 WZB-MacBook:target wangzhibin$ hadoop jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar /practice/20190517_mr/input /practice/20190517_mr/output Exception in thread \"main\" java.io.IOException: Mkdirs failed to create /var/folders/gg/35tlzsrs1kj3c460vh9tvvv40000gn/T/hadoop-unjar2170725475686001105/META-INF/license at org.apache.hadoop.util.RunJar.ensureDirectory(RunJar.java:140) at org.apache.hadoop.util.RunJar.unJar(RunJar.java:109) at org.apache.hadoop.util.RunJar.unJar(RunJar.java:85) at org.apache.hadoop.util.RunJar.run(RunJar.java:222) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) 后来找到几篇文章，发现是Mac的问题，在stackoverflow中找到解释： The issue is that a /tmp/hadoop-xxx/xxx/LICENSE file and a /tmp/hadoop-xxx/xxx/license directory are being created on a case-insensitive file system when unjarring the mahout jobs. 参考资料： Hadoop java.io.IOException: Mkdirs failed to create /some/path Mac下hadoop运行word count的坑 解决方案：删除原来压缩包的META-INF/LICENS即可。 zip -d bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar META-INF/LICENSE jar tvf bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar | grep LICENSE 接下来就可以使用hadoop jar命令执行了。 WZB-MacBook:target wangzhibin$ hadoop jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 22:07:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 22:07:10 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 22:07:10 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 22:07:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558078701666_0004 19/05/17 22:07:11 INFO impl.YarnClientImpl: Submitted application application_1558078701666_0004 19/05/17 22:07:11 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1558078701666_0004/ 19/05/17 22:07:11 INFO mapreduce.Job: Running job: job_1558078701666_0004 查看结果。 WZB-MacBook:target wangzhibin$ hadoop fs -cat /practice/20190517_mr/output/part-r-00000 Hadoop 1 Hello 2 World 1 问题 一、Hadoop 2.x中还有hadoop-core-x.x.jar吗？ 答：2.x系列已经没有hadoop-core的jar包了，取而代之的是 对于Hadoop2.x.x版本，需要引入4个jar： hadoop-common hadoop-hdfs hadoop-mapreduce-client-core hadoop-client jdk.tools（一般需要引入，否则报错） 参考：Hadoop需要的Jar包 参考资料 IDEA 配置Hadoop开发（开发调试） Hadoop入门学习之（二）：Intellij 开发Hadoop环境搭建 Hadoop: Intellij结合Maven本地运行和调试MapReduce程序 (无需搭载Hadoop和HDFS环境) 大数据系列（hadoop） 集群环境搭建 idea 开发设置 Mac下hadoop运行word count的坑 ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:00:43 "},"20190522HadoopPrinciple.html":{"url":"20190522HadoopPrinciple.html","title":"Hadoop原理与架构解析","keywords":"","body":"2019-05-22 | Hadoop原理与架构解析 2019-05-22 | 大数据学习之路05 Hadoop简介 基本介绍 Hadoop 是 Apache 开源组织的一个分布式计算开源框架，是一个可以更容易开发和运行处理大规模数据的解决方案，它提供了一套分布式系统基础架构，允许使用简单的编程模型跨大型计算机的大型数据集进行分布式处理。 Hadoop架构 Hadoop框架包括以下四个模块： Hadoop Common：这些是其他Hadoop模块所需的Java库和实用程序。这些库提供文件系统和操作系统级抽象，并包含启动Hadoop所需的必要Java文件和脚本。 Hadoop YARN：这是作业调度和集群资源管理的框架。 Hadoop分布式文件系统（HDFS）：提供对应用程序数据的高吞吐量访问的分布式文件系统。 Hadoop MapReduce： 这是基于YARN的大型数据集并行处理系统。 Hadoop 框架中最核心的设计就是：MapReduce 和 HDFS。 MapReduce 的思想是由 Google 的一篇论文所提及而被广为流传的，简单的一句话解释 MapReduce 就是“任务的分解与结果的汇总”。 HDFS 是 Hadoop 分布式文件系统（Hadoop Distributed File System）的缩写，为分布式计算存储提供了底层支持。 HDFS运行原理 HDFS简介 HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。 HDFS有很多特点： 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。为防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其他某几个主机上。 运行在廉价的机器上。 适合大数据的处理。HDFS会将一个完整的大文件平均分块存储到不同计算机上，默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。 流式数据访问，一次写入多次读写，和传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化只能在文件末尾添加 HDFS架构原理 HDFS 架构原理HDFS采用Master/Slave架构。 一个HDFS集群包含一个单独的NameNode和多个DataNode。 NameNode作为Master服务，它负责管理文件系统的命名空间和客户端对文件的访问。NameNode会保存文件系统的具体信息，包括文件信息、文件被分割成具体block块的信息、以及每一个block块归属的DataNode的信息。对于整个集群来说，HDFS通过NameNode对用户提供了一个单一的命名空间。 DataNode作为Slave服务，在集群中可以存在多个。通常每一个DataNode都对应于一个物理节点。DataNode负责管理节点上它们拥有的存储，它将存储划分为多个block块，管理block块信息，同时周期性的将其所有的block块信息发送给NameNode。 下图为HDFS系统架构图，主要有三个角色，Client、NameNode、DataNode。 HDFS的一些关键元素 Block：将文件分块，通常为64M。 NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间。保存整个文件系统的目录信息、文件信息及分块信息，由唯一一台主机专门保存。 SecondaryNameNode：是一个小弟，分担大哥NameNode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给NameNode。（热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。） DataNode：是Slave节点，奴隶，干活的。负责存储Client发来的数据块block；执行数据块的读写操作。 fsimage：元数据镜像文件（文件系统的目录树） edits：元数据的操作日志（针对文件系统做的修改操作记录） HDFS设计重点 HDFS 数据备份HDFS被设计成一个可以在大集群中、跨机器、可靠的存储海量数据的框架。它将所有文件存储成block块组成的序列，除了最后一个block块，所有的block块大小都是一样的。 HDFS中的文件默认规则是write one（一次写、多次读）的，并且严格要求在任何时候只有一个writer。 NameNode全权管理数据块的复制，它周期性地从集群中的每个DataNode接受心跳信号和块状态报告（BlockReport）。接收到心跳信号以为该DataNode工作正常，块状态报告包含了一个该DataNode上所有数据块的列表。 NameNode内存中存储的是=fsimage+edits。SecondaryNameNode负责定时（默认1小时）从NameNode上，获取fsimage和edits来进行合并，然后再发送给NameNode。减少NameNode的工作量。 文件写入 Client向NameNode发起文件写入的请求。 NameNode根据文件大小和文件块配置情况，返回给Client它所管理部分DataNode的信息。 Client将文件划分为多个block块，并根据DataNode的地址信息，按顺序写入到每一个DataNode块中。 以下过程完全参考自（【Hadoop】HDFS的运行原理） 例如：有一个文件FileA，100M大小。Client将FileA写入到HDFS上。 HDFS按默认配置。 HDFS分布在三个机架上Rack1，Rack2，Rack3。 文件写入过程如下： Client将FileA按64M分块。分成两块，block1和Block2; Client向NameNode发送写数据请求，如图蓝色虚线①------>。 NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②--------->。 Block1: host2,host1,host3 Block2: host7,host8,host4 原理： NameNode具有RackAware机架感知功能，这个可以配置。 若Client为DataNode节点，那存储block时，规则为：副本1，同Client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。 若Client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。 Client向DataNode发送block1；发送过程是以流式写入。流式写入过程如下： 将64M的block1按64k的package划分; 然后将第一个package发送给host2; host2接收完后，将第一个package发送给host1，同时Client想host2发送第二个package； host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。 以此类推，如图红线实线所示，直到将block1发送完毕。 host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。 Client收到host2发来的消息后，向NameNode发送消息，说我写完了。这样就真完成了。如图黄色粗实线 发送完block1后，再向host7、host8、host4发送block2，如图蓝色实线所示。 发送完block2后，host7、host8、host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。 Client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。 分析：通过写过程，我们可以了解到 写1T文件，我们需要3T的存储，3T的网络流量贷款。 在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。 挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。 文件读取 当文件读取： Client向NameNode发起文件读取的请求。 NameNode返回文件存储的block块信息、及其block块所在DataNode的信息。 Client读取文件信息。 如图所示，Client要从DataNode上，读取FileA。而FileA由block1和block2组成。读操作流程如下： Client向NameNode发送读请求。 NameNode查看Metadata信息，返回FileA的block的位置。 block1:host2,host1,host3 block2:host7,host8,host4 block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取。 上面例子中，Client位于机架外，那么如果Client位于机架内某个DataNode上，例如,Client是host6。那么读取的时候，遵循的规律是：优选读取本机架上的数据。 问题：如果读取block是按照先后顺序读，是否意味着在不同副本之间的读取是不平均的，没有考虑通过负载策略来提高读效率吗？ 备份数据的存放 备份数据的存放是HDFS可靠性和性能的关键。HDFS采用一种称为rack-aware的策略来决定备份数据的存放。 通过一个称为Rack Awareness的过程，NameNode决定每个DataNode所属rack id。 缺省情况下，一个block块会有三个备份： 一个在NameNode指定的DataNode上 一个在指定DataNode非同一rack的DataNode上 一个在指定DataNode同一rack的DataNode上。 这种策略综合考虑了同一rack失效、以及不同rack之间数据复制性能问题。 副本的选择：为了降低整体的带宽消耗和读取延时，HDFS会尽量读取最近的副本。如果在同一个rack上有一个副本，那么就读该副本。如果一个HDFS集群跨越多个数据中心，那么将首先尝试读本地数据中心的副本。 MapReduce运行原理 MapReduce简介 MapReduce是一种分布式计算模型，由Google提出，主要用于搜索领域，解决海量数据的计算问题。 MapReduce分成两个部分：Map（映射）和Reduce（归纳）。 当你向MapReduce框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map任务，然后分配到不同的节点上去执行，每一个Map任务处理输入数据中的一部分。 当Map任务完成后，它会生成一些中间文件，这些中间文件将会作为Reduce任务的输入数据。Reduce任务的主要目标就是把前面若干个Map的输出汇总并输出。 MapReduce的基本模型和处理思想 大规模数据处理时，MapReduce在三个层面上的基本构思 参考（MapReduce的基本工作原理） 如何对付大数据处理：分而治之 对相互间不具有计算依赖关系的大数据，实现并行最自然的办法就是采取分而治之的策略。 什么样的计算任务可进行并行化计算？ A：不可分拆的计算任务或相互间有依赖关系的数据无法进行并行计算！ 一个大数据若可以分为具有同样计算过程的数据块，并且这些数据块之间不存在数据依赖关系，则提高处理速度的最好办法就是并行计算。 上升到抽象模型：Mapper与Reducer MPI等并行计算方法缺少高层并行编程模型，为了克服这一缺陷，MapReduce借鉴了Lisp函数式语言中的思想，用Map和Reduce两个函数提供了高层的并行编程抽象模型。 关键思想：为大数据处理过程中的两个主要处理操作提供一种抽象机制。Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。 MapReduce借鉴了函数式程序设计语言Lisp中的思想，定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现: map: (k1; v1) → [(k2; v2)]。 输入：键值对(k1; v1)表示的数据 处理：文档数据记录(如文本文件中的行，或数据表格中的行)将以“键值对”形式传入map函数；map函数将处理这些键值对，并以另一种键值对形式输出处理的一组键值对中间结果[(k2; v2)] 输出：键值对[(k2; v2)]表示的一组中间数据 reduce: (k2; [v2]) → [(k3; v3)] 输入： 由map输出的一组键值对[(k2; v2)] 将被进行合并处理将同样主键下的不同数值合并到一个列表[v2]中，故reduce的输入为(k2; [v2]) 处理：对传入的中间结果列表数据进行某种整理或进一步的处理,并产生最终的某种形式的结果输出[(k3; v3)] 。 输出：最终输出结果[(k3; v3)] 示例：假设有4组原始文本数据 Text 1: the weather is good Text 2: today is good Text 3: good weather is good Text 4: today has good weather MapReduce处理方式： 使用4个map节点： map节点1: 输入：(text1, “the weather is good”) 输出：(the, 1), (weather, 1), (is, 1), (good, 1) map节点2: 输入：(text2, “today is good”) 输出：(today, 1), (is, 1), (good, 1) map节点3: 输入：(text3, “good weather is good”) 输出：(good, 1), (weather, 1), (is, 1), (good, 1) map节点4: 输入：(text3, “today has good weather”) 输出：(today, 1), (has, 1), (good, 1), (weather, 1) 使用3个reduce节点： reduce节点1： 输入：(good, 1),(good, 1),(good, 1),(good, 1),(good, 1) 输出：(good, 5) reduce节点2： 输入：(has, 1),(is, 1),(is, 1),(is, 1) 输出：(has, 1),(is, 3) reduce节点3： 输入：(the, 1),(today, 1),(today, 1),(weather, 1),(weather, 1),(weather, 1) 输出：(the, 1),(today, 2),(weather, 3) 上升到构架：统一构架，为程序员隐藏系统层细节 MPI等并行计算方法缺少统一的计算框架支持，程序员需要考虑数据存储、划分、分发、结果收集、错误恢复等诸多细节；为此，MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。 各个map函数对所划分的数据并行处理，从不同的输入数据产生不同的中间结果输出 各个reduce也各自并行计算，各自负责处理不同的中间结果数据集合进行reduce处理之前，必须等到所有的map函数做完，因此，在进入reduce前需要有一个同步障(barrier)；这个阶段也负责对map的中间结果数据进行收集整理(aggregation & shuffle)处理，以便reduce更有效地计算最终结果最终汇总所有reduce的输出结果即可获得最终结果。 MapReduce提供一个统一的计算框架，可完成： 计算任务的划分和调度 数据的分布存储和划分 处理数据与计算任务的同步 结果数据的收集整理(sorting, combining, partitioning,…) 系统通信、负载平衡、计算性能优化处理 处理系统节点出错检测和失效恢复 MapReduce运行流程 MapReduce的物理架构 Map-Reduce的处理过程主要涉及以下四个部分： 客户端Client：用于提交Map-reduce任务job JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件 MapReduce的逻辑运行流程 MapReduce运行按照时间顺序包括五个阶段：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。 输入分片（input split） 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务。 输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组 输入分片（input split）和hdfs的block（块）关系很密切。假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。 map阶段：就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行。 combiner阶段： Combiner是一个本地化的reduce操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作 shuffle阶段 将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。 具体shuffle的过程不介绍了。 reduce阶段：和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。 简单的来说： 有一个待处理的大数据，被划分成大小相同的数据库(如64MB)，以及与此相应的用户作业程序。 系统中有一个负责调度的主节点(JobTracker)，以及数据Map和Reduce工作节点(TaskTracker). 用户作业提交给主节点JobTracker。 主节点为作业程序寻找和配备可用的Map节点，并将程序传送给map节点。 主节点也为作业程序寻找和配备可用的Reduce节点，并将程序传送给Reduce节点。 主节点启动每一个Map节点执行程序，每个Map节点尽可能读取本地或本机架的数据进行计算。(实现代码向数据靠拢，减少集群中数据的通信量)。 每个Map节点处理读取的数据块，并做一些数据整理工作(combining,sorting等)并将数据存储在本地机器上；同时通知主节点计算任务完成并告知主节点中间结果数据的存储位置。 主节点等所有Map节点计算完成后，开始启动Reduce节点运行；Reduce节点从主节点所掌握的中间结果数据位置信息，远程读取这些数据。 Reduce节点计算结果汇总输出到一个结果文件，即获得整个处理结果。 YARN运行原理 YARN简介 Yarn是Hadoop集群的分布式资源管理系统。Hadoop2.0对MapReduce框架做了彻底的设计重构，我们称Hadoop2.0中的MapReduce为MRv2或者Yarn，YARN是为了提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。 原MapReduce框架的不足 Hadoop 原 MapReduce 架构如下： 原 MapReduce 程序的流程及设计思路： 首先用户程序 (JobClient) 提交了一个 job，job 的信息会发送到 Job Tracker 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。 TaskTracker 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。 TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以给新提交的 job 分配运行在哪些机器上。上图虚线箭头就是表示消息的发送 - 接收的过程。 随着分布式系统集群的规模和其工作负荷的增长，原框架的问题逐渐浮出水面，主要的问题集中如下： JobTracker 是 Map-reduce 的集中处理点，存在单点故障。 JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗。业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限。 在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一块，很容易出现 OOM。 在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。 源代码层面分析的时候，会发现代码非常的难读，常常因为一个 class 做了太多的事情，代码量达 3000 多行，造成 class 的任务不清晰，增加 bug 修复和版本维护的难度。 从操作的角度来看，现在的 Hadoop MapReduce 框架在有任何重要的或者不重要的变化 ( 例如 bug 修复，性能提升和特性化 ) 时，都会强制进行系统级别的升级更新。更糟的是，它不管用户的喜好，强制让分布式集群系统的每一个用户端同时更新。这些更新会让用户为了验证他们之前的应用程序是不是适用新的 Hadoop 版本而浪费大量时间。 Yarn/MRv2的产生 为从根本上解决旧 MapReduce 框架的性能瓶颈，促进 Hadoop 框架的更长远发展，从 0.23.0 版本开始，Hadoop 的 MapReduce 框架完全重构，发生了根本的变化。新的 Hadoop MapReduce 框架命名为 MapReduceV2 或者叫 Yarn。 在Yarn中把job的概念换成了application，因为在新的Hadoop2.x中，运行的应用不只是MapReduce了，还有可能是其它应用如一个DAG（有向无环图Directed Acyclic Graph，例如storm应用）。 Yarn的另一个目标就是拓展Hadoop，使得它不仅仅可以支持MapReduce计算，还能很方便的管理诸如Hive、Hbase、Pig、Spark/Shark等应用。各种应用就可以互不干扰的运行在同一个Hadoop系统中，共享整个集群资源。 YARN组件与架构 Yarn主要由以下几个组件组成： ResourceManager：Global（全局）的进程 NodeManager：运行在每个节点上的进程 ApplicationMaster：Application-specific（应用级别）的进程。ApplicationMaster是对运行在Yarn中某个应用的抽象，它其实就是某个类型应用的实例，ApplicationMaster是应用级别的，它的主要功能就是向ResourceManager（全局的）申请计算资源（Containers）并且和NodeManager交互来执行和监控具体的task。 Scheduler：是ResourceManager的一个组件。Scheduler是ResourceManager专门进行资源管理的一个组件，负责分配NodeManager上的Container资源，NodeManager也会不断发送自己Container使用情况给ResourceManager。 Container：节点上一组CPU和内存资源。Container是Yarn对计算机计算资源的抽象，它其实就是一组CPU和内存资源，所有的应用都会运行在Container中。 新的 Hadoop MapReduce 框架（Yarn）架构 YARN执行过程 Application在Yarn中的执行过程，整个执行过程可以总结为三步： 应用程序提交 启动应用的ApplicationMaster实例 ApplicationMaster实例管理应用程序的执行 客户端程序向ResourceManager提交应用并请求一个ApplicationMaster实例 ResourceManager找到可以运行一个Container的NodeManager，并在这个Container中启动ApplicationMaster实例 ApplicationMaster向ResourceManager进行注册，注册之后客户端就可以查询ResourceManager获得自己ApplicationMaster的详细信息，以后就可以和自己的ApplicationMaster直接交互了 在平常的操作过程中，ApplicationMaster根据resource-request协议向ResourceManager发送resource-request请求 当Container被成功分配之后，ApplicationMaster通过向NodeManager发送container-launch-specification信息来启动Container， container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料 应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster 在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，交流的协议也是application-specific协议 一但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，用到所有的Container也归还给系统 。 思考题 还是那个经典的题目，一个10G大小的文件，给定1G大小的内存，如何使用Java程序统计出现次数最多的10个单词及次数 参考资料 Hadoop简介 分布式计算开源框架 Hadoop 介绍 Hadoop-介绍 【Hadoop】HDFS的运行原理 分布式计算框架Hadoop原理及架构全解 Hadoop 原理总结 MapReduce原理与设计思想 MapReduce的基本工作原理 Hadoop 之MapReduce 运行原理全解析 hadoop 学习笔记：mapreduce框架详解 Hadoop核心之HDFS 架构设计 Hadoop核心之MapReduce架构设计 Hadoop Yarn详解 Hadoop 新 MapReduce 框架 Yarn 详解 ChangeLog 20190522 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 17:12:50 "},"20190525HiveQuickStart.html":{"url":"20190525HiveQuickStart.html","title":"Hive安装与使用","keywords":"","body":"2019-05-25 | Hive安装与使用 2019-05-25 | 大数据学习之路06 Hive简介 这里先简单介绍，明确Hive的目标是什么。后续会详细介绍Hive架构与原理。 Hive是基于Hadoop的数据仓库工具，可以对存储在HDFS中的文件的数据进行数据整理、查询、分析。 Hive提供了类似与SQL的查询语言——HiveQL，可以通过HQL实现简单的MR统计，Hive将HQL语句转换成MR任务进行执行。 Hive下载 Hive与Hadoop对应关系 截止当前（2019-05-25），Hive最新版本有三种：hive-1.2.2、hive-2.3.5、hive-3.1.1。 Hive官网下载页面说明，hive-2.3.5对应Hadoop版本是2.x.y，hive-3.1.1对应Hadoop版本是3.x.y。 本人安装Hadoop版本是2.8.4，故下载hive-2.3.5。 Hive下载地址 下载地址：http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz WZB-MacBook:50_bigdata wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata WZB-MacBook:50_bigdata wangzhibin$ wget http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz Hive安装配置 Hive安装 解压 WZB-MacBook:50_bigdata wangzhibin$ tar zxvf apache-hive-2.3.5-bin.tar.gz 配置.bash_profile WZB-MacBook:50_bigdata wangzhibin$ vi ~/.bash_profile WZB-MacBook:50_bigdata wangzhibin$ source ~/.bash_profile 增加如下配置： # hive export HIVE_HOME=/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin export PATH=$PATH:$HIVE_HOME/bin 验证是否安装成功 WZB-MacBook:50_bigdata wangzhibin$ hive --version Hive 2.3.5 Git git://HW13934/Users/gates/git/hive -r 76595628ae13b95162e77bba365fe4d2c60b3f29 Compiled by gates on Tue May 7 15:45:09 PDT 2019 From source with checksum c7864fc25abcb9cf7a36953ac6be4665 Hive配置 由于hive是默认将元数据保存在本地内嵌的 Derby 数据库中，但是这种做法缺点也很明显，Derby不支持多会话连接，因此本文将选择mysql作为元数据存储。 需要先安装Mysql，本文不做过多介绍，可以自行百度。 需要下载mysql的jdbc，然后将下载后的jdbc放到hive安装包的lib目录下。 WZB-MacBook:50_bigdata wangzhibin$ wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz WZB-MacBook:50_bigdata wangzhibin$ tar zxvf mysql-connector-java-5.1.47.tar.gz WZB-MacBook:50_bigdata wangzhibin$ cd mysql-connector-java-5.1.47 WZB-MacBook:mysql-connector-java-5.1.47 wangzhibin$ cp mysql-connector-java-5.1.47-bin.jar $HIVE_HOME/lib/ 修改配置hive-site.xml WZB-MacBook:~ wangzhibin$ cd $HIVE_HOME/conf WZB-MacBook:conf wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/conf WZB-MacBook:conf wangzhibin$ cp hive-default.xml.template hive-site.xml WZB-MacBook:conf wangzhibin$ vim hive-site.xml 配置文件如下： javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword mysql javax.jdo.option.ConnectionURL jdbc:mysql://localhost:3306/hive javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver 在mysql中初始化hive的schema（在此之前需要创建mysql下的hive数据库） WZB-MacBook:conf wangzhibin$ cd $HIVE_HOME/bin WZB-MacBook:bin wangzhibin$ schematool -dbType mysql -initSchema hive库中会初始化一些模型表： 到此配置完毕。HDFS中并未初始化数据仓库位置。 Hive使用 创建一个hive测试库 WZB-MacBook:bin wangzhibin$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/lib/hive-common-2.3.5.jar!/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases. hive> hive> create database hive_1; OK Time taken: 4.089 seconds hive> show databases; OK default hive_1 Time taken: 0.123 seconds, Fetched: 2 row(s) hive> 看看HDFS目录发生了什么变化 WZB-MacBook:conf wangzhibin$ hadoop fs -ls -R /user drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse/hive_1.db drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 15:53 /user/wangzhibin 看看mysql下的hive库有什么变化 mysql> use hive; mysql> select * from DBS; +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ | DB_ID | DESC | DB_LOCATION_URI | NAME | OWNER_NAME | OWNER_TYPE | +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ | 1 | Default Hive database | hdfs://localhost:9000/user/hive/warehouse | default | public | ROLE | | 2 | NULL | hdfs://localhost:9000/user/hive/warehouse/hive_1.db | hive_1 | wangzhibin | USER | +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ 2 rows in set (0.00 sec) 创建一个hive测试表 hive> use hive_1; OK Time taken: 3.772 seconds hive> create table hive_01 (id int,name string); OK Time taken: 0.582 seconds hive> show tables; OK hive_01 Time taken: 0.087 seconds, Fetched: 1 row(s) hive> 看看HDFS目录发生了什么变化 WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /user drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:28 /user/hive/warehouse/hive_1.db drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:28 /user/hive/warehouse/hive_1.db/hive_01 drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 15:53 /user/wangzhibin 看看mysql下的hive库有什么变化 mysql> select * from TBLS; +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ | TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER | RETENTION | SD_ID | TBL_NAME | TBL_TYPE | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT | IS_REWRITE_ENABLED | +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ | 1 | 1558780134 | 2 | 0 | wangzhibin | 0 | 1 | hive_01 | MANAGED_TABLE | NULL | NULL | | +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ 1 row in set (0.00 sec) 看一下web上有什么变化。 以上就是hive的简单使用，说白了，hive与mysql的使用差不多；对应于hdfs，hive_1库是hdfs中的一个目录，hive_01表也是一个目录。 参考资料 Hive基础知识介绍 Hive详细介绍及简单应用 hive简介 Hive安装与配置详解 Hive官方文档 Getting Started Guide on the Hive wiki ChangeLog 20190611 | 增加Hive官方文档链接 20190525 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-13 09:52:35 "},"20190527WordcountProgramByHive.html":{"url":"20190527WordcountProgramByHive.html","title":"Hive实现wordcount词频统计","keywords":"","body":"2019-05-27 | Hive实现wordcount词频统计 2019-05-27 | 大数据学习之路07 新建测试文件 WZB-MacBook:tmp wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata/tmp WZB-MacBook:tmp wangzhibin$ vi test.txt 增加内容： hello man what are you doing now my running hello kevin hi man 文件导入到hive 建表并指定文件内容分隔符 hive> use hive_1; OK Time taken: 0.024 seconds hive> create table wc(txt String) row format delimited fields terminated by '\\t'; OK Time taken: 0.715 seconds hive> show tables; OK hive_01 wc Time taken: 0.035 seconds, Fetched: 2 row(s) 导入文件 HDFS初始无数据 WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /user/hive/warehouse/hive_1.db/wc 返回无数据 导入文件 hive> load data local inpath '/Users/wangzhibin/00_dev_suite/50_bigdata/tmp/test.txt' overwrite into table wc; Loading data to table hive_1.wc OK Time taken: 2.235 seconds hive> select * from wc; OK hello man what are you doing now my running hello kevin hi man Time taken: 1.602 seconds, Fetched: 6 row(s) HDFS文件内容 WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /user/hive/warehouse/hive_1.db/wc -rwxr-xr-x 1 wangzhibin supergroup 63 2019-05-27 21:09 /user/hive/warehouse/hive_1.db/wc/test.txt WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -cat /user/hive/warehouse/hive_1.db/wc/test.txt hello man what are you doing now my running hello kevin hi man 使用HQL统计单词 hive> select split(txt,' ') from wc; OK [\"hello\",\"man\"] [\"what\",\"are\",\"you\",\"doing\",\"now\"] [\"my\",\"running\"] [\"hello\"] [\"kevin\"] [\"hi\",\"man\"] Time taken: 0.337 seconds, Fetched: 6 row(s) hive> select explode(split(txt,' ')) from wc; OK hello man what are you doing now my running hello kevin hi man Time taken: 0.094 seconds, Fetched: 13 row(s) hive> select t1.word,count(t1.word) from (select explode(split(txt,' '))word from wc)t1 group by t1.word; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases. Query ID = wangzhibin_20190527214319_1532be66-b6e5-4603-83a9-dc4c7d6ec466 Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks not specified. Defaulting to jobconf value of: 1 In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer= In order to limit the maximum number of reducers: set hive.exec.reducers.max= In order to set a constant number of reducers: set mapreduce.job.reduces= Starting Job = job_1558964487152_0004, Tracking URL = http://WZB-MacBook.local:8088/proxy/application_1558964487152_0004/ Kill Command = /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/bin/hadoop job -kill job_1558964487152_0004 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1 2019-05-27 21:43:25,976 Stage-1 map = 0%, reduce = 0% 2019-05-27 21:43:31,173 Stage-1 map = 100%, reduce = 0% 2019-05-27 21:43:36,365 Stage-1 map = 100%, reduce = 100% Ended Job = job_1558964487152_0004 MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 HDFS Read: 8923 HDFS Write: 294 SUCCESS Total MapReduce CPU Time Spent: 0 msec OK are 1 doing 1 hello 2 hi 1 kevin 1 man 2 my 1 now 1 running 1 what 1 you 1 Time taken: 17.562 seconds, Fetched: 11 row(s) 小插曲：[执行结果未出来的原因是：没有启动yarn] 总结 split--------------------------列变数组 explode------------------------数组拆分成多行 group by和count----------------对行分组后求各行出现的次数 参考资料： Hive实现wordcount词频统计 ChangeLog 20190527 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:09:01 "},"20190529HiveArchitecture.html":{"url":"20190529HiveArchitecture.html","title":"Hive原理详解","keywords":"","body":"2019-05-29 | Hive原理详解 2019-05-29 | 大数据学习之路08 Hive概述 数据仓库的概念 首先要来看一下数据库与数据仓库的区别。 数据库：传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。 数据仓库：数据仓库系统的主要应用主要是OLAP（On-Line Analytical Processing），支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 主要区别如下： 数据库偏重数据的业务处理（transaction），属于OLTP（Online transaction processing）层面，后者着重于分析，可能会重点面向某个行业，属于OLAP（Online analytical processing）层面。 数据库一般叫“业务型数据库”，数据仓库被称为“分析型数据库”。数据库常采用行式存储，而数据仓库常采用列式存储，数据结构有利于查询和分析。 前者的用户数量大（主要是业务人员），既要执行“读”操作也要执行“写”操作，每次写的量不大，但是对时间敏感。后者的用户数量小（主要是决策人员），一般只需要执行读操作，每次读取的数据量很大，对反应时间不那么敏感。 把所需要的数据从业务型数据库导入分析型数据仓库的过程，称为ETL（Extract-Transform-Load，“抽取-转换-加载”）。 数据库用到的工具主要有MySQL, Oracle, MS SQLServer等，数据仓库用到的工具主要有Hive, AWSRedshift, Green Plum, SAP HANA等。 参考： 数据库 与 数据仓库的本质区别是什么？ \"数据库\" vs. \"数据仓库\": 区别与联系 Hive简介 Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取、转化、加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。 Hive是一个构建于Hadoop顶层的数据仓库工具，可以查询和管理PB级别的分布式数据。 支持大规模数据存储、分析，具有良好的可扩展性 某种程度上可以看作是用户编程接口，本身不存储和处理数据。 依赖分布式文件系统HDFS存储数据。 依赖分布式并行计算模型MapReduce处理数据。 定义了简单的类似SQL 的查询语言——HiveQL。 用户可以通过编写的HiveQL语句运行MapReduce任务。 可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到Hadoop平台上。 是一个可以提供有效、合理、直观组织和使用数据的分析工具。 参考：Hive技术原理解析 Hive与传统数据库的区别 Hive适用场景 Hive不适用于大规模数据集实现低延迟快速查询。 Hive 构建在基于静态批处理的Hadoop 之上，Hadoop 通常都有较高的延迟并且在作业提交和调度的时候需要大量的开销。因此，Hive 并不能够在大规模数据集上实现低延迟快速的查询，例如，Hive 在几百MB 的数据集上执行查询一般有分钟级的时间延迟。 Hive 并不适合那些需要低延迟的应用，例如，联机事务处理（OLTP）。Hive 查询操作过程严格遵守Hadoop MapReduce 的作业执行模型，Hive 将用户的HiveQL 语句通过解释器转换为MapReduce 作业提交到Hadoop 集群上，Hadoop 监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，Hive 并不提供实时的查询和基于行级的数据更新操作。Hive 的最佳使用场合是大数据集的批处理作业，例如，网络日志分析。 Hive适用于以下场景： 数据挖掘：用户行为分析；兴趣分区；区域展示； 非实时分析：日志分析；文本分析。 数据汇总：每天/每周用户点击数，流量统计。 数据仓库：数据抽取，加载，转换（ETL）。 Hive功能与架构 Hive与Hadoop Hive的执行入口是Driver，执行的SQL语句首先提交到Drive驱动，然后调用compiler解释驱动，最终解释成MapReduce任务去执行。 Hive与Hadoop生态系统中其他组件的关系 Hive依赖于HDFS 存储数据 Hive依赖于MapReduce 处理数据 在某些场景下Pig可以作为Hive的替代工具 HBase 提供数据的实时访问 Hive系统架构 （图片来源：Design - Apache Hive） Hive的工作原理简单的说就是一个查询引擎，接收到一个SQL，后续工作包括： 词法分析/语法分析 使用antlr将SQL语句解析成抽象语法树(AST) 语义分析 从Metastore获取模式信息，验证SQL语句中队表名，列名，以及数据类型的检查和隐式转换，以及Hive提供的函数和用户自定义的函数(UDF/UAF) 逻辑计划生成 生成逻辑计划--算子树 逻辑计划优化 对算子树进行优化，包括列剪枝，分区剪枝，谓词下推等 物理计划生成 将逻辑计划生成包含由MapReduce任务组成的DAG的物理计划 物理计划执行 将DAG发送到Hadoop集群进行执行 （图片来源：http://infolab.stanford.edu/~ragho/hive-icde2010.pdf ） Hive的数据模型 Hive的数据存储在HDFS上，基本存储单位是表或者分区，Hive内部把表或者分区称作SD，即Storage Descriptor。一个SD通常是一个HDFS路径，或者其它文件系统路径。SD的元数据信息存储在Hive MetaStore中，如文件路径，文件格式，列，数据类型，分隔符。Hive默认的分格符有三种，分别是\\^A、\\^B和\\^C，即ASCii码的1、2和3，分别用于分隔列，分隔列中的数组元素，和元素Key-Value对中的Key和Value。 分区：数据表可以按照某个字段的值划分分区。 每个分区是一个目录。 分区数量不固定。 分区下可再有分区或者桶。 桶：数据可以根据桶的方式将不同数据放入不同的桶中。 每个桶是一个文件。 建表时指定桶个数，桶内可排序。 数据按照某个字段的值Hash后放入某个桶中。 Hive可以创建托管表和外部表： 默认创建托管表，Hiva会将数据移动到数据仓库的目录。 创建外部表，这时Hiva会到仓库目录以外的位置访问数据。 如果所有处理都由Hive完成，建议使用托管表。 如果要用Hive和其他工具来处理同一个数据集，建议使用外部表。 Hive工作原理 SQL语句转换成MapReduce作业的基本原理 join的实现原理 group by实现原理 Hive中SQL查询转换成MapReduce作用的过程 当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作： 驱动模块接收该命令或查询编译器 对该命令或查询进行解析编译 由优化器对该命令或查询进行优化计算 该命令或查询通过执行器进行执行 第1步：由Hive驱动模块中的编译器对用户输入的SQL语言进行词法和语法解析，将SQL语句转化为抽象语法树的形式。 第2步：抽象语法树的结构仍很复杂，不方便直接翻译为MapReduce算法程序，因此，把抽象语法书转化为查询块。 第3步：把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符。 第4步：重写逻辑查询计划，进行优化，合并多余操作，减少MapReduce任务数量。 第5步：将逻辑操作符转换成需要执行的具体MapReduce任务。 第6步：对生成的MapReduce任务进行优化，生成最终的MapReduce任务执行计划。 第7步：由Hive驱动模块中的执行器，对最终的MapReduce任务进行执行输出。 几点说明： 当启动MapReduce程序时，Hive本身是不会生成MapReduce算法程序的。 需要通过一个表示“Job执行计划”的XML文件驱动执行内置的、原生的Mapper和Reducer模块。 Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在JobTracker所在的管理节点上执行。 通常在大型集群上，会有专门的网关机来部署Hive工具。网关机的作用主要是远程操作和管理节点上的JobTracker通信来执行任务。 数据文件通常存储在HDFS上，HDFS由名称节点管理。 Hive 的数据类型 Hive 的数据存储支持 HDFS 的一些文件格式，比如 CSV, Sequence File, Avro, RC File, ORC, Parquet。也支持访问 HBase。 Hive 支持原子和复杂数据类型，原子数据类型包括：数据值、布尔类型、字符串类型等，复杂的类型包括：Array、Map和Struct。其中Array和Map和java中的Array和Map是相似的，Struct和C语言中的Struct相似。 Create table test( col1 Array, col2 Map, col3 Struct ); 参考资料 Design-Apache Hive Hive原理及查询优化 Hive技术原理 Hive技术原理解析 Hive 工作原理详解 如何通俗地理解Hive的工作原理？ ChangeLog 20190609 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 17:01:55 "},"20190609HiveCommands.html":{"url":"20190609HiveCommands.html","title":"Hive常用命令","keywords":"","body":"Hive 基本命令 连接 Hive shell 连接 Hive WZB-MacBook:hadoop wangzhibin$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/Cellar_w/raw/apache-hive-2.3.5-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/Cellar_w/raw/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in jar:file:/usr/local/Cellar_w/raw/apache-hive-2.3.5-bin/lib/hive-common-2.3.5.jar!/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases. hive> 退出 Hive hive> quit; hive> quit; Mon Jun 10 11:21:55 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:55 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:55 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:55 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:56 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:56 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:56 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Mon Jun 10 11:21:56 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. WZB-MacBook:hadoop wangzhibin$ 注意：hive > exit; 会影响之前的使用，所以需要下一句 kill 掉 hadoop 的进程：hine > hadoop job -kill [jobid] 数据库相关 创建数据库 hive> create database test_01; OK Time taken: 5.708 seconds 如果数据库已经存在就会抛出一个错误信息，使用如下语句可以避免抛出错误信息： hive> create database test_01; FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database test_01 already exists hive> create database if not exists test_01; OK Time taken: 0.012 seconds 查看数据库 hive> show databases; OK default hive_1 test_01 Time taken: 0.146 seconds, Fetched: 3 row(s) 如果数据库比较多的话，也可以用正则表达式来查看： hive> show databases like 'h.*'; OK hive_1 Time taken: 0.228 seconds, Fetched: 1 row(s) 使用数据库 hive> use test_01; OK Time taken: 0.027 seconds 查看数据库的描述及路径 hive> describe database test_01; OK test_01 hdfs://localhost:9900/user/hive/warehouse/test_01.db wangzhibin USER Time taken: 0.017 seconds, Fetched: 1 row(s) 修改数据库的路径（没有尝试） hive> creat database database_name location '路径'; 删除库 hive> drop database if exists database_name; --删除空的数据库 hive> drop database if exists database_name cascade; --先删除数据库中的表再删除数据库； hive> show databases; OK default hive_1 test_01 Time taken: 0.299 seconds, Fetched: 3 row(s) hive> drop database if exists test_01 cascade; OK Time taken: 0.387 seconds hive> show databases; OK default hive_1 Time taken: 0.016 seconds, Fetched: 2 row(s) 表相关 创建表 hive> create table test(key string); OK Time taken: 0.875 seconds 创建一个新表，结构与其他一样 hive> create table test_01 like test; OK Time taken: 0.394 seconds 分区表与数据加载 创建分区表 hive> create table logs(ts bigint,line string) partitioned by (dt String,country String) row format delimited fields terminated by ','; OK Time taken: 0.073 seconds 数据加载 load data local inpath '/home/wangzb/file1' into table logs partition (dt='2001-01-01',country='GB'); 数据加载的两种方式 hive>load data inpath '/root/inner_table.dat' into table t1; 移动hdfs中数据到t1表中 hive>load data local inpath '/root/inner_table.dat' into table t1;上传本地数据到hdfs中 有 local 的速度明显比没有 local 慢。 说明： 首先，创建分区表的时候，需要通过关键字 partitioned by (dt String,country String) 声明该表是分区表，并且是按照字段 dt、country 进行分区的； 其次，向分区表导入数据的时候，要通过关键字 partition (dt='2001-01-01',country='GB') 显示声明数据要导入到表的哪个分区中。 所谓分区，这是将满足某些条件的记录打包，做个记号，在查询时提高效率，相当于按文件夹对文件进行分类，文件夹名可类比分区字段。 这个分区字段形式上存在于数据表中，在查询时会显示到客户端上，但并不真正在存储在数据表文件中，是所谓伪列。所以，千万不要以为是对属性表中真正存在的列按照属性值的异同进行分区。比如上面的分区依据的列 dt、country 并不真正的存在于数据表中，是我们为了方便管理添加的一个伪列，这个列的值也是我们人为规定的，不是从数据表中读取之后根据值的不同将其分区。 我们并不能按照某个数据表中真实存在的列，如 ts 来分区。 可参见示例：Hive实现wordcount词频统计 命令：创建分桶 create table t_bluk(id string, name string) clustered by(id) sort by (id) into 4 buckets; 解析：clustered by(id) 意思是根据id分成4个桶，并且桶内按照id排序。 上述命令执行后，将会在相应的hdfs文件目录下创建四个子目录，如： 可能遇到的问题：当使用命令“insert into t_buck select * from other”时，出现t_buck目录下并没有四个子目录，只有一个子目录，需要如下操作： 设置如下变量： #设置变量, 设置分桶为true, 设置reduce数量是分桶的数量个数 set hive.enforce.bucketing = true; set mapreduce.job.reduces=4; 使用“insert ... select ...”命令向t_buck插入数据才会最终生成四个分区。 额外说明：insert into t_buck select id,name from t_p distribute by (id) sort by (id);。distribute by (id)指定分区字段； sort by (id) 指定排序字段。当排序和分桶的字段相同的时候可以使用 distribute by (sno) sort by (sno asc) 或Cluster by(字段)。cluster by等同于分桶+排序(sort) 分区和分桶的区别： 分区依据的是伪列，分桶则是相对分区进行更细粒度的划分。 分桶将整个数据内容按照某列属性值的hash值进行区分，如要按照name属性分为3个桶，就是对name属性值的hash值对3取摸，按照取模结果对数据分桶。如取模结果为0的数据记录存放到一个文件，取模为1的数据存放到一个文件，取模为2的数据存放到一个文件。 与分区不同的是，分区依据的不是真实数据表文件中的列，而是我们指定的伪列，但是分桶是依据数据表中真实的列而不是伪列。所以在指定分区依据的列的时候要指定列的类型，因为在数据表文件中不存在这个列，相当于新建一个列。而分桶依据的是表中已经存在的列，这个列的数据类型显然是已知的，所以不需要指定列的类型。 参考：Hive基本命令解析 查看所有表 hive> show tables; OK logs test test_01 Time taken: 0.304 seconds, Fetched: 3 row(s) 查看表的结构与路径 hive> describe test; OK key string Time taken: 0.048 seconds, Fetched: 1 row(s) hive> describe logs; OK ts bigint line string dt string country string # Partition Information # col_name data_type comment dt string country string Time taken: 0.11 seconds, Fetched: 10 row(s) 更新表名称 hive> alter table test rename to test_02; OK Time taken: 0.555 seconds hive> show tables; OK logs test_01 test_02 Time taken: 0.024 seconds, Fetched: 3 row(s) 添加新一列 hive> describe test_01; OK key string Time taken: 0.059 seconds, Fetched: 1 row(s) hive> alter table test_01 add columns(value string comment 'a comment'); OK Time taken: 0.097 seconds hive> describe test_01; OK key string value string a comment Time taken: 0.046 seconds, Fetched: 2 row(s) 删除表 hive> drop table test_01; --或者 drop table if exists test_01; OK Time taken: 0.429 seconds hive> show tables; OK logs test_02 Time taken: 0.022 seconds, Fetched: 2 row(s) 删除表，保留结构 hive> truncate table tableName; Hive其他命令 查询当前linux文件夹下的文件 hive> !ls /usr; bin lib libexec local sbin share standalone 查询当前hdfs文件系统下的文件 hive> dfs -ls -R /user; drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive drwxr-xr-x - wangzhibin supergroup 0 2019-06-10 15:10 /user/hive/warehouse drwxr-xr-x - wangzhibin supergroup 0 2019-05-27 21:06 /user/hive/warehouse/hive_1.db drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:28 /user/hive/warehouse/hive_1.db/hive_01 drwxr-xr-x - wangzhibin supergroup 0 2019-05-27 21:09 /user/hive/warehouse/hive_1.db/wc -rwxr-xr-x 1 wangzhibin supergroup 63 2019-05-27 21:09 /user/hive/warehouse/hive_1.db/wc/test.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 15:53 /user/wangzhibin 高级命令 加载本地数据 Hive不支持行级插入操作、更新操作和删除操作，也不支持事务，那么往表里面装数据的唯一的途径就是使用一种「大量」的数据装载操作，或者仅仅将文件写入到正确的目录下面，即以load的方式加载到建立好的表中，且数据一旦导入就不可以修改，加载的目标可以是一个表，或者分区，如果表包含分区，必须指定每一个分区名。 使用overwrite关键字，加载本地数据，同时给分区信息： hive>load data local inpath '${env:HOME}/目录' overwrite into table table_name partition (ds='2018-05-05')； 目标表（或者分区）中的内容（如果有）会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中，如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 将查询结果插入hive表 hive> insert overwrite table tab_name [partition(par1=val1,par2=val2)] select_statement1 from from_statement; hive> insert into table tab_name [partition(par1=val1,par2=val2)] select_statement1 from from_statement; 将查询结果写入HDFS文件系统 hive> insert overwrite [local] directory directory1 select ... from ...; 参考资料 Hive基本命令解析 Hive shell 基本命令 Hive入门及常用指令 ChangeLog 20190610 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-10 15:38:01 "},"20190611SqoopQuickStart.html":{"url":"20190611SqoopQuickStart.html","title":"Sqoop 安装与使用","keywords":"","body":"Sqoop安装与使用 Sqoop简介 Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。 导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统； 导出数据：从Hadoop的文件系统中导出数据到关系数据库 Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。 本文针对的是Sqoop1，不涉及到Sqoop2，两者有很大区别。 Sqoop下载与安装 下载Sqoop 下载页面下有两个链接，使用sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz，包含hadoop支持。不要用sqoop-1.4.7.tar.gz。 WZB-MacBook:raw wangzhibin$ wget http://mirror.bit.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 解压Sqoop安装文件 WZB-MacBook:raw wangzhibin$ tar zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz WZB-MacBook:Cellar_w wangzhibin$ cd .. WZB-MacBook:Cellar_w wangzhibin$ ln -s raw/sqoop-1.4.7.bin__hadoop-2.6.0 sqoop WZB-MacBook:Cellar_w wangzhibin$ ls -l lrwxr-xr-x 1 wangzhibin wheel 33 6 11 14:53 sqoop -> raw/sqoop-1.4.7.bin__hadoop-2.6.0 配置环境变量 # sqoop export SQOOP_HOME=/usr/local/Cellar_w/sqoop export PATH=$PATH:$SQOOP_HOME/bin Sqoop 配置文件修改。 进入 $SQOOP_HOME/conf 目录下，将 sqoop-env-template.sh 复制一份，并取名为 sqoop-env.sh WZB-MacBook:sqoop wangzhibin$ cd $SQOOP_HOME/conf WZB-MacBook:conf wangzhibin$ cp sqoop-env-template.sh sqoop-env.sh WZB-MacBook:conf wangzhibin$ ls oraoop-site-template.xml sqoop-env-template.cmd sqoop-env-template.sh sqoop-env.sh sqoop-site-template.xml sqoop-site.xml WZB-MacBook:conf wangzhibin$ 在 sqoop-env.sh 文件末尾加入配置： export HADOOP_COMMON_HOME=/usr/local/Cellar_w/hadoop export HADOOP_MAPRED_HOME=/usr/local/Cellar_w/hadoop export HIVE_HOME=/usr/local/Cellar_w/hive # export HBASE_HOME=/home/hadoop/hbase-1.2.2 MySQL驱动包 把 MySQL 的驱动包上传到 Sqoop 的 lib 目录下。之前在 Hive 安装过程中用到过 MySQL 的驱动包，直接使用。 WZB-MacBook:lib wangzhibin$ cp /usr/local/Cellar_w/hive/lib/mysql-connector-java-5.1.47-bin.jar . Sqoop使用 Sqoop Help WZB-MacBook:sqoop wangzhibin$ sqoop help Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/11 15:36:18 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 usage: sqoop COMMAND [ARGS] Available commands: codegen Generate code to interact with database records create-hive-table Import a table definition into Hive eval Evaluate a SQL statement and display the results export Export an HDFS directory to a database table help List available commands import Import a table from a database to HDFS import-all-tables Import tables from a database to HDFS import-mainframe Import datasets from a mainframe server to HDFS job Work with saved jobs list-databases List available databases on a server list-tables List available tables in a database merge Merge results of incremental imports metastore Run a standalone Sqoop metastore version Display version information See 'sqoop help COMMAND' for information on a specific command. WZB-MacBook:sqoop wangzhibin$ 查看 MySQL 中的数据库。 执行如下命令，连接mysql看有多少个数据库。 WZB-MacBook:sqoop wangzhibin$ sqoop list-databases --connect jdbc:mysql://localhost:3306?characterEncoding=UTF-8 --username root --password 'mysql' Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/11 15:39:34 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 19/06/11 15:39:34 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/11 15:39:34 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. Tue Jun 11 15:39:34 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. information_schema hive my_summary_v1.0 mysql performance_schema sys test_hdfs_import 查看 MySQL 有多少个表 WZB-MacBook:sqoop wangzhibin$ sqoop list-tables --connect jdbc:mysql://localhost:3306/test_hdfs_import?characterEncoding=UTF-8 --username root --password 'mysql' Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/11 15:50:36 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 19/06/11 15:50:36 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/11 15:50:36 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. Tue Jun 11 15:50:36 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. role user 把 MySQL 中的表导入 HDFS 中 前提：一定要启动hdfs和yarn。 MySQL数据库user ​ 导入HDFS。 WZB-MacBook:sqoop wangzhibin$ sqoop import -m 1 --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password mysql --table user --target-dir /user/sqoop/test_mysql_import Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/11 15:55:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 19/06/11 15:55:16 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/11 15:55:17 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 19/06/11 15:55:17 INFO tool.CodeGenTool: Beginning code generation Tue Jun 11 15:55:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/11 15:55:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/11 15:55:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/11 15:55:17 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/Cellar_w/hadoop 注: /tmp/sqoop-wangzhibin/compile/8fa026c1f508000a44160a00979d17d3/user.java使用或覆盖了已过时的 API。 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。 19/06/11 15:55:19 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-wangzhibin/compile/8fa026c1f508000a44160a00979d17d3/user.jar 19/06/11 15:55:19 WARN manager.MySQLManager: It looks like you are importing from mysql. 19/06/11 15:55:19 WARN manager.MySQLManager: This transfer can be faster! Use the --direct 19/06/11 15:55:19 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path. 19/06/11 15:55:19 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql) 19/06/11 15:55:19 INFO mapreduce.ImportJobBase: Beginning import of user 19/06/11 15:55:19 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 19/06/11 15:55:20 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 19/06/11 15:55:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Tue Jun 11 15:55:23 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/11 15:55:23 INFO db.DBInputFormat: Using read commited transaction isolation 19/06/11 15:55:24 INFO mapreduce.JobSubmitter: number of splits:1 19/06/11 15:55:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1560136240717_0001 19/06/11 15:55:25 INFO impl.YarnClientImpl: Submitted application application_1560136240717_0001 19/06/11 15:55:25 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1560136240717_0001/ 19/06/11 15:55:25 INFO mapreduce.Job: Running job: job_1560136240717_0001 19/06/11 15:55:37 INFO mapreduce.Job: Job job_1560136240717_0001 running in uber mode : false 19/06/11 15:55:37 INFO mapreduce.Job: map 0% reduce 0% 19/06/11 15:55:42 INFO mapreduce.Job: map 100% reduce 0% 19/06/11 15:55:43 INFO mapreduce.Job: Job job_1560136240717_0001 completed successfully 19/06/11 15:55:43 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=178561 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=87 HDFS: Number of bytes written=48 HDFS: Number of read operations=4 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Other local map tasks=1 Total time spent by all maps in occupied slots (ms)=2995 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=2995 Total vcore-milliseconds taken by all map tasks=2995 Total megabyte-milliseconds taken by all map tasks=3066880 Map-Reduce Framework Map input records=2 Map output records=2 Input split bytes=87 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=21 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=201326592 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=48 19/06/11 15:55:43 INFO mapreduce.ImportJobBase: Transferred 48 bytes in 23.6248 seconds (2.0318 bytes/sec) 19/06/11 15:55:43 INFO mapreduce.ImportJobBase: Retrieved 2 records. 导入结果 WZB-MacBook:sqoop wangzhibin$ hadoop fs -ls -R /user/sqoop drwxr-xr-x - wangzhibin supergroup 0 2019-06-11 15:55 /user/sqoop drwxr-xr-x - wangzhibin supergroup 0 2019-06-11 15:55 /user/sqoop/test_mysql_import -rw-r--r-- 1 wangzhibin supergroup 0 2019-06-11 15:55 /user/sqoop/test_mysql_import/_SUCCESS -rw-r--r-- 1 wangzhibin supergroup 48 2019-06-11 15:55 /user/sqoop/test_mysql_import/part-m-00000 WZB-MacBook:sqoop wangzhibin$ hadoop fs -cat /user/sqoop/test_mysql_import/part-m-00000 1,wangzhibin,wangzhibin 2,testuser,测试用户 将 MySQL 中的数据表导入到Hive中 创建 Hive 表 test_user 字段与MySQL中一致。 use hive_1; drop table if exists hive_1.test_user ; create table hive_1.test_user( id int, name string, desc string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' ; 导入关系表到HIVE sqoop import \\ --connect jdbc:mysql://localhost:3306/test_hdfs_import \\ --username root \\ --password mysql \\ --table user \\ --fields-terminated-by '\\t' \\ --delete-target-dir \\ --hive-import \\ --hive-database hive_1 \\ --hive-table test_user \\ -m 1 执行结果： WZB-MacBook:lib wangzhibin$ sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password mysql --table user --fields-terminated-by '\\t' --delete-target-dir --hive-import --hive-database hive_1 --hive-table test_user -m 1 Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/13 10:46:50 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/usr/local/Cellar_w/raw/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/usr/local/Cellar_w/raw/apache-hive-2.3.5-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 19/06/13 10:46:50 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/13 10:46:50 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 19/06/13 10:46:50 INFO tool.CodeGenTool: Beginning code generation Thu Jun 13 10:46:50 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:46:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 10:46:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 10:46:50 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/Cellar_w/hadoop 注: /tmp/sqoop-wangzhibin/compile/93f56e3a0bd9ae4384d9c3fc01ef8e1b/user.java使用或覆盖了已过时的 API。 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。 19/06/13 10:46:53 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-wangzhibin/compile/93f56e3a0bd9ae4384d9c3fc01ef8e1b/user.jar 19/06/13 10:46:54 INFO tool.ImportTool: Destination directory user deleted. 19/06/13 10:46:54 WARN manager.MySQLManager: It looks like you are importing from mysql. 19/06/13 10:46:54 WARN manager.MySQLManager: This transfer can be faster! Use the --direct 19/06/13 10:46:54 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path. 19/06/13 10:46:54 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql) 19/06/13 10:46:54 INFO mapreduce.ImportJobBase: Beginning import of user 19/06/13 10:46:54 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 19/06/13 10:46:54 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 19/06/13 10:46:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Thu Jun 13 10:46:57 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:46:57 INFO db.DBInputFormat: Using read commited transaction isolation 19/06/13 10:46:57 INFO mapreduce.JobSubmitter: number of splits:1 19/06/13 10:46:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1560136240717_0009 19/06/13 10:46:57 INFO impl.YarnClientImpl: Submitted application application_1560136240717_0009 19/06/13 10:46:57 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1560136240717_0009/ 19/06/13 10:46:57 INFO mapreduce.Job: Running job: job_1560136240717_0009 19/06/13 10:47:05 INFO mapreduce.Job: Job job_1560136240717_0009 running in uber mode : false 19/06/13 10:47:05 INFO mapreduce.Job: map 0% reduce 0% 19/06/13 10:47:10 INFO mapreduce.Job: map 100% reduce 0% 19/06/13 10:47:11 INFO mapreduce.Job: Job job_1560136240717_0009 completed successfully 19/06/13 10:47:12 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=183612 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=87 HDFS: Number of bytes written=48 HDFS: Number of read operations=4 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=1 Other local map tasks=1 Total time spent by all maps in occupied slots (ms)=3032 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=3032 Total vcore-milliseconds taken by all map tasks=3032 Total megabyte-milliseconds taken by all map tasks=3104768 Map-Reduce Framework Map input records=2 Map output records=2 Input split bytes=87 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=26 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=201326592 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=48 19/06/13 10:47:12 INFO mapreduce.ImportJobBase: Transferred 48 bytes in 17.5847 seconds (2.7296 bytes/sec) 19/06/13 10:47:12 INFO mapreduce.ImportJobBase: Retrieved 2 records. 19/06/13 10:47:12 INFO mapreduce.ImportJobBase: Publishing Hive/Hcat import job data to Listeners for table user Thu Jun 13 10:47:12 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:47:12 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 10:47:12 INFO hive.HiveImport: Loading uploaded data into Hive 19/06/13 10:47:12 INFO conf.HiveConf: Found configuration file file:/usr/local/Cellar_w/hive/conf/hive-site.xml Logging initialized using configuration in jar:file:/usr/local/Cellar_w/raw/sqoop-1.4.7.bin__hadoop-2.6.0/lib/hive-exec-2.3.5.jar!/hive-log4j2.properties Async: true 19/06/13 10:47:14 INFO SessionState: Logging initialized using configuration in jar:file:/usr/local/Cellar_w/raw/sqoop-1.4.7.bin__hadoop-2.6.0/lib/hive-exec-2.3.5.jar!/hive-log4j2.properties Async: true 19/06/13 10:47:15 INFO session.SessionState: Created HDFS directory: /tmp/hive/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:15 INFO session.SessionState: Created local directory: /var/folders/gg/35tlzsrs1kj3c460vh9tvvv40000gn/T/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:15 INFO session.SessionState: Created HDFS directory: /tmp/hive/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36/_tmp_space.db 19/06/13 10:47:15 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:15 INFO session.SessionState: Updating thread name to 7041efcf-5e5a-4c48-a381-89bd445e7b36 main 19/06/13 10:47:15 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:15 INFO ql.Driver: Compiling command(queryId=wangzhibin_20190613104715_4762c8b4-54b9-4399-9ab7-acd0c1b732c1): CREATE TABLE IF NOT EXISTS `hive_1`.`test_user` ( `ID` INT, `USER_NAME` STRING, `USER_DESC` STRING) COMMENT 'Imported by sqoop on 2019/06/13 10:47:12' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\011' LINES TERMINATED BY '\\012' STORED AS TEXTFILE 19/06/13 10:47:17 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:17 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:17 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored 19/06/13 10:47:17 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored Thu Jun 13 10:47:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:17 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:47:18 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\" Thu Jun 13 10:47:20 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:20 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:20 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. Thu Jun 13 10:47:20 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 10:47:22 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:22 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:22 INFO metastore.HiveMetaStore: Added admin role in metastore 19/06/13 10:47:22 INFO metastore.HiveMetaStore: Added public role in metastore 19/06/13 10:47:22 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty 19/06/13 10:47:22 INFO metastore.HiveMetaStore: 0: get_all_functions 19/06/13 10:47:22 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_all_functions 19/06/13 10:47:22 INFO parse.CalcitePlanner: Starting Semantic Analysis 19/06/13 10:47:22 INFO parse.CalcitePlanner: Creating table hive_1.test_user position=27 19/06/13 10:47:22 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:22 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO ql.Driver: Semantic Analysis Completed 19/06/13 10:47:23 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null) 19/06/13 10:47:23 INFO ql.Driver: Completed compiling command(queryId=wangzhibin_20190613104715_4762c8b4-54b9-4399-9ab7-acd0c1b732c1); Time taken: 7.893 seconds 19/06/13 10:47:23 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager 19/06/13 10:47:23 INFO ql.Driver: Executing command(queryId=wangzhibin_20190613104715_4762c8b4-54b9-4399-9ab7-acd0c1b732c1): CREATE TABLE IF NOT EXISTS `hive_1`.`test_user` ( `ID` INT, `USER_NAME` STRING, `USER_DESC` STRING) COMMENT 'Imported by sqoop on 2019/06/13 10:47:12' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\011' LINES TERMINATED BY '\\012' STORED AS TEXTFILE 19/06/13 10:47:23 INFO sqlstd.SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7041efcf-5e5a-4c48-a381-89bd445e7b36, clientType=HIVECLI] 19/06/13 10:47:23 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory. 19/06/13 10:47:23 INFO hive.metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore 19/06/13 10:47:23 INFO ql.Driver: Completed executing command(queryId=wangzhibin_20190613104715_4762c8b4-54b9-4399-9ab7-acd0c1b732c1); Time taken: 0.058 seconds OK 19/06/13 10:47:23 INFO ql.Driver: OK Time taken: 7.959 seconds 19/06/13 10:47:23 INFO CliDriver: Time taken: 7.959 seconds 19/06/13 10:47:23 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:23 INFO session.SessionState: Resetting thread name to main 19/06/13 10:47:23 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:23 INFO session.SessionState: Updating thread name to 7041efcf-5e5a-4c48-a381-89bd445e7b36 main 19/06/13 10:47:23 INFO ql.Driver: Compiling command(queryId=wangzhibin_20190613104723_983989d1-e1b0-42b5-b587-1c922b71ee1b): LOAD DATA INPATH 'hdfs://localhost:9900/user/wangzhibin/user' INTO TABLE `hive_1`.`test_user` 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:23 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:23 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:23 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO ql.Driver: Semantic Analysis Completed 19/06/13 10:47:23 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null) 19/06/13 10:47:23 INFO ql.Driver: Completed compiling command(queryId=wangzhibin_20190613104723_983989d1-e1b0-42b5-b587-1c922b71ee1b); Time taken: 0.374 seconds 19/06/13 10:47:23 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager 19/06/13 10:47:23 INFO ql.Driver: Executing command(queryId=wangzhibin_20190613104723_983989d1-e1b0-42b5-b587-1c922b71ee1b): LOAD DATA INPATH 'hdfs://localhost:9900/user/wangzhibin/user' INTO TABLE `hive_1`.`test_user` 19/06/13 10:47:23 INFO ql.Driver: Starting task [Stage-0:MOVE] in serial mode 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore Loading data to table hive_1.test_user 19/06/13 10:47:23 INFO exec.Task: Loading data to table hive_1.test_user from hdfs://localhost:9900/user/wangzhibin/user 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:23 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:23 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:23 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:23 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:23 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: alter_table: db=hive_1 tbl=test_user newtbl=test_user 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=alter_table: db=hive_1 tbl=test_user newtbl=test_user 19/06/13 10:47:24 INFO ql.Driver: Starting task [Stage-1:STATS] in serial mode 19/06/13 10:47:24 INFO exec.StatsTask: Executing stats task 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:24 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:24 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:24 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=get_table : db=hive_1 tbl=test_user 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: alter_table: db=hive_1 tbl=test_user newtbl=test_user 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=alter_table: db=hive_1 tbl=test_user newtbl=test_user 19/06/13 10:47:24 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore 19/06/13 10:47:24 INFO metastore.ObjectStore: ObjectStore, initialize called 19/06/13 10:47:24 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 19/06/13 10:47:24 INFO metastore.ObjectStore: Initialized ObjectStore 19/06/13 10:47:24 INFO hive.log: Updating table stats fast for test_user 19/06/13 10:47:24 INFO hive.log: Updated size of table test_user to 96 19/06/13 10:47:24 INFO exec.StatsTask: Table hive_1.test_user stats: [numFiles=2, numRows=0, totalSize=96, rawDataSize=0] 19/06/13 10:47:24 INFO ql.Driver: Completed executing command(queryId=wangzhibin_20190613104723_983989d1-e1b0-42b5-b587-1c922b71ee1b); Time taken: 0.947 seconds OK 19/06/13 10:47:24 INFO ql.Driver: OK Time taken: 1.321 seconds 19/06/13 10:47:24 INFO CliDriver: Time taken: 1.321 seconds 19/06/13 10:47:24 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:24 INFO session.SessionState: Resetting thread name to main 19/06/13 10:47:24 INFO conf.HiveConf: Using the default value passed in for log id: 7041efcf-5e5a-4c48-a381-89bd445e7b36 19/06/13 10:47:24 INFO session.SessionState: Deleted directory: /tmp/hive/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36 on fs with scheme hdfs 19/06/13 10:47:24 INFO session.SessionState: Deleted directory: /var/folders/gg/35tlzsrs1kj3c460vh9tvvv40000gn/T/wangzhibin/7041efcf-5e5a-4c48-a381-89bd445e7b36 on fs with scheme file 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Cleaning up thread local RawStore... 19/06/13 10:47:24 INFO metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO HiveMetaStore.audit: ugi=wangzhibin ip=unknown-ip-addr cmd=Done cleaning up thread local RawStore 19/06/13 10:47:24 INFO hive.HiveImport: Hive import complete. 19/06/13 10:47:24 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory. 查看Hive表结果 WZB-MacBook:security wangzhibin$ hive hive> use hive_1; OK Time taken: 4.181 seconds hive> show tables; OK hive_01 test_user wc Time taken: 0.143 seconds, Fetched: 3 row(s) hive> select * from test_user; OK 1 wangzhibin wangzhibin 2 testuser 测试用户 Time taken: 1.594 seconds, Fetched: 2 row(s) 导入 Hive 的过程 现将MySQL数据导入到HDFS的 /user/wangzhibin/[tablename] 中 再从 /user/wangzhibin/[tablename] 通过load加载到Hive制定数据表中。 删除 /user/wangzhibin/[tablename] 将 HDFS 上的数据导出到数据库中 HDFS文件 WZB-MacBook:sqoop wangzhibin$ hadoop fs -cat /user/sqoop/test_mysql_export/file.txt 10 test_wzb 20 test_wzb1 21 test_wzb2 Sqoop导出 WZB-MacBook:sqoop wangzhibin$ sqoop export --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password mysql --export-dir /user/sqoop/test_mysql_export --table user -m 1 --fields-terminated-by ' ' Warning: /usr/local/Cellar_w/sqoop/../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. Warning: /usr/local/Cellar_w/sqoop/../hcatalog does not exist! HCatalog jobs will fail. Please set $HCAT_HOME to the root of your HCatalog installation. Warning: /usr/local/Cellar_w/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. Warning: /usr/local/Cellar_w/sqoop/../zookeeper does not exist! Accumulo imports will fail. Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation. 19/06/13 08:36:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 19/06/13 08:36:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/13 08:36:51 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 19/06/13 08:36:51 INFO tool.CodeGenTool: Beginning code generation Thu Jun 13 08:36:51 CST 2019 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 19/06/13 08:36:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 08:36:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 1 19/06/13 08:36:51 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/Cellar_w/hadoop 注: /tmp/sqoop-wangzhibin/compile/d065816cee026699216afea62bb3193e/user.java使用或覆盖了已过时的 API。 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。 19/06/13 08:36:55 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-wangzhibin/compile/d065816cee026699216afea62bb3193e/user.jar 19/06/13 08:36:55 INFO mapreduce.ExportJobBase: Beginning export of user 19/06/13 08:36:55 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 19/06/13 08:36:55 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative 19/06/13 08:36:55 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative 19/06/13 08:36:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 19/06/13 08:36:56 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/06/13 08:37:00 INFO input.FileInputFormat: Total input files to process : 1 19/06/13 08:37:00 INFO input.FileInputFormat: Total input files to process : 1 19/06/13 08:37:00 INFO mapreduce.JobSubmitter: number of splits:1 19/06/13 08:37:00 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative 19/06/13 08:37:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1560136240717_0002 19/06/13 08:37:01 INFO impl.YarnClientImpl: Submitted application application_1560136240717_0002 19/06/13 08:37:01 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1560136240717_0002/ 19/06/13 08:37:01 INFO mapreduce.Job: Running job: job_1560136240717_0002 19/06/13 08:37:11 INFO mapreduce.Job: Job job_1560136240717_0002 running in uber mode : false 19/06/13 08:37:11 INFO mapreduce.Job: map 0% reduce 0% 19/06/13 08:37:16 INFO mapreduce.Job: map 100% reduce 0% 19/06/13 08:37:16 INFO mapreduce.Job: Job job_1560136240717_0002 completed successfully 19/06/13 08:37:17 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=178209 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=181 HDFS: Number of bytes written=0 HDFS: Number of read operations=4 HDFS: Number of large read operations=0 HDFS: Number of write operations=0 Job Counters Launched map tasks=1 Data-local map tasks=1 Total time spent by all maps in occupied slots (ms)=2859 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=2859 Total vcore-milliseconds taken by all map tasks=2859 Total megabyte-milliseconds taken by all map tasks=2927616 Map-Reduce Framework Map input records=3 Map output records=3 Input split bytes=140 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=22 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=201326592 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=0 19/06/13 08:37:17 INFO mapreduce.ExportJobBase: Transferred 181 bytes in 21.1273 seconds (8.5671 bytes/sec) 19/06/13 08:37:17 INFO mapreduce.ExportJobBase: Exported 3 records. 导出结果 其他命令 数据库中的数据导入到HDFS上 sqoop import --connect jdbc:mysql://localhost:3306/itcast --username root --password 123 --table trade_detail --columns 'id, account, income, expenses' 指定输出路径、指定数据分隔符 sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password 123 --table trade_detail --target-dir '/sqoop/td' --fields-terminated-by '\\t' 指定Map数量 -m sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password 123 --table trade_detail --target-dir '/sqoop/td1' --fields-terminated-by '\\t' -m 2 增加where条件 注意：条件必须用引号引起来 sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password 123 --table trade_detail --where 'id>3' --target-dir '/sqoop/td2' 增加query语句(使用 \\ 将语句换行) sqoop import --connect jdbc:mysql://localhost:3306/test_hdfs_import --username root --password 123 \\ --query 'SELECT * FROM trade_detail where id > 2 AND $CONDITIONS' --split-by trade_detail.id --target-dir '/sqoop/td3' 注意： 如果使用 --query 这个命令的时候，需要注意的是 where 后面的参数，AND $CONDITIONS 这个参数必须加上 单引号与双引号的区别，如果--query 后面使用的是双引号，那么需要 $CONDITIONS 前加上\\ 即\\$CONDITIONS 如果设置map数量为1个时即-m 1，不用加上--split-by ${tablename.column}，否则需要加上 Sqoop遇到的坑 sqoop Could not initialize class org.apache.derby.jdbc.AutoloadedDriver40 解决方法：将 $HIVE_HOME/lib 中的 derby-x.x.jar 拷贝到 $SQOOP_HOME/lib。 java.security.AccessControlException: access denied (\"javax.management.MBeanTrustPermission\" \"register\") 解决方法： vim /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home/jre/lib/java.policy 新增语句： grant { // JMX Java Management eXtensions permission javax.management.MBeanTrustPermission \"register\"; }; tool.ImportTool: Import failed: java.io.IOException: Hive CliDriver exited with status=1 解决方法： cp $HIVE_HOME/lib/libthrift*.jar $SQOOP_HOME/lib java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.readerFor(Ljava/lang/Class;)Lcom/fasterxml/jackson/databind/ObjectReader; 原因：Sqoop的jackson版本号与Hive中的jackson版本号不一致。 解决方法： WZB-MacBook:lib wangzhibin$ mv jackson-* ../bak/ WZB-MacBook:lib wangzhibin$ cp /usr/local/Cellar_w/hive/lib/jackson-* . 参考资料 官方文档 Sqoop最佳实践 sqoop部署及使用 Sqoop安装 Sqoop安装和简单使用 Hadoop集群中sqoop的安装使用 sqoop数据导入hive 遇到的问题 Sqoop 工作原理 ChangeLog 20190613 | 完善Sqoop从MySQL导入到Hive的部分 20190611 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-13 11:06:50 "},"20190703IntroduceOfDataX.html":{"url":"20190703IntroduceOfDataX.html","title":"DataX 介绍","keywords":"","body":"DataX 介绍 什么是 DataX 目前 DataX 开源版本是 3.0。用官方的介绍就是： DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。 DataX 设计思路 在 DataX 之前，我们对不同数据源之间的数据同步，采用的是网状模式，任意两种数据源之间的数据同步，都要写一个程序来实现，或者采用已有工具实现，比如使用 Sqoop 作为 MySQL 与 Hive 之间数据同步工具。 但是随着业务的发展与复杂性的提升，对各种数据源之间的同步需求越来越大，同时也对数据同步过程中的完整性、无损传输提出了更高的要求。 DataX 采用了星型数据链路来讲各种不同的数据源连接起来，可以实现任意数据源之间的无缝数据同步。 为了解决异构数据源同步问题，DataX 将复杂的网状的同步链路变成了星型数据链路，DataX 作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到 DataX，便能跟已有的数据源做到无缝数据同步。 DataX 框架设计 DataX 采用的是 FrameWork + Plugins 的架构设计，将数据源的读取和写入操作抽象成 ReadPlugin 与 WritePlugin，连接到 FrameWork 中。 数据流如下： 通过 ReadPlugin 将数据源的数据采集发送到 FrameWork 中 FrameWork 不负责主动发送数据，而是由 WritePlugin 从 FrameWork 中主动拉取数据，并收集到目的数据源中。 FrameWork 的主要任务就是负责处理缓冲、流控、并发、数据转换等核心技术问题。 DataX 核心架构 单个数据同步作业在 DataX 中我们称之为Job。当 DataX 收到一个Job时，会启动一个进程来完成整个数据同步过程。 Job 承担了数据清理、子任务切分为 Tasks、TaskGroup 管理等功能。DataX 启动一个 Job 后，会根据不同的数据源切分策略，将一个 Job 切分为多个 Task，以便能并发执行，Task 是 DataX 作业的最小执行单元。 切分 Task 后，DataX 会启动 Schedule 调度器，将 Task 分配到不同的 TaskGroup 进行执行。每个 TaskGroup 默认5个并发，如果 DataX 配置了20个并发，则需要分配 4 个 TaskGroup来执行。如果一个 Job 拆分了 200 个 Task，则每个 TaskGroup 会分配 50 个 Task，并以 5 个并发来执行中 50 个 Task。 一个 Task 的执行过程包括 Reader - Channel - Writer 三个线程，并按照顺序执行，完成同步任务。 Task 执行过程中，Job 监控这些 TaskGroup 的执行过程，并等待执行完成后，成功退出。 DataX VS Sqoop 功能 DataX Sqoop 运行模式 单进程多线程 MR MySQL 读写 单机压力大；读写粒度容易控制 mr 模式重，写出错处理麻烦 Hive 读写 单机压力大 很好 文件格式 orc 支持 orc 不支持，可添加 分布式 不支持，可以通过调度系统规避 支持 流控 有流控功能 需要定制 统计信息 已有一些统计，上报需定制 没有，分布式的数据收集不方便 数据校验 在 core 部分有校验功能 没有，分布式的数据收集不方便 监控 需要定制 需要定制 社区 开源不久，社区不活跃 一直活跃，核心部分变动很少 参考资料 阿里云开源离线同步工具DataX3.0介绍-云栖社区-阿里云 DataX/introduction.md at master · alibaba/DataX · GitHub DataX/userGuid.md at master · alibaba/DataX · GitHub DataX的执行流程分析 - 简书 ChangeLog 20190704 | 补充「DataX 核心架构」 20190703 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-04 13:41:57 "},"20190703QuickStartOfDataX.html":{"url":"20190703QuickStartOfDataX.html","title":"DataX 快速使用指南","keywords":"","body":"DataX 快速使用指南 安装部署 本次安装部署 DataX 的环境为 MacOS。 环境要求 Linux JDK(1.8以上，推荐1.8) Python(推荐Python2.6.X) Apache Maven 3.x (Compile DataX) 下载 DataX WZB-MacBook:raw wangzhibin$ pwd /usr/local/Cellar_w/raw WZB-MacBook:raw wangzhibin$ wget http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz WZB-MacBook:raw wangzhibin$ tar zxvf datax.tar.gz 下载后解压至本地某个目录，进入bin目录，即可运行同步作业： $ cd {YOUR_DATAX_HOME}/bin $ python datax.py {YOUR_JOB.json} 检查是否安装成功 通过执行一个示例来检查DataX是否正常执行。示例json在$DATAX_HOME/job/job.json。 WZB-MacBook:bin wangzhibin$ pwd /usr/local/Cellar_w/datax/bin WZB-MacBook:bin wangzhibin$ python datax.py ../job/job.json DataX (DATAX-OPENSOURCE-3.0), From Alibaba ! Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved. 2019-07-03 17:30:00.067 [main] INFO VMInfo - VMInfo# operatingSystem class => com.sun.management.UnixOperatingSystem 2019-07-03 17:30:00.085 [main] INFO Engine - the machine info => osInfo: Oracle Corporation 1.7 24.80-b11 jvmInfo: Mac OS X x86_64 10.14.5 cpu num: 8 totalPhysicalMemory: 16.00G freePhysicalMemory: 0.22G maxFileDescriptorCount: 10240 currentOpenFileDescriptorCount: 76 GC Names [PS MarkSweep, PS Scavenge] MEMORY_NAME | allocation_size | init_size PS Survivor Space | 42.50MB | 42.50MB PS Old Gen | 683.00MB | 683.00MB PS Eden Space | 256.50MB | 256.50MB Code Cache | 48.00MB | 2.44MB PS Perm Gen | 82.00MB | 21.00MB 2019-07-03 17:30:00.112 [main] INFO Engine - { \"content\":[ { \"reader\":{ \"name\":\"streamreader\", \"parameter\":{ \"column\":[ { \"type\":\"string\", \"value\":\"DataX\" }, { \"type\":\"long\", \"value\":19890604 }, { \"type\":\"date\", \"value\":\"1989-06-04 00:00:00\" }, { \"type\":\"bool\", \"value\":true }, { \"type\":\"bytes\", \"value\":\"test\" } ], \"sliceRecordCount\":100000 } }, \"writer\":{ \"name\":\"streamwriter\", \"parameter\":{ \"encoding\":\"UTF-8\", \"print\":false } } } ], \"setting\":{ \"errorLimit\":{ \"percentage\":0.02, \"record\":0 }, \"speed\":{ \"byte\":10485760 } } } 2019-07-03 17:30:00.141 [main] WARN Engine - prioriy set to 0, because NumberFormatException, the value is: null 2019-07-03 17:30:00.144 [main] INFO PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0 2019-07-03 17:30:00.145 [main] INFO JobContainer - DataX jobContainer starts job. 2019-07-03 17:30:00.147 [main] INFO JobContainer - Set jobId = 0 2019-07-03 17:30:00.172 [job-0] INFO JobContainer - jobContainer starts to do prepare ... 2019-07-03 17:30:00.172 [job-0] INFO JobContainer - DataX Reader.Job [streamreader] do prepare work . 2019-07-03 17:30:00.173 [job-0] INFO JobContainer - DataX Writer.Job [streamwriter] do prepare work . 2019-07-03 17:30:00.173 [job-0] INFO JobContainer - jobContainer starts to do split ... 2019-07-03 17:30:00.174 [job-0] INFO JobContainer - Job set Max-Byte-Speed to 10485760 bytes. 2019-07-03 17:30:00.175 [job-0] INFO JobContainer - DataX Reader.Job [streamreader] splits to [1] tasks. 2019-07-03 17:30:00.176 [job-0] INFO JobContainer - DataX Writer.Job [streamwriter] splits to [1] tasks. 2019-07-03 17:30:00.204 [job-0] INFO JobContainer - jobContainer starts to do schedule ... 2019-07-03 17:30:00.212 [job-0] INFO JobContainer - Scheduler starts [1] taskGroups. 2019-07-03 17:30:00.215 [job-0] INFO JobContainer - Running by standalone Mode. 2019-07-03 17:30:00.229 [taskGroup-0] INFO TaskGroupContainer - taskGroupId=[0] start [1] channels for [1] tasks. 2019-07-03 17:30:00.234 [taskGroup-0] INFO Channel - Channel set byte_speed_limit to -1, No bps activated. 2019-07-03 17:30:00.234 [taskGroup-0] INFO Channel - Channel set record_speed_limit to -1, No tps activated. 2019-07-03 17:30:00.248 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started 2019-07-03 17:30:00.561 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[313]ms 2019-07-03 17:30:00.562 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] completed it's tasks. 2019-07-03 17:30:10.247 [job-0] INFO StandAloneJobContainerCommunicator - Total 100000 records, 2600000 bytes | Speed 253.91KB/s, 10000 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.030s | All Task WaitReaderTime 0.054s | Percentage 100.00% 2019-07-03 17:30:10.247 [job-0] INFO AbstractScheduler - Scheduler accomplished all tasks. 2019-07-03 17:30:10.249 [job-0] INFO JobContainer - DataX Writer.Job [streamwriter] do post work. 2019-07-03 17:30:10.250 [job-0] INFO JobContainer - DataX Reader.Job [streamreader] do post work. 2019-07-03 17:30:10.251 [job-0] INFO JobContainer - DataX jobId [0] completed successfully. 2019-07-03 17:30:10.253 [job-0] INFO HookInvoker - No hook invoked, because base dir not exists or is a file: /usr/local/Cellar_w/raw/datax/hook 2019-07-03 17:30:10.264 [job-0] INFO JobContainer - [total cpu info] => averageCpu | maxDeltaCpu | minDeltaCpu 1.85% | 1.85% | 1.85% [total gc info] => NAME | totalGCCount | maxDeltaGCCount | minDeltaGCCount | totalGCTime | maxDeltaGCTime | minDeltaGCTime PS MarkSweep | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s PS Scavenge | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s 2019-07-03 17:30:10.265 [job-0] INFO JobContainer - PerfTrace not enable! 2019-07-03 17:30:10.266 [job-0] INFO StandAloneJobContainerCommunicator - Total 100000 records, 2600000 bytes | Speed 253.91KB/s, 10000 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.030s | All Task WaitReaderTime 0.054s | Percentage 100.00% 2019-07-03 17:30:10.267 [job-0] INFO JobContainer - 任务启动时刻 : 2019-07-03 17:30:00 任务结束时刻 : 2019-07-03 17:30:10 任务总计耗时 : 10s 任务平均流量 : 253.91KB/s 记录写入速度 : 10000rec/s 读出记录总数 : 100000 读写失败总数 : 0 WZB-MacBook:bin wangzhibin$ 查看模板 可以通过以下命令来自动生成一个json模板，这里的命令是生成stream to stream的json模板，如果是MySQL to Oracle，就执行命令：python datax.py -r mysql -w oracle。 WZB-MacBook:bin wangzhibin$ python datax.py -r streamreader -w streamwriter DataX (DATAX-OPENSOURCE-3.0), From Alibaba ! Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved. Please refer to the streamreader document: https://github.com/alibaba/DataX/blob/master/streamreader/doc/streamreader.md Please refer to the streamwriter document: https://github.com/alibaba/DataX/blob/master/streamwriter/doc/streamwriter.md Please save the following configuration as a json file and use python {DATAX_HOME}/bin/datax.py {JSON_FILE_NAME}.json to run the job. { \"job\": { \"content\": [ { \"reader\": { \"name\": \"streamreader\", \"parameter\": { \"column\": [], \"sliceRecordCount\": \"\" } }, \"writer\": { \"name\": \"streamwriter\", \"parameter\": { \"encoding\": \"\", \"print\": true } } } ], \"setting\": { \"speed\": { \"channel\": \"\" } } } } Stream to Stream 这个示例，是通过流输入同步到流输出。 WZB-MacBook:job wangzhibin$ pwd /usr/local/Cellar_w/datax/job WZB-MacBook:job wangzhibin$ cat > stream2stream.json com.sun.management.UnixOperatingSystem 2019-07-03 17:48:57.012 [main] INFO Engine - the machine info => osInfo: Oracle Corporation 1.7 24.80-b11 jvmInfo: Mac OS X x86_64 10.14.5 cpu num: 8 totalPhysicalMemory: 16.00G freePhysicalMemory: 0.03G maxFileDescriptorCount: 10240 currentOpenFileDescriptorCount: 76 GC Names [PS MarkSweep, PS Scavenge] MEMORY_NAME | allocation_size | init_size PS Survivor Space | 42.50MB | 42.50MB PS Old Gen | 683.00MB | 683.00MB PS Eden Space | 256.50MB | 256.50MB Code Cache | 48.00MB | 2.44MB PS Perm Gen | 82.00MB | 21.00MB 2019-07-03 17:48:57.034 [main] INFO Engine - { \"content\":[ { \"reader\":{ \"name\":\"streamreader\", \"parameter\":{ \"column\":[ { \"type\":\"String\", \"value\":\"helle DataX\" }, { \"type\":\"string\", \"value\":\"你吃完晚饭了吗？\" }, { \"type\":\"string\", \"value\":\"不要在写代码了，太累了\" } ], \"sliceRecordCount\":10 } }, \"writer\":{ \"name\":\"streamwriter\", \"parameter\":{ \"encoding\":\"GBK\", \"print\":true } } } ], \"setting\":{ \"speed\":{ \"channel\":5 } } } 2019-07-03 17:48:57.065 [main] WARN Engine - prioriy set to 0, because NumberFormatException, the value is: null 2019-07-03 17:48:57.068 [main] INFO PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0 2019-07-03 17:48:57.068 [main] INFO JobContainer - DataX jobContainer starts job. 2019-07-03 17:48:57.070 [main] INFO JobContainer - Set jobId = 0 2019-07-03 17:48:57.093 [job-0] INFO JobContainer - jobContainer starts to do prepare ... 2019-07-03 17:48:57.094 [job-0] INFO JobContainer - DataX Reader.Job [streamreader] do prepare work . 2019-07-03 17:48:57.094 [job-0] INFO JobContainer - DataX Writer.Job [streamwriter] do prepare work . 2019-07-03 17:48:57.094 [job-0] INFO JobContainer - jobContainer starts to do split ... 2019-07-03 17:48:57.094 [job-0] INFO JobContainer - Job set Channel-Number to 5 channels. 2019-07-03 17:48:57.096 [job-0] INFO JobContainer - DataX Reader.Job [streamreader] splits to [5] tasks. 2019-07-03 17:48:57.096 [job-0] INFO JobContainer - DataX Writer.Job [streamwriter] splits to [5] tasks. 2019-07-03 17:48:57.122 [job-0] INFO JobContainer - jobContainer starts to do schedule ... 2019-07-03 17:48:57.132 [job-0] INFO JobContainer - Scheduler starts [1] taskGroups. 2019-07-03 17:48:57.135 [job-0] INFO JobContainer - Running by standalone Mode. 2019-07-03 17:48:57.150 [taskGroup-0] INFO TaskGroupContainer - taskGroupId=[0] start [5] channels for [5] tasks. 2019-07-03 17:48:57.156 [taskGroup-0] INFO Channel - Channel set byte_speed_limit to -1, No bps activated. 2019-07-03 17:48:57.156 [taskGroup-0] INFO Channel - Channel set record_speed_limit to -1, No tps activated. 2019-07-03 17:48:57.177 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[2] attemptCount[1] is started helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 2019-07-03 17:48:57.183 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 2019-07-03 17:48:57.189 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[4] attemptCount[1] is started helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 2019-07-03 17:48:57.198 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[3] attemptCount[1] is started helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 2019-07-03 17:48:57.203 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[1] attemptCount[1] is started helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 helle DataX 你吃完晚饭了吗？ 不要在写代码了，太累了 2019-07-03 17:48:57.307 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[125]ms 2019-07-03 17:48:57.308 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[2] is successed, used[137]ms 2019-07-03 17:48:57.308 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[1] is successed, used[106]ms 2019-07-03 17:48:57.308 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[3] is successed, used[110]ms 2019-07-03 17:48:57.309 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[4] is successed, used[120]ms 2019-07-03 17:48:57.310 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] completed it's tasks. 2019-07-03 17:49:07.170 [job-0] INFO StandAloneJobContainerCommunicator - Total 50 records, 1500 bytes | Speed 150B/s, 5 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.000s | All Task WaitReaderTime 0.007s | Percentage 100.00% 2019-07-03 17:49:07.171 [job-0] INFO AbstractScheduler - Scheduler accomplished all tasks. 2019-07-03 17:49:07.173 [job-0] INFO JobContainer - DataX Writer.Job [streamwriter] do post work. 2019-07-03 17:49:07.175 [job-0] INFO JobContainer - DataX Reader.Job [streamreader] do post work. 2019-07-03 17:49:07.175 [job-0] INFO JobContainer - DataX jobId [0] completed successfully. 2019-07-03 17:49:07.177 [job-0] INFO HookInvoker - No hook invoked, because base dir not exists or is a file: /usr/local/Cellar_w/raw/datax/hook 2019-07-03 17:49:07.189 [job-0] INFO JobContainer - [total cpu info] => averageCpu | maxDeltaCpu | minDeltaCpu 1.38% | 1.38% | 1.38% [total gc info] => NAME | totalGCCount | maxDeltaGCCount | minDeltaGCCount | totalGCTime | maxDeltaGCTime | minDeltaGCTime PS MarkSweep | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s PS Scavenge | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s 2019-07-03 17:49:07.189 [job-0] INFO JobContainer - PerfTrace not enable! 2019-07-03 17:49:07.190 [job-0] INFO StandAloneJobContainerCommunicator - Total 50 records, 1500 bytes | Speed 150B/s, 5 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.000s | All Task WaitReaderTime 0.007s | Percentage 100.00% 2019-07-03 17:49:07.192 [job-0] INFO JobContainer - 任务启动时刻 : 2019-07-03 17:48:57 任务结束时刻 : 2019-07-03 17:49:07 任务总计耗时 : 10s 任务平均流量 : 150B/s 记录写入速度 : 5rec/s 读出记录总数 : 50 读写失败总数 : 0 MySQL to MySQL 以下示例，将本地一个MySQL的数据同步到另一个库中同名表，其中一个字段名称不一样。 WZB-MacBook:job wangzhibin$ cat > mysql2mysql.json com.sun.management.UnixOperatingSystem 2019-07-03 18:03:20.233 [main] INFO Engine - the machine info => osInfo: Oracle Corporation 1.7 24.80-b11 jvmInfo: Mac OS X x86_64 10.14.5 cpu num: 8 totalPhysicalMemory: 16.00G freePhysicalMemory: 0.04G maxFileDescriptorCount: 10240 currentOpenFileDescriptorCount: 76 GC Names [PS MarkSweep, PS Scavenge] MEMORY_NAME | allocation_size | init_size PS Survivor Space | 42.50MB | 42.50MB PS Old Gen | 683.00MB | 683.00MB PS Eden Space | 256.50MB | 256.50MB Code Cache | 48.00MB | 2.44MB PS Perm Gen | 82.00MB | 21.00MB 2019-07-03 18:03:20.256 [main] INFO Engine - { \"content\":[ { \"reader\":{ \"name\":\"mysqlreader\", \"parameter\":{ \"column\":[ \"ID\", \"USER_NAME\", \"USER_DESC\" ], \"connection\":[ { \"jdbcUrl\":[ \"jdbc:mysql://localhost:3306/test_hdfs_import?useUnicode=true&characterEncoding=utf8\" ], \"table\":[ \"user\" ] } ], \"password\":\"*****\", \"username\":\"root\" } }, \"writer\":{ \"name\":\"mysqlwriter\", \"parameter\":{ \"column\":[ \"ID\", \"USER_NAME\", \"USER_DESC_1\" ], \"connection\":[ { \"jdbcUrl\":\"jdbc:mysql://localhost:3306/test_datax_import?characterEncoding=utf8\", \"table\":[ \"user\" ] } ], \"password\":\"*****\", \"username\":\"root\" } } } ], \"setting\":{ \"speed\":{ \"channel\":1 } } } 2019-07-03 18:03:20.284 [main] WARN Engine - prioriy set to 0, because NumberFormatException, the value is: null 2019-07-03 18:03:20.286 [main] INFO PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0 2019-07-03 18:03:20.286 [main] INFO JobContainer - DataX jobContainer starts job. 2019-07-03 18:03:20.288 [main] INFO JobContainer - Set jobId = 0 2019-07-03 18:03:20.725 [job-0] INFO OriginalConfPretreatmentUtil - Available jdbcUrl:jdbc:mysql://localhost:3306/test_hdfs_import?yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true. 2019-07-03 18:03:20.738 [job-0] INFO OriginalConfPretreatmentUtil - table:[user] has columns:[ID,USER_NAME,USER_DESC]. 2019-07-03 18:03:21.007 [job-0] INFO OriginalConfPretreatmentUtil - table:[user] all columns:[ ID,USER_NAME,USER_DESC_1 ]. 2019-07-03 18:03:21.015 [job-0] INFO OriginalConfPretreatmentUtil - Write data [ INSERT INTO %s (ID,USER_NAME,USER_DESC_1) VALUES(?,?,?) ], which jdbcUrl like:[jdbc:mysql://localhost:3306/test_datax_import?yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true] 2019-07-03 18:03:21.015 [job-0] INFO JobContainer - jobContainer starts to do prepare ... 2019-07-03 18:03:21.015 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] do prepare work . 2019-07-03 18:03:21.016 [job-0] INFO JobContainer - DataX Writer.Job [mysqlwriter] do prepare work . 2019-07-03 18:03:21.017 [job-0] INFO JobContainer - jobContainer starts to do split ... 2019-07-03 18:03:21.017 [job-0] INFO JobContainer - Job set Channel-Number to 1 channels. 2019-07-03 18:03:21.022 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] splits to [1] tasks. 2019-07-03 18:03:21.022 [job-0] INFO JobContainer - DataX Writer.Job [mysqlwriter] splits to [1] tasks. 2019-07-03 18:03:21.044 [job-0] INFO JobContainer - jobContainer starts to do schedule ... 2019-07-03 18:03:21.050 [job-0] INFO JobContainer - Scheduler starts [1] taskGroups. 2019-07-03 18:03:21.053 [job-0] INFO JobContainer - Running by standalone Mode. 2019-07-03 18:03:21.066 [taskGroup-0] INFO TaskGroupContainer - taskGroupId=[0] start [1] channels for [1] tasks. 2019-07-03 18:03:21.070 [taskGroup-0] INFO Channel - Channel set byte_speed_limit to -1, No bps activated. 2019-07-03 18:03:21.071 [taskGroup-0] INFO Channel - Channel set record_speed_limit to -1, No tps activated. 2019-07-03 18:03:21.082 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started 2019-07-03 18:03:21.085 [0-0-0-reader] INFO CommonRdbmsReader$Task - Begin to read record by Sql: [select ID,USER_NAME,USER_DESC from user ] jdbcUrl:[jdbc:mysql://localhost:3306/test_hdfs_import?yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true]. 2019-07-03 18:03:21.103 [0-0-0-reader] INFO CommonRdbmsReader$Task - Finished read record by Sql: [select ID,USER_NAME,USER_DESC from user ] jdbcUrl:[jdbc:mysql://localhost:3306/test_hdfs_import?yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true]. 2019-07-03 18:03:21.186 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[105]ms 2019-07-03 18:03:21.187 [taskGroup-0] INFO TaskGroupContainer - taskGroup[0] completed it's tasks. 2019-07-03 18:03:31.087 [job-0] INFO StandAloneJobContainerCommunicator - Total 2 records, 34 bytes | Speed 3B/s, 0 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.000s | All Task WaitReaderTime 0.000s | Percentage 100.00% 2019-07-03 18:03:31.087 [job-0] INFO AbstractScheduler - Scheduler accomplished all tasks. 2019-07-03 18:03:31.088 [job-0] INFO JobContainer - DataX Writer.Job [mysqlwriter] do post work. 2019-07-03 18:03:31.090 [job-0] INFO JobContainer - DataX Reader.Job [mysqlreader] do post work. 2019-07-03 18:03:31.090 [job-0] INFO JobContainer - DataX jobId [0] completed successfully. 2019-07-03 18:03:31.092 [job-0] INFO HookInvoker - No hook invoked, because base dir not exists or is a file: /usr/local/Cellar_w/raw/datax/hook 2019-07-03 18:03:31.101 [job-0] INFO JobContainer - [total cpu info] => averageCpu | maxDeltaCpu | minDeltaCpu 2.31% | 2.31% | 2.31% [total gc info] => NAME | totalGCCount | maxDeltaGCCount | minDeltaGCCount | totalGCTime | maxDeltaGCTime | minDeltaGCTime PS MarkSweep | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s PS Scavenge | 0 | 0 | 0 | 0.000s | 0.000s | 0.000s 2019-07-03 18:03:31.101 [job-0] INFO JobContainer - PerfTrace not enable! 2019-07-03 18:03:31.102 [job-0] INFO StandAloneJobContainerCommunicator - Total 2 records, 34 bytes | Speed 3B/s, 0 records/s | Error 0 records, 0 bytes | All Task WaitWriterTime 0.000s | All Task WaitReaderTime 0.000s | Percentage 100.00% 2019-07-03 18:03:31.103 [job-0] INFO JobContainer - 任务启动时刻 : 2019-07-03 18:03:20 任务结束时刻 : 2019-07-03 18:03:31 任务总计耗时 : 10s 任务平均流量 : 3B/s 记录写入速度 : 0rec/s 读出记录总数 : 2 读写失败总数 : 0 乱码问题： 解决：在reader、writer中的jdbcurl中增加?characterEncoding=utf8。 参考资料 阿里云开源离线同步工具DataX3.0介绍-云栖社区-阿里云 DataX/introduction.md at master · alibaba/DataX · GitHub DataX/userGuid.md at master · alibaba/DataX · GitHub DataX在有赞大数据平台的实践 win10 安装DataX - 周天祥的博客 - CSDN博客 DataX使用指南 - 陪你听风的博客 - CSDN博客 ChangeLog 20190703 | 创建文档，安装DataX，成功执行stream2stream、mysql2mysql两种数据源同步任务。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-03 18:25:06 "},"20180712LinuxCommands.html":{"url":"20180712LinuxCommands.html","title":"Linux常用命令","keywords":"","body":"技术 | Linux常用命令 全文检索 find / -print -type f | xargs grep '/var/lib/psql/data' 查看端口占用情况 lsof -i:端口号 netstat -tunlp|grep 端口号 这两种方式都可以查看指定端口被哪个进程占用的情况。 查看当前占用内存较多的程序 ps aux --sort -rss | more 命令说明： ps aux: 列出目前所有的正在内存当中的程序。 a显示终端上的所有进程,包括其他用户地进程(有的进程没有终端)。 -a 显示所有终端机下执行的进程。 -u 以用户为主的格式来显示进程状况。 -x 显示所有进程，不以终端机来区分。 a会包括其他用户(否则只有用户本身); x会包括其他终端; aux就可以包括内存所有; 命令结果说明： USER：该 process 属于那个使用者账号的 PID ：该 process 的进程号 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 Linux 查看CPU信息，机器型号，内存等信息 系统 # uname -a # 查看内核/操作系统/CPU信息 # lsb_release -a # 查看操作系统版本 (适用于所有的linux，包括Redhat、SuSE、Debian等发行版，但是在debian下要安装lsb) # cat /etc/issue | cat /etc/redhat-release # 查看CentOS操作系统版本。 # cat /proc/cpuinfo # 查看CPU信息 # hostname # 查看计算机名 # lspci -tv # 列出所有PCI设备 # lsusb -tv # 列出所有USB设备 # lsmod # 列出加载的内核模块 # env # 查看环境变量 资源 # free -m # 查看内存使用量和交换区使用量 # df -h # 查看各分区使用情况 # du -sh # 查看指定目录的大小 # grep MemTotal /proc/meminfo # 查看内存总量 # grep MemFree /proc/meminfo # 查看空闲内存量 # uptime # 查看系统运行时间、用户数、负载 # cat /proc/loadavg # 查看系统负载 磁盘和分区 # mount | column -t # 查看挂接的分区状态 # fdisk -l # 查看所有分区 # swapon -s # 查看所有交换分区 # hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) # dmesg | grep IDE # 查看启动时IDE设备检测状况 网络 # ifconfig # 查看所有网络接口的属性 # iptables -L # 查看防火墙设置 # route -n # 查看路由表 # netstat -lntp # 查看所有监听端口 # netstat -antp # 查看所有已经建立的连接 # netstat -s # 查看网络统计信息 进程 # ps -ef # 查看所有进程 # top # 实时显示进程状态 # jps # 查看java进程 用户 # w # 查看活动用户 # id # 查看指定用户信息 # last # 查看用户登录日志 # cut -d: -f1 /etc/passwd # 查看系统所有用户 # cut -d: -f1 /etc/group # 查看系统所有组 # crontab -l # 查看当前用户的计划任务 服务 # chkconfig --list # 列出所有系统服务 # chkconfig --list | grep on # 列出所有启动的系统服务 程序 # rpm -qa # 查看所有安装的软件包 查看CPU信息（型号） # cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 8 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz (看到有8个逻辑CPU, 也知道了CPU型号) # cat /proc/cpuinfo | grep physical | uniq -c 4 physical id : 0 4 physical id : 1 (说明实际上是两颗4核的CPU) # getconf LONG_BIT 32 (说明当前CPU运行在32bit模式下, 但不代表CPU不支持64bit) # cat /proc/cpuinfo | grep flags | grep ' lm ' | wc -l 8 (结果大于0, 说明支持64bit计算. lm指long mode, 支持lm则是64bit) 再完整看cpu详细信息, 不过大部分我们都不关心而已. # dmidecode | grep 'Processor Information' 查看内存信息 # cat /proc/meminfo # uname -a Linux euis1 2.6.9-55.ELsmp #1 SMP Fri Apr 20 17:03:35 EDT 2007 i686 i686 i386 GNU/Linux (查看当前操作系统内核信息) # cat /etc/issue | grep Linux Red Hat Enterprise Linux AS release 4 (Nahant Update 5) (查看当前操作系统发行版信息) 查看机器型号 # dmidecode | grep \"Product Name\" 查看网卡信息 # dmesg | grep -i eth 设置网络信息（以CentOS为例） ifconfig查看当前网卡名称与信息 [root@node1 ~]# ifconfig eth0 Link encap:Ethernet HWaddr 00:0C:29:A4:9D:C2 inet addr:192.168.129.137 Bcast:192.168.129.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fea4:9dc2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:908629128 errors:0 dropped:0 overruns:0 frame:0 TX packets:820212029 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:101322481671 (94.3 GiB) TX bytes:109980885936 (102.4 GiB) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:90328695 errors:0 dropped:0 overruns:0 frame:0 TX packets:90328695 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:6989862905 (6.5 GiB) TX bytes:6989862905 (6.5 GiB) 设置网络信息 vi /etc/sysconfig/network-scripts/ifcfg-eth0 内容示例： DEVICE=eth0 TYPE=Ethernet ONBOOT=yes NM_CONTROLLED=yes BOOTPROTO=none DEFROUTE=yes IPV4_FAILURE_FATAL=yes IPV6INIT=no NAME=\"System eth0\" HWADDR=00:0C:29:A4:9D:C2 IPADDR=192.168.129.137 PREFIX=24 GATEWAY=192.168.129.10 UUID=5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03 LAST_CONNECT=1516781563 DNS1=202.102.192.68 重新网卡 service network restart Linux基本操作（以CentOS 7.2为例） 设置主机名 //永久性的修改主机名称，重启后能保持修改后的。 hostnamectl set-hostname xxx 或者修改/etc/hostname。 创建用户并添加sudo权限 #添加用户组dev groupadd dev #添加libb用户，并归属于dev组（增加-m 自动创建目录） useradd -m -g dev libb #给tydic用户改密码 passwd libb #给已有的用户增加工作组 gpasswd -a user group #查看组以及组员 cat /etc/group 添加sudo权限 #sudoers 文件添加可写权限 chmod -v u+w /etc/sudoers #打开sudoers文件添加 #表示dev组下所有用户都可以执行sudo，且不用密码 %dev ALL=(ALL) NOPASSWD: ALL #表示libb用户可以执行sudo，且不用密码 libb ALL=(ALL) NOPASSWD: ALL #表示libb用户可以执行sudo libb ALL=(ALL) ALL #最后取消sudoers 文件可写权限 chmod -v u-w /etc/sudoers 关闭防火墙 #firewall-cmd --state // 查看防火墙状态 # systemctl stop firewalld.service #停止firewall # systemctl disable firewalld.service #禁止firewall开机启动 配置yum源 不建议使用CentOS 7 自带的yum源，因为安装软件和依赖时会非常慢甚至超时失败。这里，我们使用阿里云的源予以替换，执行如下命令，替换文件 /etc/yum.repos.d/CentOS-Base.repo。 # wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # yum makecache SSH免密登录 比如：有两台服务器 Node1 与 Node2，希望彼此SSH免密登录 先在两台服务器都执行以下命令： # ssh-keygen -t rsa # cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 然后在每台服务器都执行： scp xxx@Nodex:~/.ssh/id_rsa.pub ~/.ssh/tmp # 如：scp root@192.168.129.114:~/.ssh/id_rsa.pub ~/.ssh/tmp cat ~/.ssh/tmp >> ~/.ssh/authorized_keys rm -rf ~/.ssh/tmp ChangeLog 20190620 | 增加「配置yum源」 20190618 | 增加「Linux基本操作命令」 20190612 | 增加「查看当前占用内存较多的程序」 20190605 | 增加「查看端口占用情况」 20180712 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-03 18:35:41 "},"20190612ProblemOfSpringbootProgram.html":{"url":"20190612ProblemOfSpringbootProgram.html","title":"SpringBoot程序内存占用过高解决方法","keywords":"","body":"SpringBoot程序内存占用过高解决方法 问题 使用top查看服务器内存占用情况，发现交换内存也已经被占用很多了，说明正在运行的程序占用内存非常高。 使用ps aux --sort -rss命令查看各个程序占用内存数如下，发现使用SpringBoot打包的程序运行占用内存达到2G以上，而实际程序包大小大约40M左右。 初步判断是JVM设置问题。 关于jvm配置 参考 IntelliJ IDEA设置JVM运行参数 有如下描述: 设置JVM内存的参数有四个： -Xmx Java Heap最大值，默认值为物理内存的1/4，最佳设值应该视物理内存大小及计算机内其他内存开销而定； -Xms Java Heap初始值，Server端JVM最好将-Xms和-Xmx设为相同值，开发测试机JVM可以保留默认值； -Xmn Java Heap Young区大小，不熟悉最好保留默认值； -Xss 每个线程的Stack大小，不熟悉最好保留默认值； 参考 java--jvm启动的参数 有如下描述： 一般用到最多的是： -Xms512m 设置JVM促使内存为512m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 -Xmx512m ，设置JVM最大可用内存为512M。 -Xmn200m：设置年轻代大小为200M。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 -Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。(实际发现-Xss128k设置无法启动SpringBoot程序，并提示至少需要228k。) JVM参数设置示例： java -Xms64m #JVM启动时的初始堆大小 -Xmx128m #最大堆大小 -Xmn64m #年轻代的大小，其余的空间是老年代 -XX:MaxMetaspaceSize=128m # -XX:CompressedClassSpaceSize=64m #使用 -XX：CompressedClassSpaceSize 设置为压缩类空间保留的最大内存。 -Xss256k #线程 -XX:InitialCodeCacheSize=4m # -XX:ReservedCodeCacheSize=8m # 这是由 JIT（即时）编译器编译为本地代码的本机代码（如JNI）或 Java 方法的空间 -XX:MaxDirectMemorySize=16m -jar app.jar 参考启动脚本 java -Xms512m -Xmx512m -Xmn200m -Xss500k -jar dap-service-manager-0.0.1.jar --server.port=9200 实际效果：能将程序占用内存数减少至600M所有。 参考资料 Spring cloud开发内存占用过高解决方法 ChangeLog 20190612 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-12 14:45:01 "},"20190619VMInstallRemotely.html":{"url":"20190619VMInstallRemotely.html","title":"使用vmfusion远程安装虚拟机","keywords":"","body":"使用vmfusion远程安装虚拟机 连接远程服务器 选择「在远程服务器上创建虚拟机」 输入远程地址、用户名、密码，点击连接即可。 连接完成后，即展示该服务器上的所有虚拟机。 创建虚拟机 点击「创建新的虚拟机」，选择「localhost.localdomain()」，选择存储盘，点击「继续」。 选择硬件版本，默认即可。 选择网络，默认即可。 选择操作系统「Linux」-「CentOS 4/5/6/7(64位)」。 选择固件类型，默认即可。 配置虚拟磁盘。 修改虚拟机名称，然后点击「自定义设置」。 修改处理和内存。 点击「添加设备」-「CD/DVD驱动器」。 选择主机中的ISO文件。（如果没有请参考上传ISO文件到主机） 或者选择「CD/DVD（IDE）」，加载本地ISO文件。 开始安装系统 选择安装过程中的语言 初始化安装信息——安装位置。 初始化安装信息——网络和主机名（DNS：202.102.192.68） 开始安装 设置Root密码 创建用户 等待安装完成即可。 上传ISO文件到主机 服务器安装的是Vmware ESXi服务器，并不是普通的Linux服务器，访问https://192.168.129.95/ui/，使用用户名密码即可登录管理服务器。 选择「存储」-「datastore1」-「数据存储浏览器」，即可上传下载文件到主机了。 参考资料 ChangeLog 20190619 | 新建文档。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-21 12:33:02 "},"20190620InstallationOfDockerAndK8s.html":{"url":"20190620InstallationOfDockerAndK8s.html","title":"Docker + K8S 集群环境安装指南","keywords":"","body":"CentOS 7 安装 Docker + K8S 集群指南 安装环境准备 操作系统：CentOS 7 64位 系统内核：3.10以上 关闭防火墙 安装yum-config-manager：yum -y install yum-utils。增加Docker-CE仓库需要这个工具。 新手入门 问题： 安装 Docker 需要准备什么？ 如何搭建 Docker 集群？Docker 集群使用 k8s ？ 安装 Docker 后能达到什么效果？ 安装 Docker 参考官方安装文档 增加 Docker 仓库 # 建议使用阿里云仓库，如果使用官方仓库可能访问不了。你懂的~ # yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo [root@bd129137 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 已加载插件：fastestmirror adding repo from: https://download.docker.com/linux/centos/docker-ce.repo grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo repo saved to /etc/yum.repos.d/docker-ce.repo [root@bd129137 ~]# yum repolist 已加载插件：fastestmirror Loading mirror speeds from cached hostfile * base: mirrors.163.com * extras: mirrors.cn99.com * updates: mirrors.cn99.com 源标识 源名称 状态 base/7/x86_64 CentOS-7 - Base 10,019 docker-ce-stable/x86_64 Docker CE Stable - x86_64 43 extras/7/x86_64 CentOS-7 - Extras 419 updates/7/x86_64 CentOS-7 - Updates 2,089 repolist: 12,570 使用 yum 安装 Docker-CE # yum -y install docker-ce docker-ce-cli containerd.io 启动 Docker 后台服务 # systemctl start docker 查看 Docker 版本 [root@bd129137 ~]# yum list docker-ce 已加载插件：fastestmirror Loading mirror speeds from cached hostfile * base: mirrors.163.com * extras: mirrors.cn99.com * updates: mirrors.cn99.com 已安装的软件包 docker-ce.x86_64 3:18.09.6-3.el7 @docker-ce-stable [root@bd129137 ~]# yum list docker-ce --showduplicates | sort -r 查看可用版本 [root@bd129137 ~]# docker --version Docker version 18.09.6, build 481bc77156 查看 Docker 信息 Docker 根目录：/var/lib/docker [root@bd129137 ~]# docker info Containers: 1 Running: 0 Paused: 0 Stopped: 1 Images: 1 Server Version: 18.09.6 Storage Driver: devicemapper Pool Name: docker-253:0-201703810-pool Pool Blocksize: 65.54kB Base Device Size: 10.74GB Backing Filesystem: xfs Udev Sync Supported: true Data file: /dev/loop0 Metadata file: /dev/loop1 Data loop file: /var/lib/docker/devicemapper/devicemapper/data Metadata loop file: /var/lib/docker/devicemapper/devicemapper/metadata Data Space Used: 19.33MB Data Space Total: 107.4GB Data Space Available: 51.17GB Metadata Space Used: 593.9kB Metadata Space Total: 2.147GB Metadata Space Available: 2.147GB Thin Pool Minimum Free Space: 10.74GB Deferred Removal Enabled: true Deferred Deletion Enabled: true Deferred Deleted Device Count: 0 Library Version: 1.02.149-RHEL7 (2018-07-20) Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84 runc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30 init version: fec3683 Security Options: seccomp Profile: default Kernel Version: 3.10.0-327.el7.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 12 Total Memory: 31.25GiB Name: bd129137 ID: X3VZ:RZUQ:KIZQ:LRR6:6CCP:KG2R:HHH4:CGB2:EXBD:OMH2:QLYB:OUJN Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false Product License: Community Engine WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled WARNING: the devicemapper storage-driver is deprecated, and will be removed in a future release. WARNING: devicemapper: usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device. 验证 Docker [root@bd129137 ~]# docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:41a65640635299bab090f783209c1e3a3f11934cf7756b09cb2f1e02147c6ed8 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 走过的坑 获取密钥失败 安装期间出现如下错误： warning: /var/cache/yum/x86_64/7/docker-ce-stable/packages/containerd.io-1.2.5-3.1.el7.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 621e9f35: NOKEY 从 https://download.docker.com/linux/centos/gpg 检索密钥 获取 GPG 密钥失败：[Errno 12] Timeout on https://download.docker.com/linux/centos/gpg: (28, 'Operation timed out after 30001 milliseconds with 0 out of 0 bytes received') 原因：连接 download.docker.com 超时。 解决方法：在增加 Docker 仓库时选择阿里云的仓库。速度杠杠的。 # yum-config-manager --disable docker-ce-stable # yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 参考CentOS Docker 安装。 Docker 命令 基础命令 # docker version # 查看docker的版本号，包括客户端、服务端、依赖的Go等 # docker info # 查看系统(docker)层面信息，包括管理的images, containers数等 # docker pull centos #下载镜像 # docker images #查看镜像 # docker run -i -t centos /bin/bash #启动一个bash应用容器 使用镜像 # docker search # 在docker index中搜索image # docker pull # 从docker registry server 中下拉image # docker images # 列出images # docker images -a # 列出所有的images（包含历史） # docker rmi ： # 删除一个或多个image 使用容器 # 使用镜像创建容器 # docker run -i -t sauloal/ubuntu14.04 # docker run -i -t sauloal/ubuntu14.04 /bin/bash # 创建一个容器，让其中运行 bash 应用，退出后容器关闭 # docker run -itd --name centos_aways --restart=always centos #创建一个名称centos_aways的容器，自动重启 # --restart参数：always始终重启；on-failure退出状态非0时重启；默认为，no不重启 # 查看容器 # docker ps ：列出当前所有正在运行的container # docker ps -l ：列出最近一次启动的container # docker ps -a ：列出所有的container（包含历史，即运行过的container） # docker ps -q ：列出最近一次运行的container ID # 再次启动容器 # docker start/stop/restart #：开启/停止/重启container # docker start [container_id] #：再次运行某个container （包括历史container） #进入正在运行的docker容器 # docker exec -it [container_id] /bin/bash # docker run -i -t -p #：映射 HOST 端口到容器，方便外部访问容器内服务，host_port 可以省略，省略表示把 # container_port 映射到一个动态端口。 # 删除容器 # docker rm #：删除一个或多个container # docker rm `docker ps -a -q` #：删除所有的container # docker ps -a -q | xargs docker rm #：同上, 删除所有的container 备注：docker run 和 docker create 参数基本一样，run是创建容器并后台启动，create是只创建容器。 docker run 相当于docker create 和 docker start Docker run 命令详解 命令格式：docker run [OPTIONS] IMAGE [COMMAND] [ARG...] Usage: Run a command in a new container 中文意思为：通过run命令创建一个新的容器（container） 常用选项说明 -d, --detach=false， 指定容器运行于前台还是后台，默认为false -i, --interactive=false， 打开STDIN，用于控制台交互 -t, --tty=false， 分配tty设备，该可以支持终端登录，默认为false -u, --user=\"\"， 指定容器的用户 -a, --attach=[]， 登录容器（必须是以docker run -d启动的容器） -w, --workdir=\"\"， 指定容器的工作目录 -c, --cpu-shares=0， 设置容器CPU权重，在CPU共享场景使用 -e, --env=[]， 指定环境变量，容器中可以使用该环境变量 -m, --memory=\"\"， 指定容器的内存上限 -P, --publish-all=false， 指定容器暴露的端口 -p, --publish=[]， 指定容器暴露的端口 -h, --hostname=\"\"， 指定容器的主机名 -v, --volume=[]， 给容器挂载存储卷，挂载到容器的某个目录 --volumes-from=[]， 给容器挂载其他容器上的卷，挂载到容器的某个目录 --cap-add=[]， 添加权限，权限清单详见：http://linux.die.net/man/7/capabilities --cap-drop=[]， 删除权限，权限清单详见：http://linux.die.net/man/7/capabilities --cidfile=\"\"， 运行容器后，在指定文件中写入容器PID值，一种典型的监控系统用法 --cpuset=\"\"， 设置容器可以使用哪些CPU，此参数可以用来容器独占CPU --device=[]， 添加主机设备给容器，相当于设备直通 --dns=[]， 指定容器的dns服务器 --dns-search=[]， 指定容器的dns搜索域名，写入到容器的/etc/resolv.conf文件 --entrypoint=\"\"， 覆盖image的入口点 --env-file=[]， 指定环境变量文件，文件格式为每行一个环境变量 --expose=[]， 指定容器暴露的端口，即修改镜像的暴露端口 --link=[]， 指定容器间的关联，使用其他容器的IP、env等信息 --lxc-conf=[]， 指定容器的配置文件，只有在指定--exec-driver=lxc时使用 --name=\"\"， 指定容器名字，后续可以通过名字进行容器管理，links特性需要使用名字 --net=\"bridge\"， 容器网络设置: bridge 使用docker daemon指定的网桥 host //容器使用主机的网络 container:NAME_or_ID >//使用其他容器的网路，共享IP和PORT等网络资源 none 容器使用自己的网络（类似--net=bridge），但是不进行配置 --privileged=false， 指定容器是否为特权容器，特权容器拥有所有的capabilities --restart=\"no\"， 指定容器停止后的重启策略: no：容器退出时不重启 on-failure：容器故障退出（返回值非零）时重启 always：容器退出时总是重启 --rm=false， 指定容器停止后自动删除容器(不支持以docker run -d启动的容器) --sig-proxy=true， 设置由代理接受并处理信号，但是SIGCHLD、SIGSTOP和SIGKILL不能被代理 示例 运行一个在后台执行的容器，同时，还能用控制台管理：docker run -i -t -d ubuntu:latest 运行一个带命令在后台不断执行的容器，不直接展示容器内部信息：docker run -d ubuntu:latest ping www.docker.com 运行一个在后台不断执行的容器，同时带有命令，程序被终止后还能重启继续跑，还能用控制台管理，docker run -d --restart=always ubuntu:latest ping www.docker.com 为容器指定一个名字，docker run -d --name=ubuntu_server ubuntu:latest 容器暴露80端口，并指定宿主机80端口与其通信(: 之前是宿主机端口，之后是容器需暴露的端口)，docker run -d --name=ubuntu_server -p 80:80 ubuntu:latest 指定容器内目录与宿主机目录共享(: 之前是宿主机文件夹，之后是容器需共享的文件夹)，docker run -d --name=ubuntu_server -v /etc/www:/var/www ubuntu:latest 安装Kubernetes 官方文档永远是最好的参考资料：https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 配置K8S的yum源 官方仓库无法使用，建议使用阿里源的仓库，执行以下命令添加kubernetes.repo仓库： # cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 关闭swap # k8s要求关闭swap # swapoff -a && sysctl -w vm.swappiness=0 # 关闭swap # sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab # 取消开机挂载swap 关闭SeLinux # 关闭SElinux # setenforce 0 # sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config 安装K8S组件 # 安装kubelet kubeadm kubectl # yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 修改docker cgroup驱动 修改docker cgroup驱动，与k8s一致，使用systemd # 修改docker cgroup驱动：native.cgroupdriver=systemd # cat > /etc/docker/daemon.json 启动kubelet 根据官方文档描述，安装kubelet、kubeadm、kubectl三者后，要求启动kubelet： # systemctl enable kubelet && systemctl start kubelet 但实际测试发现，无法启动，报如下错误： [root@bd129137 ~]# systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: activating (auto-restart) (Result: exit-code) since 四 2019-06-20 19:50:46 CST; 6s ago Docs: https://kubernetes.io/docs/ Process: 67625 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=255) Main PID: 67625 (code=exited, status=255) 6月 20 19:50:46 bd129137 systemd[1]: Unit kubelet.service entered failed state. 6月 20 19:50:46 bd129137 systemd[1]: kubelet.service failed. 解决方案：暂时不需要启动，后面的kubeadm init操作会创建证书，继续往后操作，才能启动。 创建Docker集群 使用虚拟机的可以做完以上步骤后，进行克隆，以下区分 Master 与 Node 节点。规划集群为1 Master，2 Node。 准备工作 Master端 kubeadm config images pull # 拉取集群所需镜像，这个需要翻墙 # --- 不能翻墙可以尝试以下办法 --- kubeadm config images list # 列出所需镜像 #(不是一定是下面的,根据实际情况来) # 根据所需镜像名字先拉取国内资源 docker pull mirrorgooglecontainers/kube-apiserver:v1.14.3 docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.3 docker pull mirrorgooglecontainers/kube-scheduler:v1.14.3 docker pull mirrorgooglecontainers/kube-proxy:v1.14.3 docker pull mirrorgooglecontainers/pause:3.1 docker pull mirrorgooglecontainers/etcd:3.3.10 docker pull coredns/coredns:1.3.1 # 这个在mirrorgooglecontainers中没有 # 修改镜像tag docker tag mirrorgooglecontainers/kube-apiserver:v1.14.3 k8s.gcr.io/kube-apiserver:v1.14.3 docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.3 k8s.gcr.io/kube-controller-manager:v1.14.3 docker tag mirrorgooglecontainers/kube-scheduler:v1.14.3 k8s.gcr.io/kube-scheduler:v1.14.3 docker tag mirrorgooglecontainers/kube-proxy:v1.14.3 k8s.gcr.io/kube-proxy:v1.14.3 docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10 docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1 # 把所需的镜像下载好，init的时候就不会再拉镜像，由于无法连接google镜像库导致出错 # docker images # 查看当前镜像 # 删除原来的镜像 docker rmi mirrorgooglecontainers/kube-apiserver:v1.14.3 docker rmi mirrorgooglecontainers/kube-controller-manager:v1.14.3 docker rmi mirrorgooglecontainers/kube-scheduler:v1.14.3 docker rmi mirrorgooglecontainers/kube-proxy:v1.14.3 docker rmi mirrorgooglecontainers/pause:3.1 docker rmi mirrorgooglecontainers/etcd:3.3.10 docker rmi coredns/coredns:1.3.1 # --- 不能翻墙可以尝试使用 --- Node端 # 根据所需镜像名字先拉取国内资源 docker pull mirrorgooglecontainers/kube-proxy:v1.14.3 docker pull mirrorgooglecontainers/pause:3.1 # 修改镜像tag docker tag mirrorgooglecontainers/kube-proxy:v1.14.3 k8s.gcr.io/kube-proxy:v1.14.3 docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 # 删除原来的镜像 docker rmi mirrorgooglecontainers/kube-proxy:v1.14.3 docker rmi mirrorgooglecontainers/pause:3.1 # 不加载镜像node节点不能 使用kubeadm创建集群 Docker集群规划 Master：192.168.129.137 bd129137 Node1：192.168.129.114 bd129114 Node2：192.168.129.132 bd129132 创建集群 在Master主节点（bd129137）上执行: kubeadm init --pod-network-cidr=192.168.0.0/16 --kubernetes-version=v1.14.3 --apiserver-advertise-address=192.168.129.137 含义： 选项--pod-network-cidr=192.168.0.0/16表示集群将使用Calico网络，这里需要提前指定Calico的子网范围 选项--kubernetes-version=v1.10.0指定K8S版本，这里必须与之前导入到Docker镜像版本v1.10.0一致，否则会访问谷歌去重新下载K8S最新版的Docker镜像 选项--apiserver-advertise-address表示绑定的网卡IP，这里一定要绑定前面提到的enp0s8网卡，否则会默认使用enp0s3网卡 若执行kubeadm init出错或强制终止，则再需要执行该命令时，需要先执行kubeadm reset重置 问题： [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1 解决： [root@k8snode1 k8s_images]# echo \"1\" >/proc/sys/net/bridge/bridge-nf-call-iptables 执行结果： [root@bd129137 ~]# kubeadm init --pod-network-cidr=192.168.0.0/16 --kubernetes-version=v1.14.3 --apiserver-advertise-address=192.168.129.137 [init] Using Kubernetes version: v1.14.3 [preflight] Running pre-flight checks [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service' [WARNING Hostname]: hostname \"bd129137\" could not be reached [WARNING Hostname]: hostname \"bd129137\": lookup bd129137 on 202.102.192.68:53: no such host [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [bd129137 localhost] and IPs [192.168.129.137 127.0.0.1 ::1] [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [bd129137 localhost] and IPs [192.168.129.137 127.0.0.1 ::1] [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [bd129137 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.129.137] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 17.505580 seconds [upload-config] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.14\" in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --experimental-upload-certs [mark-control-plane] Marking the node bd129137 as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node bd129137 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: hgrnmd.f6k1berx8s8r9l2k [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.129.137:6443 --token hgrnmd.f6k1berx8s8r9l2k \\ --discovery-token-ca-cert-hash sha256:c029eab4924e93b5daae1339f06392e7ee003778a7d5d2b99e0d59b99cf6e9b9 可以看到，提示集群成功初始化，并且我们需要执行以下命令： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 另外， 提示我们还需要创建网络，并且让其他节点执行kubeadm join...加入集群。 kubeadm join 192.168.129.137:6443 --token hgrnmd.f6k1berx8s8r9l2k \\ --discovery-token-ca-cert-hash sha256:c029eab4924e93b5daae1339f06392e7ee003778a7d5d2b99e0d59b99cf6e9b9 应用flannel网络 # kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Node 加入集群 # Node1: kubeadm join 192.168.129.137:6443 --token hgrnmd.f6k1berx8s8r9l2k \\ --discovery-token-ca-cert-hash sha256:c029eab4924e93b5daae1339f06392e7ee003778a7d5d2b99e0d59b99cf6e9b9 # Node2: kubeadm join 192.168.129.137:6443 --token hgrnmd.f6k1berx8s8r9l2k \\ --discovery-token-ca-cert-hash sha256:c029eab4924e93b5daae1339f06392e7ee003778a7d5d2b99e0d59b99cf6e9b9 执行结果： [root@bd129114 ~]# kubeadm join 192.168.129.137:6443 --token hgrnmd.f6k1berx8s8r9l2k --discovery-token-ca-cert-hash sha256:c029eab4924e93b5daae1339f06392e7ee003778a7d5d2b99e0d59b99cf6e9b9 [preflight] Running pre-flight checks [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service' [WARNING Hostname]: hostname \"bd129114\" could not be reached [WARNING Hostname]: hostname \"bd129114\": lookup bd129114 on 202.102.192.68:53: no such host [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service' [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.14\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 遇到的坑：couldn't validate the identity of the API Server [root@bd129114 ~]# kubeadm join 192.168.129.137:6443 --token hgrnmd.f6k1berx8s8r9l2k --discovery-token-ca-cert-hash sha256:c029eab4924e93b5daae1339f06392e7ee003778a7d5d2b99e0d59b99cf6e9b9 [preflight] Running pre-flight checks [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service' [WARNING Hostname]: hostname \"bd129114\" could not be reached [WARNING Hostname]: hostname \"bd129114\": lookup bd129114 on 202.102.192.68:53: no such host [WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service' error execution phase preflight: couldn't validate the identity of the API Server: abort connecting to API servers after timeout of 5m0s 解决： # yum -y install device-mapper-persistent-data lvm2 排错命令 journalctl -f # 当前输出日志 journalctl -f -u kubelet # 只看当前的kubelet进程日志 验证集群是否正常 当所有节点加入集群后，稍等片刻，在主节点上运行 kubectl get nodes 可以看到： [root@bd129137 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION bd129114 NotReady 2m20s v1.14.3 bd129132 NotReady 44s v1.14.3 bd129137 Ready master 33m v1.14.3 # 若提示notReady则表示节点尚未准备好，可能正在进行其他初始化操作，等待全部变为Ready即可。 [root@bd129137 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION bd129114 Ready 4m36s v1.14.3 bd129132 Ready 3m v1.14.3 bd129137 Ready master 35m v1.14.3 另外，建议查看所有pod状态，运行 kubectl get pods -n kube-system： [root@bd129137 ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-fb8b8dccf-rx4wv 1/1 Running 0 34m kube-system coredns-fb8b8dccf-xxzcn 1/1 Running 0 34m kube-system etcd-bd129137 1/1 Running 0 34m kube-system kube-apiserver-bd129137 1/1 Running 0 34m kube-system kube-controller-manager-bd129137 1/1 Running 0 33m kube-system kube-flannel-ds-amd64-9d79f 1/1 Running 0 4m1s kube-system kube-flannel-ds-amd64-l4x2m 1/1 Running 0 22m kube-system kube-flannel-ds-amd64-xgd6m 1/1 Running 0 2m25s kube-system kube-proxy-5q6vv 1/1 Running 0 2m25s kube-system kube-proxy-l6lkm 1/1 Running 0 34m kube-system kube-proxy-pthkp 1/1 Running 0 4m1s kube-system kube-scheduler-bd129137 1/1 Running 0 34m 如上，全部 Running 则表示集群正常。至此，我们的K8S集群就搭建成功了。 以上Docker+K8S集群搭建，总共花了3个半小时。 参考资料 Docker官方安装文档 CentOS Docker 安装 kubernetes官方安装文档 2019最新k8s集群搭建教程 (centos k8s 搭建) 从零开始搭建Kubernetes集群（一、开篇） Docker命令详解（run篇） Docker run 命令 ChangeLog 20190702 | 发布到知识库 20190621 | 发布到语雀文斌技术Wiki 20190621 | 增加 docker run 命令详解 20190620 | 安装Docker+K8S集群成功，创建文档。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-02 09:49:57 "},"20190621DockerAndK8sStart.html":{"url":"20190621DockerAndK8sStart.html","title":"Docker 与 Kubernetes 快速入门","keywords":"","body":"Docker 与 Kubernetes 快速入门 本文参考了 Docker 和 Kubernetes 从听过到略懂：给程序员的旋风教程，结合自己的实际环境，记录下实践情况。 搭建 Docker 与 Kubernetes 环境以后，就可以开始尝试使用了，本文旨在对 Docker 与 Kubernetes 做一个最基础的使用指导。 Docker 使用示例 作为示例，我们在 Docker 的一个容器里运行简单的 HTTP 服务。 为了尽量简化例子，我们要部署的服务是用 Nginx 来 serve 一个简单的 HTML 文件 html/index.html。 Docker版本 [root@bd129137 ~]# docker version Client: Version: 18.09.6 API version: 1.39 Go version: go1.10.8 Git commit: 481bc77156 Built: Sat May 4 02:34:58 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.6 API version: 1.39 (minimum version 1.12) Go version: go1.10.8 Git commit: 481bc77 Built: Sat May 4 02:02:43 2019 OS/Arch: linux/amd64 Experimental: false 创建 Dockerfile 文件 我们采用 Dockerfile 方式来创建镜像。 先创建临时目录 docker-demo，并创建 html 文件。 $ mkdir docker-demo $ cd docker-demo $ mkdir html $ echo 'Hello Docker!' > html/index.html 接下来在当前目录创建一个叫 Dockerfile 的新文件，包含下面的内容： # cat Dockerfile FROM nginx COPY html/* /usr/share/nginx/html EOF Dockerfile说明： 每个 Dockerfile 都以 FROM ... 开头。 FROM nginx 的意思是以 Nginx 官方提供的镜像为基础来构建我们的镜像。（我们使用 docker search nginx 就能找到 Nginx 官方提供的镜像） 在构建时，Docker 会从 Docker Hub 查找和下载需要的镜像。Docker Hub 对于 Docker 镜像的作用就像 GitHub 对于代码的作用一样，它是一个托管和共享镜像的服务。使用过和构建的镜像都会被缓存在本地。 第二行是把我们的静态文件复制到镜像的 /usr/share/nginx/html 目录下。也就是 Nginx 寻找静态文件的目录。 Dockerfile 包含构建镜像的指令，更详细的信息可以参考这里。 构建 Docker 镜像 在 docker-demo 目录中执行以下命令： [root@bd129137 docker-demo]# docker build -t docker-demo:0.1 . Sending build context to Docker daemon 3.584kB Step 1/2 : FROM nginx latest: Pulling from library/nginx fc7181108d40: Pull complete c4277fc40ec2: Pull complete 780053e98559: Pull complete Digest: sha256:bdbf36b7f1f77ffe7bd2a32e59235dff6ecf131e3b6b5b96061c652f30685f3a Status: Downloaded newer image for nginx:latest ---> 719cd2e3ed04 Step 2/2 : COPY html/* /usr/share/nginx/html ---> d939ace6c09c Successfully built d939ace6c09c Successfully tagged docker-demo:0.1 命令说明： 命令中的名称 docker-demo 可以理解为这个镜像对应的应用名或服务名 0.1 是标签。Docker 通过名称和标签的组合来标识镜像。 可以用下面的命令来看到刚刚创建的镜像： [root@bd129137 docker-demo]# docker images docker-demo REPOSITORY TAG IMAGE ID CREATED SIZE docker-demo 0.1 d939ace6c09c About an hour ago 109MB 运行 docker-demo 镜像 下面我们把这个镜像运行起来。Nginx 默认监听在 80 端口，所以我们把宿主机的 8080 端口映射到容器的 80 端口： [root@bd129137 docker-demo]# docker run --name docker-demo -d -p 8080:80 docker-demo:0.1 ab88897d93c543a5f3de8c678bfd7377a609467eb283c90a870092af435e07ea 命令说明： docker run 命令中 -d 参数表示在后台运行。 -p 8080:80 指定容器暴露的端口，前者指的是宿主服务器端口，后者是容器内部的端口。 --name docker-demo 表示镜像运行容器名为 docker-demo。 用下面的命令可以看到正在运行中的容器： [root@bd129137 docker-demo]# docker container ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ab88897d93c5 docker-demo:0.1 \"nginx -g 'daemon of…\" 13 minutes ago Up 13 minutes 0.0.0.0:8080->80/tcp docker-demo 检查是否执行成功 这时如果你用浏览器访问 http://192.168.129.137:8080，就能看到我们刚才创建的「Hello Docker!」页面。 在现实的生产环境中 Docker 本身是一个相对底层的容器引擎，在有很多服务器的集群中，不太可能以上面的方式来管理任务和资源。所以我们需要 Kubernetes 这样的系统来进行任务的编排和调度。在进入下一步前，别忘了把实验用的容器清理掉。 停止并删除容器 $ docker container stop docker-demo $ docker container rm docker-demo Docker 通过仓库拉取镜像并执行 Kubernetes 运行容器 一个 Kubernetes 集群包含一个 Master 节点和很多 Node节点。Master 节点是控制集群的中心，Node 节点是提供 CPU、内存和存储资源的节点。 Master 上运行着多个进程，包括面向用户的 API 服务、负责维护集群状态的 Controller Manager、负责调度任务的 Scheduler 等。 每个 Node 上运行着维护 Node 状态并和 Master 通信的 kubelet，以及实现集群网络服务的 kube-proxy。 查看集群节点 [root@bd129137 docker-demo]# kubectl get nodes NAME STATUS ROLES AGE VERSION bd129114 Ready 19h v1.14.3 bd129132 Ready 19h v1.14.3 bd129137 Ready master 20h v1.14.3 部署一个单实例服务 重新构建一个镜像 我们先尝试部署一个简单的服务。Kubernetes 中部署的最小单位是 pod，而不是 Docker 容器。事实上 Kubernetes 是不依赖于 Docker 的，完全可以使用其他的容器引擎在 Kubernetes 管理的集群中替代 Docker。在与 Docker 结合使用时，一个 pod 中可以包含一个或多个 Docker 容器。但除了有紧密耦合的情况下，通常一个 pod 中只有一个容器，这样方便不同的服务各自独立地扩展。 在继续之前，要重新构建一遍我们的镜像，还在之前的目录，顺便改一下名字，叫它 k8s-demo:0.1： $ sudo docker build -t k8s-demo:0.1 . 定义 Pod 然后创建一个叫 pod.yml 的定义文件： $ sudo docker build -t k8s-demo:0.1 . [root@bd129137 docker-demo]# cat > pod.yml 这里定义了一个叫 k8s-demo 的 Pod，使用我们刚才构建的 k8s-demo:0.1 镜像。这个文件也告诉 Kubernetes 容器内的进程会监听 80 端口。然后把它跑起来： 让 Pod 跑起来 [root@bd129137 docker-demo]# kubectl create -f pod.yml pod/k8s-demo created kubectl 把这个文件提交给 Kubernetes API 服务，然后 Kubernetes Master 会按照要求把 Pod 分配到 node 上。用下面的命令可以看到这个新建的 Pod，一开始的 STATUS 是 ImagePullBackOff，需要再等待一下，STATUS 变成 Running，就表示成功了。 [root@bd129137 docker-demo]# kubectl get pods NAME READY STATUS RESTARTS AGE k8s-demo 0/1 ImagePullBackOff 0 25s [root@bd129137 .ssh]# kubectl get pods NAME READY STATUS RESTARTS AGE k8s-demo 1/1 Running 0 89m 排坑记一：ErrImagePull错误 在查看Pod状态时发现以下错误： [root@bd129137 docker-demo]# kubectl get pods NAME READY STATUS RESTARTS AGE k8s-demo 0/1 ErrImagePull 0 3m43s 错误详细信息： [root@bd129137 docker-demo]# kubectl describe pod k8s-demo Name: k8s-demo Namespace: default Priority: 0 PriorityClassName: Node: bd129114/192.168.129.114 Start Time: Fri, 21 Jun 2019 16:28:41 +0800 Labels: Annotations: Status: Pending IP: 192.168.1.2 Containers: k8s-demo: Container ID: Image: k8s-demo:0.1 Image ID: Port: 80/TCP Host Port: 0/TCP State: Waiting Reason: ImagePullBackOff Ready: False Restart Count: 0 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-lz7pb (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: default-token-lz7pb: Type: Secret (a volume populated by a Secret) SecretName: default-token-lz7pb Optional: false QoS Class: BestEffort Node-Selectors: Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulling 30m (x4 over 32m) kubelet, bd129114 Pulling image \"k8s-demo:0.1\" Warning Failed 30m (x4 over 32m) kubelet, bd129114 Failed to pull image \"k8s-demo:0.1\": rpc error: code = Unknown desc = Error response from daemon: pull access denied for k8s-demo, repository does not exist or may require 'docker login' Warning Failed 30m (x4 over 32m) kubelet, bd129114 Error: ErrImagePull Warning Failed 30m (x6 over 32m) kubelet, bd129114 Error: ImagePullBackOff Normal BackOff 27m (x20 over 32m) kubelet, bd129114 Back-off pulling image \"k8s-demo:0.1\" Normal Scheduled 7m53s default-scheduler Successfully assigned default/k8s-demo to bd129114 问题分析： 我们可以看到 repository does not exist or may require 'docker login' 错误，是说仓库不存在或者需要登录docker，因为 k8s-demo 是一个私有镜像，另一台服务器无法下载。需要把镜像上传到私有仓库，并配置docker集群使用私有仓库。 问题解决： 解决方案可以有两种：搭建私有仓库；使用脚本，将私有镜像分发到 Node 服务器。本次采用自动分发与加载的脚本。后续再考虑搭建私有仓库。 [root@bd129137 ~]# mkdir install [root@bd129137 ~]# docker save k8s-demo:0.1 | gzip > ./install/k8s-demo-0.1.tar.gz [root@bd129137 ~]# cat > k8s-demo-scp.sh 查看执行情况 执行以下命令查看运行在哪个节点上： [root@bd129137 docker-demo]# kubectl describe pod | grep Node Node: bd129114/192.168.129.114 Node-Selectors: 说明 k8s-demo 运行在 192.168.129.114 上，可以在 192.168.129.114 查看运行情况： [root@bd129114 ~]# docker ps | grep k8s-demo 338b34c49d07 d939ace6c09c \"nginx -g 'daemon of…\" 23 minutes ago Up 23 minutes k8s_k8s-demo_k8s-demo_default_042e3786-9402-11e9-9002-000c29479fb9_0 40c08061a4a8 k8s.gcr.io/pause:3.1 \"/pause\" 2 hours ago Up 2 hours k8s_POD_k8s-demo_default_042e3786-9402-11e9-9002-000c29479fb9_0 创建服务 虽然这个 pod 在运行，但是我们是无法像之前测试 Docker 时一样用浏览器访问它运行的服务的。可以理解为 pod 都运行在一个内网，我们无法从外部直接访问。要把服务暴露出来，我们需要创建一个 Service。Service 的作用有点像建立了一个反向代理和负载均衡器，负责把请求分发给后面的 pod。 创建一个 Service 的定义文件 svc.yml： [root@bd129137 docker-demo]# cat > svc.yml 这个 service 会把容器的 80 端口从 node 的 30050 端口暴露出来。注意文件最后两行的 selector 部分，这里决定了请求会被发送给集群里的哪些 pod。这里的定义是所有包含「app: k8s-demo」这个标签的 pod。然而我们之前部署的 pod 并没有设置标签： $ kubectl describe pods | grep Labels Labels: 所以要先更新一下 pod.yml，把标签加上（注意在 metadata: 下增加了 labels 部分）： apiVersion: v1 kind: Pod metadata: name: k8s-demo labels: app: k8s-demo spec: containers: - name: k8s-demo image: k8s-demo:0.1 ports: - containerPort: 80 然后更新 pod 并确认成功新增了标签： [root@bd129137 docker-demo]# kubectl apply -f pod.yml pod/k8s-demo configured [root@bd129137 docker-demo]# kubectl describe pods | grep Labels Labels: app=k8s-demo 然后就可以创建这个 service 了： [root@bd129137 docker-demo]# kubectl create -f svc.yml service/k8s-demo-svc created 访问服务 查看这个服务在K8S集群中的IP端口与外部端口信息。 [root@bd129137 docker-demo]# kubectl get svc k8s-demo-svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR k8s-demo-svc NodePort 10.98.65.150 80:30050/TCP 2m29s app=k8s-demo [root@bd129137 docker-demo]# kubectl describe pod | grep Node Node: bd129114/192.168.129.114 Node-Selectors: K8S集群中的应用可以通过 10.98.65.150:80 来访问 k8s-demo 中的 Nginx 服务。外部可以通过 192.168.129.114:30050 来访问 Nginx 服务。 参考资料 Docker 和 Kubernetes 从听过到略懂：给程序员的旋风教程 ChangeLog 20190702 | 发布到知识库 20190621 | 发布到语雀文斌技术Wiki 20190621 | 花费 3 小时，完成 Docker 简单 Nginx 服务运行与 K8S 集群单实例服务的运行。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-02 09:56:48 "},"20190624NginxPHPInDocker.html":{"url":"20190624NginxPHPInDocker.html","title":"Docker 部署 Nginx + PHP 环境","keywords":"","body":"Docker 部署 Nginx + PHP 环境 因为搭建 Dokuwiki 知识库需要 Nginx + PHP 环境，本文就介绍如何在 Docker 中搭建 Nginx + PHP 环境，Docker环境的安装配置参见 Docker + K8S 集群环境安装指南 。 拉取 Nginx + PHP 官方镜像 docker pull nginx docker pull bitnami/php-fpm 使用php-fpm镜像开启php-fpm应用容器 docker run -d --name tydic-php -v /var/www/html:/usr/share/nginx/html bitnami/php-fpm 说明： -d 后台运行容器的意思 -v 指定宿主机与容器的映射关系。 /var/www/html为宿主机的项目目录（自定义的），如果项目目录中没有文件，可以新建index.php文件。 [root@bd129137 html]# cat > index.php EOF /usr/share/nginx/html为nginx服务器项目默认的路径。 开启nginx容器 docker run -d --name tydic-nginx -p 8080:80 -v /var/www/html:/usr/share/nginx/html nginx 说明： -p : 该参数设置端口对应的关系。所有访问宿主机8080端口的URL会转发到nginx容器的80端口。 查看php-fpm的ip地址，配置nginx用 [root@bd129137 html]# docker inspect tydic-php | grep IPAddress \"SecondaryIPAddresses\": null, \"IPAddress\": \"172.17.0.2\", \"IPAddress\": \"172.17.0.2\", 修改nginx配置 进入容器 docker exec -it tydic-nginx /bin/bash 说明： -i : --interactive，交互模式。 -t : --tty，开启一个伪终端。 /bin/bash : 必须写，否则会报错。这是开始伪终端时，进入bash界面，也就是命令行界面。 查看对应的配置文件位置 /etc/nginx/conf.d/default.conf 复制配置文件到usr路径下 进入usr目录下 /usr/share/nginx/html 复制nginx容器中的默认配置 docker cp tydic-nginx:etc/nginx/conf.d/default.conf ./default.conf 这样就在当前路径下生成了一个默认配置，vim default.conf 编辑 location ~ \\.php$ { fastcgi_pass 172.17.0.2:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /usr/share/nginx/html$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; include fastcgi_params; } 完整配置如下： server { listen 80; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location ~ \\.php$ { fastcgi_pass 172.17.0.2:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /usr/share/nginx/html$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; include fastcgi_params; } } 然后同步配置到容器中 docker cp ./default.conf tydic-nginx:/etc/nginx/conf.d/default.conf 进入nginx容器重新加载配置 docker exec -it tydic-nginx /bin/bash service nginx reload 访问192.168.129.137:8080/index.php 使用 Nginx + PHP 代理 Dokuwiki Nginx + PHP 环境搭建完成，现在开始使用它们来代理 Dokuwiki。首先需要保证 Dokuwiki的文件夹放在宿主目录中，防止 Docker 镜像重启导致数据丢失。 目录规划 # 宿主主机 /usr/docker/nginx/conf/conf.d nginx配置目录，映射到docker中。 /usr/docker/nginx/html/dokuwiki dokuwiki工程 dokuwiki的Nginx配置 在 /usr/docker/nginx/conf/conf.d 中新建 dokuwiki.conf文件： server { listen 80; root /usr/share/nginx/html/dokuwiki; server_name localhost; index index.php index.html doku.php; location ~ ^/(data|conf|bin|inc) { return 404; } location ~ ^/lib.*\\.(gif|png|ico|jpg)$ { expires 31d; } #location / { # try_files $uri $uri/ @dokuwiki; #} location @dokuwiki { rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&id=$2 last; rewrite ^/tag/(.*) /doku.php?id=tag:$1&do=showtag&tag=tag:$1 last; rewrite ^/(.*) /doku.php?id=$1&$args last; } location ~ .+\\.php($|/) { include fastcgi_params; set $real_script_name $uri; set $path_info \"/\"; if ($fastcgi_script_name ~ \"^(.+\\.php)(/.+)$\") { set $real_script_name $1; set $path_info $2; } fastcgi_param SCRIPT_FILENAME $document_root$real_script_name; fastcgi_param SCRIPT_NAME $real_script_name; fastcgi_param PATH_INFO $path_info; fastcgi_pass 172.17.0.2:9000;#监听9000端口 fastcgi_index index.php; } } 启动 Nginx + PHP 命令 docker run -d --name tydic-php -v /usr/docker/nginx/html:/usr/share/nginx/html bitnami/php-fpm docker run -d --name tydic-nginx -p 6200:80 -v /usr/docker/nginx/html:/usr/share/nginx/html -v /usr/docker/nginx/conf/conf.d:/etc/nginx/conf.d nginx 说明： 将 /usr/docker/nginx/html 目录映射到 php 和 nginx 容器中，对应目录 /usr/share/nginx/html 将 /usr/docker/nginx/conf/conf.d 目录映射到 nginx 容器的 /etc/nginx/conf.d，作为 nginx 服务的配置加载路径。 Dokuwiki 访问 访问 http://192.168.129.137:6200 即可进入 Dokuwiki 界面。 参考资料 docker部署php+nginx环境 ChangeLog 20190624 | 完善文字说明。 20190624 | 发布到语雀文斌技术Wiki 20190624 | 新建文章，搞定nginx+php+dokuwiki环境搭建。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-24 21:50:04 "},"20190426InstallationOfPython.html":{"url":"20190426InstallationOfPython.html","title":"MacOS中Python环境搭建","keywords":"","body":"2019-04-26 | Mac OS搭建Python开发环境 安装Python Brew方式安装 $ brew install python3 Warning: python 3.7.3 is already installed, it's just not linked You can use `brew link python` to link this version. 问题：发现/usr/local/下没有路径/usr/local/Frameworks。需要新建该路径，并修改权限 $ brew install python3 Error: An unexpected error occurred during the `brew link` step The formula built, but is not symlinked into /usr/local Permission denied @ dir_s_mkdir - /usr/local/Frameworks Error: Permission denied @ dir_s_mkdir - /usr/local/Frameworks 解决： $ sudo mkdir /usr/local/Frameworks $ sudo chown $(whoami):admin /usr/local/Frameworks brew link python3 ``` $ brew link python3 Linking /usr/local/Cellar/python/3.7.3... 20 symlinks created ``` which python3 ``` $ which python3 /usr/local/bin/python3 ``` 一个非常重要的问题 来源：https://www.jianshu.com/p/892a14bdda4d Mac OS自身其实已经带有Python，版本为2.7.X，这个Python主要用于支持系统文件和XCode，所以我们在安装新的Python版本时候最好不要影响这部分。 这里就会出现一个十分困扰的问题，我们按照上述步骤安装好了自己所需要Python版本，目前我们一般都会选择安装Python 3.X版本，在安装好了之后，我们输入以下命令 python --version或者python 发现所示内容仍然是2.7.X版本的Python，这是因为我们使用python这个命令，系统仍然会调用默认的Python版本(即系统版本)，网上很多教程会让我们修改配置文件或者$PATH变量将系统默认Python版本切换至我们安装的版本，但是个人感觉没有多大必要，毕竟系统的东西能不改最好不要改。 所以我们选择一个比较简单的办法，就是当我们需要使用自己安装的Python版本时(即之前安装的3.X版本)，直接使用python3作为命令即可。相同的命令为： 终端输入以下命令，查看Python安装位置which python3，终端输入以下命令，查看Python当前版本python3 --version，终端输入以下命令，进入Python交互模式python3 安装pip3 参考：https://oldtang.com/351.html https://pip.readthedocs.io/en/stable/installing/ 如果通过 homebrew 安装 python3，那么 pip3 会同时安装。所以建议直接通过 homebrew 安装 python3： brew install python3 如果你已经通过其他渠道安装了 python3 但是尚未安装 pip3，那么需要通过以下步骤实现安装： curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py cd cd /usr/local/bin/ ln -s ../Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/bin/pip3 pip3 安装完成后，可以查看一下 pip3 的版本： pip3 -V 使用 pip3 安装第三方库 这个就很简单了，直接使用 pip3 install 就行了，比如： pip3 install flask 自己想装什么就装什么即可。 Mac中Python开发IDE环境搭建 参考：https://blog.csdn.net/a464057216/article/details/55652179 https://www.readern.com/configure-vscode-for-python3-on-mac.html 安装Vscode 略 下载Python插件 配置PythonPath 新建Python文件test.py msg = \"Hello World\" print(msg) 执行python脚本 学习资料 《机器学习从认知到实践》源码：https://github.com/PacktPublishing/Python-Machine-Learning-Blueprints 参考资料 Mac OS搭建Python开发环境 brew link python3出错 Permissions issue when linking python3 ChangeLog 2019-06-10 | 修改文档样式，转移到GitBook。 20190426 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-10 16:03:34 "},"20190613InstallationOfNginx.html":{"url":"20190613InstallationOfNginx.html","title":"CentOS 6 安装 Nginx 指南","keywords":"","body":"CentOS 6 安装Nginx Nginx下载 wget http://nginx.org/download/nginx-1.16.0.tar.gz 安装依赖包 # yum -y install gcc gcc-c++ automake autoconf libtool make # yum -y install openssl openssl-devel # yum -y install pcre pcre-devel # yum -y install zlib zlib-devel Nginx安装 将nginx解压并移动到/usr/local目录下 # tar zxvf nginx-1.16.0.tar.gz # mv nginx-1.16.0 /usr/local/ 进入nginx目录 # cd /usr/local/nginx-1.16.0/ 依赖包 # yum -y install gcc automake autoconf libtool make configure [root@node1 nginx-1.16.0]# ./configure ... Configuration summary + using system PCRE library + OpenSSL library is not used + using system zlib library nginx path prefix: \"/usr/local/nginx\" nginx binary file: \"/usr/local/nginx/sbin/nginx\" nginx modules path: \"/usr/local/nginx/modules\" nginx configuration prefix: \"/usr/local/nginx/conf\" nginx configuration file: \"/usr/local/nginx/conf/nginx.conf\" nginx pid file: \"/usr/local/nginx/logs/nginx.pid\" nginx error log file: \"/usr/local/nginx/logs/error.log\" nginx http access log file: \"/usr/local/nginx/logs/access.log\" nginx http client request body temporary files: \"client_body_temp\" nginx http proxy temporary files: \"proxy_temp\" nginx http fastcgi temporary files: \"fastcgi_temp\" nginx http uwsgi temporary files: \"uwsgi_temp\" nginx http scgi temporary files: \"scgi_temp\" make # make # make install 启动 # /usr/local/nginx/sbin/nginx # /usr/local/nginx/sbin/nginx -s reload 安装成功，在浏览器访问http://192.168.129.137，展示页面： 参考资料 CentOS安装Nginx ChangeLog 20190613 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-18 18:54:27 "},"20180603IntroductionOfMarkdown.html":{"url":"20180603IntroductionOfMarkdown.html","title":"初步认识Markdown","keywords":"","body":"初步认识markdown 为什么是Markdown 文本编辑比较 无格式文本：简洁、不依赖工具 富文本：美观，重点突出 Markdown与富文本编辑器异同： 作用一致： 使用者输入纯文字，通过编辑器的处理，使其拥有一份样式，最终得到带格式的文档。 使用区别： 富文本编辑器「所见即所得」 Markdown是一种标记语言，手动切换预览与编辑模式。 Markdown同时兼具无格式文本与富文本的优势。 什么是Markdown Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。 —— 维基百科 Markdown 是一种轻量级的「标记语言」，创始人为约翰·格鲁伯。Markdown 的创始人 John Gruber 这样定义： \"Markdown\" is two things: (1) a plain text formatting syntax; (2) a software tool, that converts the plain text formatting to others. 也就是说Markdown首先意味着是一套语法规则，其次代表了编辑器，把纯文本转换为排版效果的文字。 Markdown语法演进 CommonMark GFM(GitHub Flavored Markdown) 其他语法：PHP Markdown Extra、Maruku、kramdown、RDiscount、Redcarpet、MultiMarkdown 原有的 Markdown 语法的功能稍显不足，Github Flavored Markdown 在前面所说的语法的三个方面都做出了相应的增强。 比如： 标准Markdown要在一行的最后加两个空格符才表示换行，否则是不换行的；但是GFM则没有此要求。 支持把列表变成带勾选框的任务列表 在对段落的处理方面，对原有代码块进行了增强，可以制定不同的语言类型对代码进行语法高亮。 GFM的修改参考：GFM修改，GFM语法参考：GFM语法 语法特点： 用简洁的语法代替排版，其常用的标记符号不超过十个， 相对于更为复杂的 HTML 标记语言来说，Markdown 十分的轻量 一旦熟悉这种语法规则，会有沉浸式编辑的效果。 其他增强型Markdown语法：MultiMarkdown Markdown优势 书写过程流畅 富文本编辑器编辑文字时是两个不连续的动作，输入文字时双手放在键盘上，编辑文字则需要视线和手离开输入框和键盘，去寻找和点击功能按钮。 Markdown 的「书写流畅」就体现在将这两个动作合成一个输入字符的动作。 格式不随编辑器而改变，导出与分享方便 Markdown 格式保持的文件本质上仍是一份纯文本。 书写错误容易发现。 比如Word中，用空格、分页来控制排版，容易出错。而Markdown没有不需要考虑几个空格的问题，如果有几个词语想加粗，没有渲染成功，就说明写错了。 Markdown 的局限性 什么时候不该用 Markdown？ Markdown 无法对「段落」进行灵活处理。比如：文本位置 Markdown 对非纯文本元素的排版能力很差。比如：图文混排 Markdown一开始就定位为「文字输入工具」，不适合对排版格式自定义程度较高的文档进行排版。 Markdown适用场景 网络环境下的写作 利用了 Markdown 「写作即排版」的特点，Markdown 可以让使用者专心于文章书写，而非排版。 文档协作 团队成员间可以自由选用自己喜欢的操作系统和编辑器工具来进行写作，而不局限于 Word 或者 Google Docs等只支持富文本编辑的软件。 文档的展示方式不仅仅是在编辑器中，你可以随时把文档转换成网页，任何时候任何人都可以方便地查看。 利用它「纯文本格式」的优势，用 Markdown 来文档协作会比其他工具更自由。 基础语法 标题 markdown代码： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 ### 文字格式 **This is bold text** *This text is italicized* ~~This was mistaken text~~ **This text is _extremely_ important** 区块引用 尼采说： Was mich nicht umbringt, macht mich stärker. markdown代码： #### 尼采说： > Was mich nicht umbringt, macht mich **stärker**. 列表 无序列表 George Washington John Adams Thomas Jefferson George Washington John Adams Thomas Jefferson 有序列表 James Madison James Monroe John Quincy Adams 嵌套列表 First list item First nested list item Second nested list item markdown代码： #### 无序列表 - George Washington - John Adams - Thomas Jefferson * George Washington * John Adams * Thomas Jefferson #### 有序列表 1. James Madison 2. James Monroe 3. John Quincy Adams #### 嵌套列表 1. First list item - First nested list item - Second nested list item 代码 行内代码 Use git status to list all new or modified files that haven't yet been committed. 代码块 @requires_authorization def somefunc(param1='', param2=0): '''A docstring''' if param1 > param2: # interesting print 'Greater' return (param2 - param1 + 1) or None class SomeClass: pass >>> message = '''interpreter ... prompt''' markdown代码： #### 行内代码 Use `git status` to list all new or modified files that haven't yet been committed. #### 代码块 ​```python @requires_authorization def somefunc(param1='', param2=0): '''A docstring''' if param1 > param2: # interesting print 'Greater' return (param2 - param1 + 1) or None class SomeClass: pass >>> message = '''interpreter ... prompt''' ​`` ` 分割线 这是分割线 这也是分割线 markdown代码： 这是分割线 --- 这也是分割线 *** 链接 网络链接 This site was built using GitHub Pages. This site was built using GitHub Pages. 相对链接 Test.md 图片链接 markdown代码： #### 网络链接 This site was built using [GitHub Pages](https://pages.github.com/). This site was built using [GitHub Pages][1]. #### 相对链接 [Test.md](./test.md) #### 图片链接 ![markdown](http://pic.iloc.cn/2019-06-05-I-love-markdown-syntax-language.png) 脚注 脚注demo 参考1 markdown代码： 脚注[^demo] 参考[^1] 高级语法 注：可能会有渲染不支持的情况。 表格 Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 markdown代码： #### 表格 | Item | Value | Qty | | :------- | -------: | :--: | | Computer | 1600 USD | 5 | | Phone | 12 USD | 12 | | Pipe | 1 USD | 234 | 目录 [TOC] markdown代码： #### 目录 [TOC] 待办事项 使用 - [ ] 和 - [x] 语法可以创建复选框，实现 todo-list 等功能。例如： [x] 已完成事项 [ ] 待办事项1 [ ] 待办事项2 markdown代码： #### 待办事项 使用 `- [ ]` 和 `- [x]` 语法可以创建复选框，实现 todo-list 等功能。例如： - [x] 已完成事项 - [ ] 待办事项1 - [ ] 待办事项2 公式 $$E=mc^2$$ 可以创建行内公式，例如 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。或者块级公式： $$x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ markdown代码： #### 公式 $$E=mc^2$$ 可以创建行内公式，例如 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。或者块级公式： $$x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ 流程图 st=>start: Start op=>operation: Your Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op markdown代码： #### 流程图 ​```flow st=>start: Start op=>operation: Your Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op ​`` ` 序列图 Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks! markdown代码： #### 序列图 ​```sequence Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks! ​`` ` 工具推荐 MWeb（推荐，收费，Mac/IOS） Typora（推荐，免费。全平台） 特点： WYSIWYG（What You See Is What You Get） 表格编辑功能增强 插入图片 代码和数学公式输入 支持显示目录大纲 下载：https://www.typora.io/ 介绍：https://sspai.com/post/30292 参考资料 Mastering Markdown · GitHub Guides 印象笔记 Markdown 入门指南：https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php Markdown 完全入门（上）：https://sspai.com/post/36610 Markdown 完全入门（下）：https://sspai.com/post/36682 Markdown教程：http://www.markdown.cn/ 创始人 John Gruber 的 Markdown 语法说明：https://daringfireball.net/projects/markdown/syntax Github Flavored Markdown语法：https://help.github.com/articles/basic-writing-and-formatting-syntax/ 印象笔记 Markdown 入门指南：https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php Markdown简易入门教程：https://blog.huihut.com/2017/01/25/MarkdownTutorial/ Typora介绍：https://sspai.com/post/30292 ChangeLog 20190625 | 增加参考资料：Mastering Markdown · GitHub Guides 20190621 | 发布到语雀文斌技术Wiki 20190606 | 推荐工具增加MWeb，去掉小书匠、有道云笔记等。 20190606 | 从Evernote转移到GitBook 20180603 | 完成第一版，并在小组内分享 demo. 这是一个脚注。 ↩ 1. 这也是一个脚注。 ↩ Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-07-03 18:35:37 "},"20190214GiteeCommands.html":{"url":"20190214GiteeCommands.html","title":"Git代码转移到码云相关命令","keywords":"","body":"Git代码转移到码云相关命令 查看当前用户（global）配置 git config --global --list 生成公钥 ssh-keygen -t rsa -C \"youremail@example.com\" 查看本地代码的远程仓库地址 git remote -v 校验本机是否能连接gitee ssh -T git@gitee.com 重新设置远程仓库地址 git remote set-url origin git@gitee.com:xxx/xxxx.git OR git remote set-url origin https://gitee.com/xxxx/xxxx.git git remote -v 第一次pull出现问题的处理方法 git pull --allow-unrelated-histories 代码的换行符与windows不一致的情况，处理方法： git config --global core.autocrlf input ChangeLog 20190606 | 从Evernote迁移到GitBook 20190214 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:27 "},"20190603InstallationOfDoduwiki.html":{"url":"20190603InstallationOfDoduwiki.html","title":"DokuWiki安装小记","keywords":"","body":"DokuWiki安装小记 2019-06-03 以下为MacOS平台的DokuWiki安装，其他平台类似。 什么是DokuWiki DokuWiki 是一个使用，用途多样的开源 Wiki 软件，并且不需要数据库。 比较wiki 安装准备工作 安装nginx。 安装php。参考MacOS下安装PHP与nginx配置 安装DokuWiki 安装过程参考：DokuWiki with Mac OS X and Apache 下载DokuWiki。可以在官网直接下载，我下载的是2019-05-31 snapshot版本，放到目录/usr/local/Cellar_w/raw下。 WZB-MacBook:raw wangzhibin$ pwd /usr/local/Cellar_w/raw WZB-MacBook:Cellar_w wangzhibin$ wget https://download.dokuwiki.org/src/dokuwiki/dokuwiki-stable.tgz 解压DokuWiki安装包，并创建软连接。 WZB-MacBook:raw wangzhibin$ tar zxvf splitbrain-dokuwiki-upstream-0.0.20091252c-7022-g2e6e11a.tar.gz WZB-MacBook:raw wangzhibin$ cd .. WZB-MacBook:Cellar_w wangzhibin$ pwd /usr/local/Cellar_w WZB-MacBook:Cellar_w wangzhibin$ ln -s raw/splitbrain-dokuwiki-2e6e11a/ dokuwiki 配置nginx代理 打开nginx.conf，在http块添加一行include servers/*.conf;（默认已经存在）。MacOS使用brew安装nginx的配置文件目录在/usr/local/etc/nginx/。 在/usr/local/etc/nginx/servesr下创建dokuwiki.conf。 增加以下内容： server { listen 8090; root /usr/local/Cellar_w/dokuwiki; server_name localhost; index index.php index.html doku.php; location ~ ^/(data|conf|bin|inc) { return 404; } location ~ ^/lib.*\\.(gif|png|ico|jpg)$ { expires 31d; } #location / { # try_files $uri $uri/ @dokuwiki; #} location @dokuwiki { rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&id=$2 last; rewrite ^/tag/(.*) /doku.php?id=tag:$1&do=showtag&tag=tag:$1 last; rewrite ^/(.*) /doku.php?id=$1&$args last; } location ~ .+\\.php($|/) { include fastcgi_params; set $real_script_name $uri; set $path_info \"/\"; if ($fastcgi_script_name ~ \"^(.+\\.php)(/.+)$\") { set $real_script_name $1; set $path_info $2; } fastcgi_param SCRIPT_FILENAME $document_root$real_script_name; fastcgi_param SCRIPT_NAME $real_script_name; fastcgi_param PATH_INFO $path_info; fastcgi_pass 127.0.0.1:9000;#监听9000端口 fastcgi_index index.php; } } 重新启动nginx，下面两种命令都可以。 sudo brew services restart nginx /usr/local/bin/nginx -s reload 在浏览器中打开http://localhost:8090，即可访问dokuWiki首页了。 使用DokuWiki 初始化安装与ACL启用 中文文件名乱码问题 在界面中搜索“测试页面” 点击红色的“:测试页面”，即可创建中文词条“测试页面”。随便写个内容，保存。 在后台pages目录下该词条名称乱码。 下面就开始修改源码，使得中文词条名称正常。 在dokuwiki/conf/local.php文件最后一行加上。（如果conf目录仅发现local.php.dist文件，这是没有install的缘故。） $conf[ 'fnencode' ] = 'utf-8'; #注意分号不能少。 再次尝试创建中文词条 发现pages目录正常。 Doku主题 官方主题排行 官方主题按照受欢迎程度排名靠前的如下： 主题安装 以主题Breeze为例。 下载主题Breeze Template，解压到lib/tpl/目录下，命名为breeze。 在conf/local.php文件中增加： $conf['template']='breeze'; // 配置的是tpl下主题目录名。 推荐主题 推荐的几个主题。在官网主题区可以搜索到。 * bootstrap3 * vector (强烈推荐) * sprintdoc * breeze * white 修改vector主题样式 修改侧边栏的宽度 修改lib/tpl/vector/static/3rd/vector/main-ltr.css中的样式： div#panel { ... width: 13em; ... } ... /* Content */ div#content { margin-left: 13em; ... } ... /* Footer */ div#footer { margin-left: 13em; ... } ... /* Navigation Containers */ #left-navigation { ... left: 13em; ... } 更换logo 使用Vector主题时，logo位置为：background-image:url(/lib/tpl/vector/static/3rd/dokuwiki/logo.png); 替换即可。 DokuWiki插件 侧边栏插件indexmenu 安装indexmenu插件 在sidebar页面增加配置，左侧会自动出现全部页面导航。 indexmenu使用 仅显示:wiki里面的内容 默认展开到两层 仅展现wiki下的，并且去掉toc、右键菜单。 Vector主题下不能展示sidebar的修改方法，在lib/tpl/vector/conf/default.php中修改： //$conf[\"vector_navigation_location\"] = \":wiki:navigation\"; //page/article used to store the navigation $conf[\"vector_navigation_location\"] = \":sidebar\"; //page/article used to store the navigation 增加排序 新增页面插件Add New Page 在扩展管理器中搜索“Add New Page”，安装此插件。 在sidebar页面下方增加，表示仅在wiki命名空间下增加页面。 ------------------------ false 有了page新增管理，那么命名空间怎么新增呢？ 直接在浏览器输入http://localhost:8091/doku.php?id=wiki:test:新目录:新文件，即可创建。 如何删除文件？ 进入编辑页面，文章内容全部删除后，保存，该文章就被删除了。与此同时，如果命名空间下没有文章，命名空间也被删除了 移动插件move 在扩展管理器中搜索“move”，安装此插件。 使用方法，在管理界面，会出现“页面移动/重命名...”的工具，可以进入管理界面。 贡献插件authorstats 安装authorstats插件。 在文章中增加 保存后即可查看 评论区插件discussion 插件：plugin:discussion 用法：~~DISCUSSION~~，插入该语句到 wiki 中，即可在 wiki 内容后添加评论区。 配置：管理->配置管理->Discussion，比较有用的配置： 订阅评论区 通知版主有新评论 允许未注册用户评论 可通过 wiki 语法评论 其他可直接安装使用的插件 编辑器支持markdown语法插件：plugin:markdowku 编辑器支持直接粘贴图片：plugin:imgpaste 导出Word文件plugin:OpenOffice.org 其他 修改 Dokuwiki 默认时区 Dokuwiki默认的时区不是北京时间，导致发布的页面时间总是差了 8 个小时，这个需要修改 /inc/init.php 文件，按照这个方法修改后时间就都正常了。 # 找到 date_default_timezone_set(@date_default_timezone_get()); 修改为： date_default_timezone_set('PRC'); 刷新页面即可。 附录 local.php文件内容 sidebar.txt文件内容 {{indexmenu>:wiki#2|js#vista.png notoc nomenu tsort}} --------- {{NEWPAGE>wiki}} start内容底部包括贡献内容 . 修改贡献数据 参考资料 dokuwiki 搭建 官方插件 用 Dokuwiki 管理小团队知识 dokuwiki安装使用教程（支持中文、editor.md、粘贴上传图片） ChangeLog 20190603 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-27 14:25:39 "},"20190513UseOfiTerm.html":{"url":"20190513UseOfiTerm.html","title":"iTerm2快速SSH连接并保存密码","keywords":"","body":"2019-05-13 | iTerm2快速SSH连接并保存密码 2019-05-15 发布博客。 背景 Mac自带terminal，以及比较好用的iTerm2命令行工具，都缺乏一个功能，就是远程SSH连接，无法保存密码。 一种方法是将本机的ssh_key放到远程服务器中实现无密码登录。这种方法在很多情况下无法实现，因为远程服务器大多是客户的。 本文介绍一个简单、轻量级的命令行工具——sshpass，通过它我们能够向命令提示符本身提供密码（非交互式密码验证），这样就可以实现自动连接远程服务器，而且能自动执行远程命令。 安装sshpass 下载sshpass：https://sourceforge.net/projects/sshpass/files/ 进入 sshpass目录 运行【./configure】 运行【sudo make install】 运行【sshpass 】 来测试是否安装成功 sshpass使用 Usage: sshpass [-f|-d|-p|-e] [-hV] command parameters -f filename Take password to use from file -d number Use number as file descriptor for getting password -p password Provide password as argument (security unwise) -e Password is passed as env-var \"SSHPASS\" With no parameters - password will be taken from stdin -h Show help (this screen) -V Print version information At most one of -f, -d, -p or -e should be used 使用用户名和密码登录到远程Linux ssh服务器（10.42.0.1），并检查文件系统磁盘使用情况，如图所示。 $ sshpass -p 'my_pass_here' ssh aaronkilik@10.42.0.1 'df -h' 也可以使用sshpass 通过scp传输文件或者rsync备份/同步文件，如下所示： ------- Transfer Files Using SCP ------- $ scp -r /var/www/html/example.com --rsh=\"sshpass -p 'my_pass_here' ssh -l aaronkilik\" 10.42.0.1:/var/www/html ------- Backup or Sync Files Using Rsync ------- $ rsync --rsh=\"sshpass -p 'my_pass_here' ssh -l aaronkilik\" 10.42.0.1:/data/backup/ /backup/ iTerm2集成sshpass实现快速SSH连接 打开iTerm2的Profiles菜单，进入Profiles设置。 点击Edit Profiles。 增加SSH连接。 Name：名称 Tags：分组或者标签名称 Title：设置窗口名称 Command：/usr/local/bin/sshpass -p 'xxxx' ssh root@192.168.129.116 快速连接 参考资料 sshpass：一个很棒的免交互SSH登录工具，但不要用在生产服务器上 iTerm2 保存ssh用户名密码 ChangeLog 20190609 | 转移到GitBook 20190515 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 14:19:17 "},"20190603BrewOnMacOS.html":{"url":"20190603BrewOnMacOS.html","title":"MacOS的brew update很慢问题解决方案","keywords":"","body":"2019-06-03 | MacOS的brew update很慢问题解决方案 替换为中科大源 替换brew.git cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.git 替换homebrew-core.git cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git 替换Homebrew Bottles源 就是在~/.bashrc或者~/.bash_profile文件末尾加 export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles 这两个文件可以自己创建，~/.bashrc和~/.bash_profile都可以 切换回官方源： 重置brew.git cd \"$(brew --repo)\" git remote set-url origin https://github.com/Homebrew/brew.git 重置homebrew-core cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://github.com/Homebrew/homebrew-core.git 替换为清华源 brew清华源 参考资料 Homebrew国内源设置与常用命令 ChangeLog 20190603 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:11:27 "},"20190603InstallationOfPhp.html":{"url":"20190603InstallationOfPhp.html","title":"MacOS下安装PHP与nginx配置","keywords":"","body":"2019-06-03 | PHP安装与nginx配置 MacOS安装PHP MacOS系统自带PHP的，不过还是建议自己安装。 使用brew安装PHP # brew update # brew install php@7.2 ... ... To have launchd start php@7.2 now and restart at login: brew services start php@7.2 Or, if you don't want/need a background service you can just run: php-fpm 注：如何brew update很慢的话，可以参考MacOS的brew update很慢问题解决方案进行设置。 修改配置文件 执行命令： sudo cp /private/etc/php-fpm.conf.default /private/etc/php-fpm.conf sudo cp /private/etc/php-fpm.d/www.conf.default /private/etc/php-fpm.d/www.conf 找到/private/etc/目录下的 php-fpm 文件 /private/etc/php-fpm.conf 找到24行的 error_log ，改为（整行替换，注意 ‘;’ 和空格，就是要把‘;’也删除掉） error_log = /usr/local/var/log/php-fpm.log 否则 php-fpm 时会报错： ERROR: failed to open error_log (/usr/var/log/php-fpm.log): No such file or directory 启动PHP 如果希望后台自动启动，执行以下命令： brew services start php@7.2 如果只是本次执行，直接运行： php-fpm 配置Nginx运行php 打开nginx.conf ，在http块添加一行include servers/*.conf;。MacOS使用brew安装nginx的配置文件目录在/usr/local/etc/nginx/。 在/usr/local/etc/nginx/servesr下创建xxx.conf。我这里创建的是dokuwiki.conf 增加以下内容，尤其注意location ~ .+\\.php($|/)部分。 server { listen 8090; root /usr/local/Cellar_w/dokuwiki; server_name localhost; index index.php index.html doku.php; location ~ ^/(data|conf|bin|inc) { return 404; } location ~ ^/lib.*\\.(gif|png|ico|jpg)$ { expires 31d; } #location / { # try_files $uri $uri/ @dokuwiki; #} location @dokuwiki { rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&id=$2 last; rewrite ^/tag/(.*) /doku.php?id=tag:$1&do=showtag&tag=tag:$1 last; rewrite ^/(.*) /doku.php?id=$1&$args last; } location ~ .+\\.php($|/) { include fastcgi_params; set $real_script_name $uri; set $path_info \"/\"; if ($fastcgi_script_name ~ \"^(.+\\.php)(/.+)$\") { set $real_script_name $1; set $path_info $2; } fastcgi_param SCRIPT_FILENAME $document_root$real_script_name; fastcgi_param SCRIPT_NAME $real_script_name; fastcgi_param PATH_INFO $path_info; fastcgi_pass 127.0.0.1:9000;#监听9000端口 fastcgi_index index.php; } } 重新启动nginx，下面两种命令都可以。 sudo brew services restart nginx /usr/local/bin/nginx -s reload 遇到的坑 问题一：Nginx代理PHP，报错Connection reset by peer nginx.log中错误信息如下： [error] 85581#0: *4 kevent() reported about an closed connection (54: Connection reset by peer) while reading response header from upstream, client: 127.0.0.1, server: localhost, request: \"GET / HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"localhost:8088\" 出错的原因是php-fpm未启动。 解决：sudo php-fpm 问题二、使用sudo php-fpm会报路径出错问题 可能是你配置路径那里没有修改，具体可以看上面的 修改配置文件章节。 问题二：No pool defined [root@localhost etc]# service php-fpm start Starting php-fpm [28-Nov-2016 17:13:23] WARNING: Nothing matches the include pattern ‘/usr/local/php/etc/php-fpm.d/*.conf’ from /usr/local/php/etc/php-fpm.conf at line 125. [28-Nov-2016 17:13:23] ERROR: No pool defined. at least one pool section must be specified in config file [28-Nov-2016 17:13:23] ERROR: failed to post process the configuration [28-Nov-2016 17:13:23] ERROR: FPM initialization failed 解决方法: 进入PHP安装目录/etc/php-fpm.d cp www.conf.default www.conf CentOS 6 安装 PHP 检查当前安装的PHP包 # yum list installed | grep php 如果有安装的PHP包，先删除他们。 # yum remove php.x86_64 php-cli.x86_64 php-common.x86_64 php-gd.x86_64 php-ldap.x86_64 php-mbstring.x86_64 php-mcrypt.x86_64 php-mysql.x86_64 php-pdo.x86_64 配置yum源 追加CentOS 6.5的epel及remi源。 # rpm -Uvh http://ftp.iij.ad.jp/pub/linux/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm # rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm 安装PHP 复制下面命令请勿换行执行。先复制到文本中，编辑成一行，在执行。 yum install --enablerepo=remi --enablerepo=remi-php56 php php-bcmath php-opcache php-devel php-mbstring php-mcrypt php-mysqlnd php-gd php-xml php-memcache php-redis php-fpm php-mysql php-common php-mssql 安装检查 [root@node1 ~]# php -version PHP 5.6.40 (cli) (built: May 28 2019 10:55:13) Copyright (c) 1997-2016 The PHP Group Zend Engine v2.6.0, Copyright (c) 1998-2016 Zend Technologies with Zend OPcache v7.0.6-dev, Copyright (c) 1999-2016, by Zend Technologies 配置php.ini文件，关闭php信息头。 vi etc/php.ini expose_php = Off // 关闭php信息 service php-fpm restart 设置开机启动。 chkconfig php-fpm --level 2345 on CentOS 6 安装 epel 源出现的问题 问题： Error: Cannot retrieve metalink for repository: epel. Please verify its path and try again 原因：/etc/yum.repos.d/epel.repo 配置文件中源地址没有生效 解决： vim /etc/yum.repos.d/epel.repo 修改： [epel] name=Extra Packages for Enterprise Linux 6 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/6/$basearch mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&arch=$basearch failovermethod=priority enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 修改成： [epel] .. baseurl=http://download.fedoraproject.org/pub/epel/6/$basearch #mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-6&arch=$basearch ... 保存退出后，清理源 yum clean all 参考资料 MACOS下安装PHP运行环境 Mac Nginx+php环境配置，看我就够了 在 macOS High Sierra 10.13 搭建 PHP 开发环境 php-fpm:No pool defined解决方法 centos 6.8 yum安装 PHP 5.6 安装epel源后，报错Error: Cannot retrieve metalink for repository: epel. Please verify its path.. ChangeLog 20190617 | 增加CenOS 6 安装 PHP 文档。 20190603 | 创建文档。 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-17 16:32:57 "},"20180904PopularAppOnMacOS.html":{"url":"20180904PopularAppOnMacOS.html","title":"MacOS中有哪些比较好的软件","keywords":"","body":"软件 | MacOS中有哪些比较好的软件 2018-09-04 购机 Mac常用软件 印象笔记/Evernote QQ/QQ音乐 Parallels Desktop for Mac V14：Mac下最省心的虚拟机。 https://www.newasp.net/soft/454880.html http://down-www.newasp.net/pcdown/soft/mac/parallelsdesktop14crack.dmg https://www.xp510.com/xiazai/ossoft/mactools/45226.html parallels client ： mac下最好用的windows远程桌面连接软件。 shadowsockX-NG：mac下的翻墙工具，需要自备ss账号。 google chrome：不解释 cornerstone：Mac下的svn工具。暂时没有找到理想的，这个还算不错。 坚果云：全平台云同步服务 keepassXC 2.3.4：keepass密码管理工具 microsoft offic for mac/wps for mac：Mac下支持Office真的很差，只能用这两个了，WPS在2018年底才出来，还需要优化。 foxmail 微云（主要是为了下载mac软件） FUSE for macOS 3.8.2：Mac下读写MTFS神器 https://github.com/osxfuse/osxfuse/releases Omni系列 Mac效率软件 cheatsheet：https://www.mediaatelier.com/CheatSheet/ atext alfred 3 （破解需要xcode command line） betterTouchTools（还需要研究一下怎么用。） 可以设置三指点击，=快捷键command+W，关闭标签页。 Keyboard Maestro： 如何用自动化神器 Keyboard Maestro，高效做读书笔记？（附教程） 让你的Mac成为超高效率的工作工具(Keyboard Maestro 和 Alfred的整合) AppCleaner：轻量级软件卸载工具 Dr. clean pro：Mac下的系统优化软件。 the unarchiver：Mac下的压缩与解压缩软件 iStat Menus_6.20 mac系统监控神器，可以显示电池剩余时间。 从finder到终端的小插件：cdto https://www.gracecode.com/posts/3081.html https://github.com/jbtule/cdto Magnet：可以快速实现窗口并排 AirServer：iPad投屏到Mac PopClip：快捷菜单，可以实现各种快捷操作。不太好的一点，就是每次选中都会有一个Command+c的操作，在Excel里面用的时候，就不爽了。 Typeeto/1Keyboard：可以实现mac键盘充当iphone、ipad蓝牙键盘。神器。 Bartender：Mac任务栏精简工具，可以把不常用的图标收缩起来。 iText：一款从图片中识别文字、并翻译的工具。先安装1.2.8版本，然后使用appstore升级。 开发/编辑器 axure：原型设计 navicat：数据库管理 typaro：最好用的Markdown编辑器 pandoc，为了能导出word。https://github.com/jgm/pandoc/releases/tag/2.2.3.2 macdown，与Typora配合，可以复制完整带格式的文本到邮件。轻量。但是不能拷贝图片。 idea xcode command line jdk7/8 maven https://www.jianshu.com/p/191685a33786 安装brew，主要是为了把原生的svn1.9降级到1.8，兼容cornerstone3.0.3 https://www.jianshu.com/p/4e80b42823d5 svn降级 ：https://note.devework.com/mac/mac-svn.html nodejs https://nodejs.org/en/#download，官网下载安装 Node.js v10.10.0 to /usr/local/bin/node npm v6.4.1 to /usr/local/bin/npm 安装cnpm：http://npm.taobao.org # npm install -g cnpm --registry=https://registry.npm.taobao.org Visual Studio Code：轻量开发编辑器 FortiClient：SSLVPN客户端 PostMan：http测试工具 FileZilla：FTP、SFTP客户端 nginx：brew install nginx 影音工具 Xnip：长图截图工具 Irvue：自动切换壁纸 看图：腾讯出品的图片查看器 IINA：好用的视频播放器 Final Cut Pro：不解释 Movavi Screen Capture Studio 5：视频编辑 Kap：轻量的录屏软件 Resize Master：图片缩放工具 LICEcap：gip录屏软件。 windows虚拟机安装软件 360zip qq(主要是为了远程桌面) microsoft office microsoft project visio powerdesigner 16 VPN客户端 PanDown客户端：PanDownload 2.0.1，不限速，百度云下载加速器。下载： https://www.gdaily.org/15944/pandownload https://pandownload.com 哪些已经尝试了但放弃了 DEVONThink：https://sspai.com/post/44648 因为对中文的支持不太好，中文搜索与中文格式编码的支持不太好。 占用资源太大，大约2G内存。 使用Idex方式，无法与finder中的文件同步 2019-01-24 折腾了几个小时，发现用不起来，还是放弃。 Path Finder 因为没有什么特色，也没有比原生的finder更好用。放弃！ XtraFinder（启用SIP：https://www.jianshu.com/p/a47e96645d88） 因为需要启用SIP，怕影响原生系统，放弃了。 电池使用记录 2018-09-05 工作使用场景（开PD虚拟机） 日期 开始时间 结束时间 开始电量 结束电量 使用时间 2018-09-05 09:00 11:30 100% 55% 2.5小时 2018-09-05 13:30 15:00 50% 30% 1.5小时 2018-09-05 15:00 16:00 30% 10% 1小时 2018-09-05 16:00 16:23 10% 5% 0.5小时 Mac快捷键使用 MacOS command + Q 关闭APP command + Q 锁屏 mac中delete键的5种用法： 第一种：按 delete 键，实现 Windows 键盘上退格键的功能，也就是删除光标之前的一个字符（默认）； 第二种：按 fn+delete 键，删除光标之后的一个字符； 第三种：按 option+delete 键，删除光标之前的一个单词（英文有效）； 第四种：结合第二种，按住fn+option+delete，删除光标之后的一个单词； 第五种：选中文件后按 command+delete，删除掉该文件。 QQ command + M 最小化qq shift + command + W 打开联系人（自定义） shift + command + G 搜索联系人（自定义） ctrl + command + A 截图 ctrl + command + F 搜索联系人 Evernote 最好用的快捷键：https://www.ifanr.com/app/643581 command + J 启动快捷搜索框 ctrl + command + N 快捷笔记面板 印象笔记的markdown仅适合用来写文章。不能用快捷键返回和前进 返回和前进的快捷键：command + [ 返回 / command + ] 前进 删除线 ： ctrl + command + K ⌘ N 新記事 ⌥ ⌘ S 显示/隐藏侧边栏 全局快捷键 Ctrl ⌘ E 在 Evernote 中搜尋 Ctrl ⌥ ⌘ N 新記事視窗 Ctrl ⌘ N 快速記事 Safari浏览器切换标签快捷键（参考：https://sspai.com/post/30902，《Safari for OS X 你不可不知的 10 个快捷键》） 从左向右切换标签的快捷键是： Control + tab. 反向切换标签则是： Control + shift + tab. Option + tab/command + shift + tab 标签切换。 command + w 关闭当前标签 command + z 重新打开 command + [ 上一页 command + ] 下一页 command + shift + b 显示隐藏收藏栏。 command + shift + L 打开侧边栏 command + 单击链接，实现在后台打开新标签页。 command + R 刷新 Mac 自动操作 APP 设置 全局 command + \\ 打开finder shift + command + E 打开Evernote shift + command + W 打开QQ * Keyboard Maestro快捷键设置 option+command为打开Application的快捷键前缀 option+数字为最常用的文件夹与文件快捷键 Mac小技巧 如何从finder中文件夹进入terminal？ 打开Finder中的服务偏好设置，勾选快捷键tab下新建位于文件夹位置的终端窗口。 从finder到终端的小插件 cdto 查看Mac电量剩余使用时间 打开活动监视器-能耗标签，可以查看。 使用iStat Menus_6.20 mac系统监控神器，可以显示电池剩余时间。 安装jdk 安装目录：/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/bin/java 多版本jdk切换：https://blog.csdn.net/tianxiawuzhei/article/details/48263789 输入全角空格 按Shift+Option(Alt)+B组合键打开符号选择框，在底部选中“符号”选项，第六个就是。 参考：https://www.jianshu.com/p/d0ffea021315 mac上右键菜单新增文件夹打开方式 https://www.jianshu.com/p/d5f21b8b79e8 Mac下的【预览】工具增强插件： https://www.jianshu.com/p/9b9e53db22c9 Alfred使用 http://www.alfredworkflow.com https://sspai.com/post/44624 与文件相关的命令 find in open > 常用的一些自定义搜索配置。 百度：https://www.baidu.com/s?ie=utf-8&f=8&wd={query} 简书：http://www.jianshu.com/search?utf8=%E2%9C%93&q={query} 淘宝：http://s.taobao.com/search?oe=utf-8&f=8&q={query} 京东：https://search.jd.com/Search?keyword={query}&enc=utf-8&wq={query} 微信文章：http://weixin.sogou.com/weixin?type=2&query={query} stackoverflow：http://www.stackoverflow.com/search?q={query} github：https://github.com/search?utf8=%E2%9C%93&q={query} maven：http://mvnrepository.com/search?q={query} Android API Search：https://developer.android.com/reference/classes.html#q={query} Alfred + Evernote：https://www.jianshu.com/p/604d7519b6ed https://www.alfredforum.com/topic/840-evernote-workflow-9-beta-3/?tab=comments#comment-4055 http://alfredworkflow.com/ 推荐软件 https://github.com/hzlzh/Best-App/blob/master/README.md ChangeLog 20190609 | 转移到GitBook 20180904 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-11 15:26:28 "}}