{"./":{"url":"./","title":"前言","keywords":"","body":"前言 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-04-03 20:00:24 "},"20190606_how-to-build-your-own-knowledge-base.html":{"url":"20190606_how-to-build-your-own-knowledge-base.html","title":"如何建立自己的知识库","keywords":"","body":"如何建立自己的知识库 知识库建立原则 使用知识的三层模型构建自己的知识库：卡片、文件、项目。 知识库建立最终极的目的是生成一套知识体系，提升生成新知识的能力，完成知识创造的伟大历程。 知识库类型。 知识库包括三大类：工作类、学习类、生活类 工作类包括一切公司内部公共知识与产品文档、项目文档等。 学习类包括一切通过学习获取的知识，包括技术学习、阅读书籍、听书、浏览文章等。这些都是已有的形式知识，需要通过学习总结不断内化为自己的暗隐知识，从而生成新的知识。 生活类主要是记录生活的重要事情，比如：买车买房的过程、日记等。 知识库操作指南 知识库的建立少不了工具的支持，工具的选择必须要精简，减少工具使用过程中不断做选择的精力耗散。 收集过程：使用Evernote、滴答清单进行收集。使用WorkFlowy也可以进行快速记录。 卡片类：所有的卡片全部记录在WorkFlowy中。 文件类：文件的目的是为了分享，所以每一篇文章都需要具备成果意识。用Markdown来写，使用MWeb工具。 学习与生活类文件的组织逻辑：放在 gitbook.zhbwang.com 域名文件夹中。 文件层级尽量不超过两层。比如学习类文件全部放在一个文件夹 wiki@learning 中；生活类文件全部放在一个文件夹 wiki@life 中。这种方法有一个很大的好处，就是所有文章都在一个文件夹，能形成合力。 在文件夹中新建 drafts 文件夹，临时草稿区。根目录中就是已经定稿的。 工作类的文件暂时还没有想好。 gitbook.zhbwang.com 域名文件夹使用坚果云存储。 项目类：项目是一个成果，比如一本书，一个产品，或者一个完成的产品文档。如果说把所有资料都丢掉，只能保留很小的一部分，那么能保留的部分就是这个项目类的成果了。所以项目类的成果一定要精简，并且有结构。 在文件类，已经将一篇一篇的文章写好了，那么项目就要使用README或者SUMMARY文件驱动，将文章形成有结构的一本书，或者一个体系。 这部分也同样使用 GitBook 完成。 在另一个目录新建 GitBook 文件夹，将生活类文件夹、学习类文件夹建立一个软连接到 GitBook 文件夹，写个脚本，自动编译为 html，本机中的 nginx 代理，通过 http://localhost 即可访问。 可以使用 Fluid 将 http://localhost 网址转换为APP的形式，可以很方便的打开。 下一步思考：工作类文件应该如何处理？ 参考资料 ChangeLog 20190606 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 19:39:53 "},"20190515_let-hadoop-run.html":{"url":"20190515_let-hadoop-run.html","title":"让Hadoop在MacOS上跑起来","keywords":"","body":"让Hadoop在MacOS上跑起来 大数据学习之路系列01 已发布博客：腾讯云社区、CSDN博客、语雀。 本安装文档是在MacOS中安装单机版Hadoop。 安装目录 WZB-MacBook:50_bigdata wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata 准备工作 JDK Mac安装JDK的过程略，参考：MAC下安装多版本JDK和切换几种方式 WZB-MacBook:50_bigdata wangzhibin$ java -version java version \"1.7.0_80\" Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) WZB-MacBook:50_bigdata wangzhibin$ echo $JAVA_HOME /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 下载Hadoop brew install wget wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/core/hadoop-2.8.4/hadoop-2.8.4.tar.gz WZB-MacBook:50_bigdata wangzhibin$ tar -zxvf hadoop-2.8.4.tar.gz 安装与配置Hadoop 修改JDK配置 WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hadoop-env.sh export JAVA_HOME=${JAVA_HOME}改为 export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 验证Hadoop WZB-MacBook:hadoop-2.8.4 wangzhibin$ bin/hadoop Usage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar run a jar file note: please use \"yarn jar\" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive classpath prints the class path needed to get the credential interact with credential providers Hadoop jar and the required libraries daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settings Most commands print help when invoked w/o parameters. 单机模式执行 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar grep input output 'dfs[a-z.]+' $ cat output/* 1 dfsadmin 配置core-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ mkdir -p hdfs/tmp WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/core-site.xml 增加如下配置： hadoop.tmp.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp Abase for other temporary directories. fs.defaultFS hdfs://localhost:9000 配置hdfs-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hdfs-site.xml 增加如下配置： dfs.replication 1 dfs.namenode.name.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name dfs.datanode.data.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/data 启动与停止Hadoop 配置.bash_profile # set hadoop export HADOOP_HOME=/Users/wangzhibin/00_dev_suite/50_bigdata/hadoop export PATH=$PATH:$HADOOP_HOME/bin 第一次启动hdfs需要格式化 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./bin/hdfs namenode -format ... 19/05/15 22:30:47 INFO common.Storage: Storage directory /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name has been successfully formatted. ... 启动HDFS ./sbin/start-dfs.sh 停止HDFS ./sbin/stop-dfs.sh HDFS启动状态查看 HDFS 状态：http://localhost:50070/dfshealth.html#tab-overview Secordary NameNode 状态：http://localhost:50090/status.html 本地官方文档：API文档 验证HDFS 简单的验证hadoop命令： $ hadoop fs -mkdir /test WZB-MacBook:hadoop wangzhibin$ hadoop fs -ls / Found 1 items drwxr-xr-x - wangzhibin supergroup 0 2019-05-16 11:26 /test 启动时遇到的坑 一、sh: connect to host localhost port 22: Connection refused 此时可能会出现如下错误。是因为没有配置ssh免密登录。 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:38:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: ssh: connect to host localhost port 22: Connection refused localhost: ssh: connect to host localhost port 22: Connection refused Starting secondary namenodes [0.0.0.0] 0.0.0.0: ssh: connect to host 0.0.0.0 port 22: Connection refused 19/05/15 22:38:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 采用如下方法解决： 1）解决方法是选择系统偏好设置->选择共享->点击远程登录 2）设置免密登录 $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys $ ssh localhost 二、Unable to load native-hadoop library for your platform WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:50:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: starting namenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-namenode-WZB-MacBook.local.out localhost: starting datanode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-datanode-WZB-MacBook.local.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out 19/05/15 22:50:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 参考： 官方-Native Libraries Guide Mac OSX 下 Hadoop 使用本地库提高效率 Hadoop native libraries: Installation on Mac Osx 解决方案：重新编译hadoop，将编译后的hadoop-dist/target/hadoop-2.8.4/lib/native替换$HADOOP_HOME/lib/native。 安装基础组件 $ brew install gcc autoconf automake libtool cmake snappy gzip bzip2 zlib 安装protobuf。 wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz tar zxvf protobuf-2.5.0.tar.gz cd protobuf-2.5.0 ./configure make make install 重新编译hadoop wget http://apache.fayea.com/hadoop/common/hadoop-2.8.4/hadoop-2.8.4-src.tar.gz tar zxvf hadoop-2.8.4-src.tar.gz cd hadoop-2.8.4-src mvn package -Pdist,native -DskipTests -Dtar -e cp -r /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-dist/target/hadoop-2.8.4/lib/native . 三、An Ant BuildException has occured: exec returned WZB-MacBook:hadoop-2.8.4-src wangzhibin$ mvn package -Pdist,native -DskipTests -Dtar -e ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 [ERROR] around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml [ERROR] -> [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml 参考：mac下编译Hadoop 2.8.1报错An Ant BuildException has occured: exec returned: 1，排错过程 解决方案：配置环境变量OPENSSL_ROOT_DIR、OPENSSL_INCLUDE_DIR。修改~/.bash_profile # openssl export OPENSSL_ROOT_DIR=/usr/local/Cellar/openssl/1.0.2r export OPENSSL_INCLUDE_DIR=$OPENSSL_ROOT_DIR/include 配置与启动yarn 配置mapred-site.xml cd $HADOOP_HOME/etc/hadoop/ cp mapred-site.xml.template mapred-site.xml vim mapred-site.xml mapreduce.framework.name yarn 配置yarn-site.xml vim yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle yarn启动与停止 启动 cd $HADOOP_HOME ./sbin/start-yarn.sh ./sbin/stop-yarn.sh 浏览器查看：http://localhost:8088 jps查看进程 WZB-MacBook:hadoop wangzhibin$ jps 534 NutstoreGUI 49135 DataNode 49834 ResourceManager 49234 SecondaryNameNode 49973 Jps 67596 49912 NodeManager 49057 NameNode 到此，hadoop单机模式就配置成功了！ 命令与验证 Resource Manager: http://localhost:50070 JobTracker: http://localhost:8088/ Node Specific Info: http://localhost:8042/ Command $ jps $ yarn // For resource management more information than the web interface. $ mapred // Detailed information about jobs 参考资料 Hadoop: Setting up a Single Node Cluster. centos7 hadoop 单机模式安装配置 Hadoop in OSX El-Capitan Installing Hadoop on Mac OS X 10.9.4 macOS上搭建伪分布式Hadoop环境 本地官方API文档 ChangeLog 20190515 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:32 "},"20190517_the-first-mapreduce-program.html":{"url":"20190517_the-first-mapreduce-program.html","title":"第一个MapReduce程序","keywords":"","body":"第一个MapReduce程序 2019-05-17 | 大数据学习之路系列02 已发布博客：腾讯云社区、CSDN博客、语雀。 目标 单词计数是最简单也是最能体现 MapReduce 思想的程序之一，可以称为 MapReduce 版“Hello World”。 单词计数主要完成功能是：统计一系列文本文件中每个单词出现的次数，如下图所示。 准备工作 新建目录 WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/input WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/output WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/input drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 准备文件 WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file1.txt Hello World WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file2.txt Hello Hadoop WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 14:50 /practice/20190517_mr/input -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 14:49 /practice/20190517_mr/input/file1.txt -rw-r--r-- 1 wangzhibin supergroup 13 2019-05-17 14:49 /practice/20190517_mr/input/file2.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 运行例子 在集群上运行 WordCount 程序 备注:以 input 作为输入目录，output 目录作为输出目录。 已经编译好的 WordCount 的 Jar 在“$HADOOP_HOME/share/hadoop/mapreduce/”下面，就是“hadoop-mapreduce-examples-2.8.4.jar”， MapReduce 执行过程显示信息 执行命令： hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 执行过程： WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:39:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:39:20 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558078701666_0002 19/05/17 15:39:20 INFO impl.YarnClientImpl: Submitted application application_1558078701666_0002 19/05/17 15:39:20 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1558078701666_0002/ 19/05/17 15:39:20 INFO mapreduce.Job: Running job: job_1558078701666_0002 19/05/17 15:39:28 INFO mapreduce.Job: Job job_1558078701666_0002 running in uber mode : false 19/05/17 15:39:28 INFO mapreduce.Job: map 0% reduce 0% 19/05/17 15:39:33 INFO mapreduce.Job: map 100% reduce 0% 19/05/17 15:39:39 INFO mapreduce.Job: map 100% reduce 100% 19/05/17 15:39:39 INFO mapreduce.Job: Job job_1558078701666_0002 completed successfully 19/05/17 15:39:39 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=55 FILE: Number of bytes written=474472 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=271 HDFS: Number of bytes written=25 HDFS: Number of read operations=9 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=2 Launched reduce tasks=1 Data-local map tasks=2 Total time spent by all maps in occupied slots (ms)=6213 Total time spent by all reduces in occupied slots (ms)=2848 Total time spent by all map tasks (ms)=6213 Total time spent by all reduce tasks (ms)=2848 Total vcore-milliseconds taken by all map tasks=6213 Total vcore-milliseconds taken by all reduce tasks=2848 Total megabyte-milliseconds taken by all map tasks=6362112 Total megabyte-milliseconds taken by all reduce tasks=2916352 Map-Reduce Framework Map input records=2 Map output records=4 Map output bytes=41 Map output materialized bytes=61 Input split bytes=246 Combine input records=4 Combine output records=4 Reduce input groups=3 Reduce shuffle bytes=61 Reduce input records=4 Reduce output records=3 Spilled Records=8 Shuffled Maps =2 Failed Shuffles=0 Merged Map outputs=2 GC time elapsed (ms)=97 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=603979776 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=25 File Output Format Counters Bytes Written=25 查看结果 WZB-MacBook:hadoop wangzhibin$ hadoop dfs -ls -R /practice/20190517_mr/output/ -rw-r--r-- 1 wangzhibin supergroup 0 2019-05-17 15:39 /practice/20190517_mr/output/_SUCCESS -rw-r--r-- 1 wangzhibin supergroup 25 2019-05-17 15:39 /practice/20190517_mr/output/part-r-00000 WZB-MacBook:hadoop wangzhibin$ hadoop fs -cat /practice/20190517_mr/output/* Hadoop 1 Hello 2 World 1 遇到的坑 问题一：执行到Running job: job_1557977819409_0004的地方就不往下执行了。 WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoopoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:01:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:01:03 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:01:03 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:01:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557977819409_0004 19/05/17 15:01:04 INFO impl.YarnClientImpl: Submitted application application_1557977819409_0004 19/05/17 15:01:04 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1557977819409_0004/ 19/05/17 15:01:04 INFO mapreduce.Job: Running job: job_1557977819409_0004 参考： Hadoop相关总结 Hadoop 运行wordcount任务卡在job running的一种解决办法 hadoop2.7.x运行wordcount程序卡住在INFO mapreduce.Job: Running job:job _1469603958907_0002 Can't run a MapReduce job on hadoop 2.4.0 解决方案：在$HADOOP_HOME/etc/hadoop/yarn-site.xml中增加配置。 yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 最终的yarn-site.xml如下： yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce.shuffle.class org.apache.hadoop.mapred.ShuffleHandler yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 重启yarn： ./sbin/stop-yarn.sh ./sbin/start-yarn.sh ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:02:47 "},"20190517_detailed-hadoop-commands.html":{"url":"20190517_detailed-hadoop-commands.html","title":"Hadoop常用命令详解","keywords":"","body":"Hadoop常用命令详解 大数据学习之路03 已发布博客：腾讯云社区、CSDN博客、语雀。 Hadoop基本命令 version 查看Hadoop版本。 WZB-MacBook:target wangzhibin$ hadoop version Hadoop 2.8.4 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674 Compiled by jdu on 2018-05-08T02:50Z Compiled with protoc 2.5.0 From source with checksum b02a59bb17646783210e979bea443b0 This command was run using /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/share/hadoop/common/hadoop-common-2.8.4.jar HDFS基础命令 命令格式 hadoop fs -cmd ls 列出hdfs文件系统根目录下的目录和文件 hadoop fs -ls / 列出hdfs文件系统所有的目录和文件 hadoop fs -ls -R / mkdir 一级一级的建目录，父目录不存在的话使用这个命令会报错 command: hadoop fs -mkdir eg: WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir /test/20190517 WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 所创建的目录如果父目录不存在就创建该父目录 hadoop fs -mkdir -p put 上传文件。hdfs file的父目录一定要存在，否则命令不会执行 command: hadoop fs -put eg: $ hadoop fs -put tmp/tmp.txt /test $ hadoop fs -ls -R /test 上传目录。hdfs dir 一定要存在，否则命令不会执行 command: hadoop fs -put ... eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put tmp/ /test WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 从键盘读取输入到hdfs file中，按Ctrl+D（Control+D）结束输入。hdfs file不能存在，否则命令不会执行 command: hadoop fs -put - eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put - /test/20190517.tmp.txt hello world WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 10:44 /test/20190517.tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cat 在标准输出中显示文件内容 command: hadoop fs -cat eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -cat /test/20190517.tmp.txt hello world tail 在标准输出中显示文件末尾的1KB数据 command: hadoop fs -tail eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -tail /test/20190517.tmp.txt hello world get 从hdfs中下载文件到本地。local file不能和 hdfs file名字不能相同，否则会提示文件已存在，没有重名的文件会复制到本地 command: hadoop fs -get eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test/20190517.tmp.txt . WZB-MacBook:tmp wangzhibin$ ls 20190517.tmp.txt tmp.txt 拷贝多个文件或目录到本地时，本地要为文件夹路径 command: hadoop fs -get ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test . WZB-MacBook:tmp wangzhibin$ ls -l test/ total 24 drwxr-xr-x 2 wangzhibin staff 64 5 17 10:56 20190517 -rw-r--r-- 1 wangzhibin staff 12 5 17 10:56 20190517.tmp.txt drwxr-xr-x 3 wangzhibin staff 96 5 17 10:56 tmp -rw-r--r-- 1 wangzhibin staff 4662 5 17 10:56 tmp.txt 注意：如果用户不是root， local 路径要为用户文件夹下的路径，否则会出现权限问题。 rm 每次可以删除多个文件或目录 command: hadoop fs -rm ... hadoop fs -rm -r ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -rm /test/20190517.tmp.txt Deleted /test/20190517.tmp.txt WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cp HDFS拷贝文件或文件夹。目标文件或者文件夹不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件还存在。 command: hadoop fs -cp hadoop fs -cp ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -cp /test/tmp.txt /test/20190517/ WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:03 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt mv HDFS移动文件或文件夹。目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件不存在。源路径有多个时，目标路径必须为目录，且必须存在。 command: hadoop fs -mv hadoop fs -mv ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -mv /test/20190517/tmp.20190519.txt /test/tmp WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:04 /test/tmp/tmp.20190519.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 注意：跨文件系统的移动（local到hdfs或者反过来）都是不允许的 count 统计hdfs对应路径下的目录个数，文件个数，文件总计大小 显示为目录个数，文件个数，文件总计大小，输入路径 command: hadoop fs -count eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -count /test 3 4 18648 /test du 显示hdfs对应路径下每个文件夹和文件的大小 command: hadoop fs -du eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du /test 4662 /test/20190517 9324 /test/tmp 4662 /test/tmp.txt 显示hdfs对应路径下所有文件和的大小 command: hadoop fs -du -s eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -s /test 18648 /test 显示hdfs对应路径下每个文件夹和文件的大小,文件的大小用方便阅读的形式表示，例如用64M代替67108864 command: hadoop fs -du -h eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -h /test 4.6 K /test/20190517 9.1 K /test/tmp 4.6 K /test/tmp.txt HDFS高级命令 以下命令参考：hadoop HDFS常用文件操作命令。没有实践。 moveFromLocal hadoop fs -moveFromLocal ... 与put相类似，命令执行后源文件 local src 被删除，也可以从从键盘读取输入到hdfs file中 copyFromLocal hadoop fs -copyFromLocal ... 与put相类似，也可以从从键盘读取输入到hdfs file中 moveToLocal 当前版本中还未实现此命令 copyToLocal hadoop fs -copyToLocal ... 与get相类似 getmerge hadoop fs -getmerge 将hdfs指定目录下所有文件排序后合并到local指定的文件中，文件不存在时会自动创建，文件存在时会覆盖里面的内容 hadoop fs -getmerge -nl 加上nl后，合并到local file中的hdfs文件之间会空出一行 text hadoop fs -text 将文本文件或某些格式的非文本文件通过文本格式输出 setrep hadoop fs -setrep -R 3 改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作 stat hdoop fs -stat [format] 返回对应路径的状态信息 [format]可选参数有：%b（文件大小），%o（Block大小），%n（文件名），%r（副本个数），%y（最后一次修改日期和时间） 可以这样书写hadoop fs -stat %b%o%n ，不过不建议，这样每个字符输出的结果不是太容易分清楚 archive hadoop archive -archiveName name.har -p * 命令中参数name：压缩文件名，自己任意取； ：压缩文件所在的父目录；：要压缩的文件名；：压缩文件存放路径\\示例：hadoop archive -archiveName hadoop.har -p /user 1.txt 2.txt /des* 示例中将hdfs中/user目录下的文件1.txt，2.txt压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下，如果1.txt，2.txt不写就是将/user目录下所有的目录和文件压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下 显示har的内容可以用如下命令： hadoop fs -ls /des/hadoop.jar 显示har压缩的是那些文件可以用如下命令 hadoop fs -ls -R har:///des/hadoop.har 注意：har文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少NameNode和DataNode过多的空间浪费。 balancer hdfs balancer 如果管理员发现某些DataNode保存数据过多，某些DataNode保存数据相对较少，可以使用上述命令手动启动内部的均衡过程 dfsadmin hdfs dfsadmin -help 管理员可以通过dfsadmin管理HDFS，用法可以通过上述命令查看 hdfs dfsadmin -report 显示文件系统的基本数据 hdfs dfsadmin -safemode enter：进入安全模式；leave：离开安全模式；get：获知是否开启安全模式； wait：等待离开安全模式 distcp 用来在两个HDFS之间拷贝数据 MapReduce命令 命令帮助 WZB-MacBook:target wangzhibin$ mapred -help Usage: mapred [--config confdir] [--loglevel loglevel] COMMAND where COMMAND is one of: pipes run a Pipes job job manipulate MapReduce jobs queue get information regarding JobQueues classpath prints the class path needed for running mapreduce subcommands historyserver run job history servers as a standalone daemon distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive archive-logs combine aggregated logs into hadoop archives hsadmin job history server admin interface Most commands print help when invoked w/o parameters. 列出所有任务 WZB-MacBook:target wangzhibin$ mapred job -list all 19/05/20 10:22:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Total jobs:1 JobId State StartTime UserName Queue Priority UsedContainers RsvdContainers UsedMem RsvdMem NeededMem AM info job_1558104288185_0001 SUCCEEDED 1558104322342 wangzhibin default DEFAULT N/A N/A N/A N/A N/A http://WZB-MacBook.local:8088/proxy/application_1558104288185_0001/ 强制停止任务 mapred job -kill 参考资料 1.0.4版本官方文档-Hadoop Shell命令 hadoop HDFS常用文件操作命令 大数据基本组件（Hadoop、HDFS、MapRed、YARN）入门命令 ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:14 "},"20190517_develop-mapreduce-program-with-idea.html":{"url":"20190517_develop-mapreduce-program-with-idea.html","title":"使用IDEA开发MapReduce程序","keywords":"","body":"使用IDEA开发MapReduce程序 2019-05-17 | 大数据学习之路04 环境准备 jdk1.7 intellij idea maven 本地MapReduce程序之WordCount 这里以Hadoop的官方示例程序WordCount为例，演示如何一步步编写程序直到运行。 新建一个Maven工程 使用idea新建一个普通maven项目bigdata-learn-wordcount maven依赖 org.apache.hadoop hadoop-common 2.8.4 org.apache.hadoop hadoop-hdfs 2.8.4 org.apache.hadoop hadoop-mapreduce-client-core 2.8.4 org.apache.hadoop hadoop-client 2.8.4 拷贝Hadoop中的WordCount源码 /** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ package com.zhbwang.bigdata.example; import java.io.IOException; import java.util.StringTokenizer; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class WordCount { public static class TokenizerMapper extends Mapper{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } public static class IntSumReducer extends Reducer { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length [...] \"); System.exit(2); } Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); for (int i = 0; i 初始化文件 在工程根目录新建input文件夹，增加两个文件 input - file1.TXT Hello World - file2.txt Hello Hadoop 运行配置 程序执行 执行日志 执行结果 打包到服务器使用hadoop jar命令执行 pom.xml增加打包插件 maven-assembly-plugin 2.2 com.zhbwang.bigdata.example.WordCount jar-with-dependencies make-assembly package single maven打包 $ mvn clean install 得到一个可执行jar包：bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar 使用java -jar执行 在当前可执行jar目录初始化input文件夹 执行以下命令，即可在当前目录生成output文件夹，里面就是执行结果。 java -jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar input output 使用hadoop jar执行 一开始遇到问题了，还以为打包打的不对，换了几个打包插件都不行。 WZB-MacBook:target wangzhibin$ hadoop jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar /practice/20190517_mr/input /practice/20190517_mr/output Exception in thread \"main\" java.io.IOException: Mkdirs failed to create /var/folders/gg/35tlzsrs1kj3c460vh9tvvv40000gn/T/hadoop-unjar2170725475686001105/META-INF/license at org.apache.hadoop.util.RunJar.ensureDirectory(RunJar.java:140) at org.apache.hadoop.util.RunJar.unJar(RunJar.java:109) at org.apache.hadoop.util.RunJar.unJar(RunJar.java:85) at org.apache.hadoop.util.RunJar.run(RunJar.java:222) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) 后来找到几篇文章，发现是Mac的问题，在stackoverflow中找到解释： The issue is that a /tmp/hadoop-xxx/xxx/LICENSE file and a /tmp/hadoop-xxx/xxx/license directory are being created on a case-insensitive file system when unjarring the mahout jobs. 参考资料： Hadoop java.io.IOException: Mkdirs failed to create /some/path Mac下hadoop运行word count的坑 解决方案：删除原来压缩包的META-INF/LICENS即可。 zip -d bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar META-INF/LICENSE jar tvf bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar | grep LICENSE 接下来就可以使用hadoop jar命令执行了。 WZB-MacBook:target wangzhibin$ hadoop jar bigdata-learn-wordcount-1.0-SNAPSHOT-jar-with-dependencies.jar /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 22:07:09 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 22:07:10 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 22:07:10 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 22:07:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558078701666_0004 19/05/17 22:07:11 INFO impl.YarnClientImpl: Submitted application application_1558078701666_0004 19/05/17 22:07:11 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1558078701666_0004/ 19/05/17 22:07:11 INFO mapreduce.Job: Running job: job_1558078701666_0004 查看结果。 WZB-MacBook:target wangzhibin$ hadoop fs -cat /practice/20190517_mr/output/part-r-00000 Hadoop 1 Hello 2 World 1 问题 一、Hadoop 2.x中还有hadoop-core-x.x.jar吗？ 答：2.x系列已经没有hadoop-core的jar包了，取而代之的是 对于Hadoop2.x.x版本，需要引入4个jar： hadoop-common hadoop-hdfs hadoop-mapreduce-client-core hadoop-client jdk.tools（一般需要引入，否则报错） 参考：Hadoop需要的Jar包 参考资料 IDEA 配置Hadoop开发（开发调试） Hadoop入门学习之（二）：Intellij 开发Hadoop环境搭建 Hadoop: Intellij结合Maven本地运行和调试MapReduce程序 (无需搭载Hadoop和HDFS环境) 大数据系列（hadoop） 集群环境搭建 idea 开发设置 Mac下hadoop运行word count的坑 ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:00:43 "},"20190522_principle-and-architecture-analysis-of-hadoop .html":{"url":"20190522_principle-and-architecture-analysis-of-hadoop .html","title":"Hadoop原理与架构解析","keywords":"","body":"2019-05-22 | Hadoop原理与架构解析 2019-05-22 | 大数据学习之路05 Hadoop简介 基本介绍 Hadoop 是 Apache 开源组织的一个分布式计算开源框架，是一个可以更容易开发和运行处理大规模数据的解决方案，它提供了一套分布式系统基础架构，允许使用简单的编程模型跨大型计算机的大型数据集进行分布式处理。 Hadoop架构 Hadoop框架包括以下四个模块： Hadoop Common：这些是其他Hadoop模块所需的Java库和实用程序。这些库提供文件系统和操作系统级抽象，并包含启动Hadoop所需的必要Java文件和脚本。 Hadoop YARN：这是作业调度和集群资源管理的框架。 Hadoop分布式文件系统（HDFS）：提供对应用程序数据的高吞吐量访问的分布式文件系统。 Hadoop MapReduce： 这是基于YARN的大型数据集并行处理系统。 Hadoop 框架中最核心的设计就是：MapReduce 和 HDFS。 MapReduce 的思想是由 Google 的一篇论文所提及而被广为流传的，简单的一句话解释 MapReduce 就是“任务的分解与结果的汇总”。 HDFS 是 Hadoop 分布式文件系统（Hadoop Distributed File System）的缩写，为分布式计算存储提供了底层支持。 HDFS运行原理 HDFS简介 HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。 HDFS有很多特点： 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。为防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其他某几个主机上。 运行在廉价的机器上。 适合大数据的处理。HDFS会将一个完整的大文件平均分块存储到不同计算机上，默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。 流式数据访问，一次写入多次读写，和传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化只能在文件末尾添加 HDFS架构原理 HDFS 架构原理HDFS采用Master/Slave架构。 一个HDFS集群包含一个单独的NameNode和多个DataNode。 NameNode作为Master服务，它负责管理文件系统的命名空间和客户端对文件的访问。NameNode会保存文件系统的具体信息，包括文件信息、文件被分割成具体block块的信息、以及每一个block块归属的DataNode的信息。对于整个集群来说，HDFS通过NameNode对用户提供了一个单一的命名空间。 DataNode作为Slave服务，在集群中可以存在多个。通常每一个DataNode都对应于一个物理节点。DataNode负责管理节点上它们拥有的存储，它将存储划分为多个block块，管理block块信息，同时周期性的将其所有的block块信息发送给NameNode。 下图为HDFS系统架构图，主要有三个角色，Client、NameNode、DataNode。 HDFS的一些关键元素 Block：将文件分块，通常为64M。 NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间。保存整个文件系统的目录信息、文件信息及分块信息，由唯一一台主机专门保存。 SecondaryNameNode：是一个小弟，分担大哥NameNode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给NameNode。（热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。） DataNode：是Slave节点，奴隶，干活的。负责存储Client发来的数据块block；执行数据块的读写操作。 fsimage：元数据镜像文件（文件系统的目录树） edits：元数据的操作日志（针对文件系统做的修改操作记录） HDFS设计重点 HDFS 数据备份HDFS被设计成一个可以在大集群中、跨机器、可靠的存储海量数据的框架。它将所有文件存储成block块组成的序列，除了最后一个block块，所有的block块大小都是一样的。 HDFS中的文件默认规则是write one（一次写、多次读）的，并且严格要求在任何时候只有一个writer。 NameNode全权管理数据块的复制，它周期性地从集群中的每个DataNode接受心跳信号和块状态报告（BlockReport）。接收到心跳信号以为该DataNode工作正常，块状态报告包含了一个该DataNode上所有数据块的列表。 NameNode内存中存储的是=fsimage+edits。SecondaryNameNode负责定时（默认1小时）从NameNode上，获取fsimage和edits来进行合并，然后再发送给NameNode。减少NameNode的工作量。 文件写入 Client向NameNode发起文件写入的请求。 NameNode根据文件大小和文件块配置情况，返回给Client它所管理部分DataNode的信息。 Client将文件划分为多个block块，并根据DataNode的地址信息，按顺序写入到每一个DataNode块中。 以下过程完全参考自（【Hadoop】HDFS的运行原理） 例如：有一个文件FileA，100M大小。Client将FileA写入到HDFS上。 HDFS按默认配置。 HDFS分布在三个机架上Rack1，Rack2，Rack3。 文件写入过程如下： Client将FileA按64M分块。分成两块，block1和Block2; Client向NameNode发送写数据请求，如图蓝色虚线①------>。 NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②--------->。 Block1: host2,host1,host3 Block2: host7,host8,host4 原理： NameNode具有RackAware机架感知功能，这个可以配置。 若Client为DataNode节点，那存储block时，规则为：副本1，同Client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。 若Client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。 Client向DataNode发送block1；发送过程是以流式写入。流式写入过程如下： 将64M的block1按64k的package划分; 然后将第一个package发送给host2; host2接收完后，将第一个package发送给host1，同时Client想host2发送第二个package； host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。 以此类推，如图红线实线所示，直到将block1发送完毕。 host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。 Client收到host2发来的消息后，向NameNode发送消息，说我写完了。这样就真完成了。如图黄色粗实线 发送完block1后，再向host7、host8、host4发送block2，如图蓝色实线所示。 发送完block2后，host7、host8、host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。 Client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。 分析：通过写过程，我们可以了解到 写1T文件，我们需要3T的存储，3T的网络流量贷款。 在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。 挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。 文件读取 当文件读取： Client向NameNode发起文件读取的请求。 NameNode返回文件存储的block块信息、及其block块所在DataNode的信息。 Client读取文件信息。 如图所示，Client要从DataNode上，读取FileA。而FileA由block1和block2组成。读操作流程如下： Client向NameNode发送读请求。 NameNode查看Metadata信息，返回FileA的block的位置。 block1:host2,host1,host3 block2:host7,host8,host4 block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取。 上面例子中，Client位于机架外，那么如果Client位于机架内某个DataNode上，例如,Client是host6。那么读取的时候，遵循的规律是：优选读取本机架上的数据。 问题：如果读取block是按照先后顺序读，是否意味着在不同副本之间的读取是不平均的，没有考虑通过负载策略来提高读效率吗？ 备份数据的存放 备份数据的存放是HDFS可靠性和性能的关键。HDFS采用一种称为rack-aware的策略来决定备份数据的存放。 通过一个称为Rack Awareness的过程，NameNode决定每个DataNode所属rack id。 缺省情况下，一个block块会有三个备份： 一个在NameNode指定的DataNode上 一个在指定DataNode非同一rack的DataNode上 一个在指定DataNode同一rack的DataNode上。 这种策略综合考虑了同一rack失效、以及不同rack之间数据复制性能问题。 副本的选择：为了降低整体的带宽消耗和读取延时，HDFS会尽量读取最近的副本。如果在同一个rack上有一个副本，那么就读该副本。如果一个HDFS集群跨越多个数据中心，那么将首先尝试读本地数据中心的副本。 MapReduce运行原理 MapReduce简介 MapReduce是一种分布式计算模型，由Google提出，主要用于搜索领域，解决海量数据的计算问题。 MapReduce分成两个部分：Map（映射）和Reduce（归纳）。 当你向MapReduce框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map任务，然后分配到不同的节点上去执行，每一个Map任务处理输入数据中的一部分。 当Map任务完成后，它会生成一些中间文件，这些中间文件将会作为Reduce任务的输入数据。Reduce任务的主要目标就是把前面若干个Map的输出汇总并输出。 MapReduce的基本模型和处理思想 大规模数据处理时，MapReduce在三个层面上的基本构思 参考（MapReduce的基本工作原理） 如何对付大数据处理：分而治之 对相互间不具有计算依赖关系的大数据，实现并行最自然的办法就是采取分而治之的策略。 什么样的计算任务可进行并行化计算？ A：不可分拆的计算任务或相互间有依赖关系的数据无法进行并行计算！ 一个大数据若可以分为具有同样计算过程的数据块，并且这些数据块之间不存在数据依赖关系，则提高处理速度的最好办法就是并行计算。 上升到抽象模型：Mapper与Reducer MPI等并行计算方法缺少高层并行编程模型，为了克服这一缺陷，MapReduce借鉴了Lisp函数式语言中的思想，用Map和Reduce两个函数提供了高层的并行编程抽象模型。 关键思想：为大数据处理过程中的两个主要处理操作提供一种抽象机制。Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。 MapReduce借鉴了函数式程序设计语言Lisp中的思想，定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现: map: (k1; v1) → [(k2; v2)]。 输入：键值对(k1; v1)表示的数据 处理：文档数据记录(如文本文件中的行，或数据表格中的行)将以“键值对”形式传入map函数；map函数将处理这些键值对，并以另一种键值对形式输出处理的一组键值对中间结果[(k2; v2)] 输出：键值对[(k2; v2)]表示的一组中间数据 reduce: (k2; [v2]) → [(k3; v3)] 输入： 由map输出的一组键值对[(k2; v2)] 将被进行合并处理将同样主键下的不同数值合并到一个列表[v2]中，故reduce的输入为(k2; [v2]) 处理：对传入的中间结果列表数据进行某种整理或进一步的处理,并产生最终的某种形式的结果输出[(k3; v3)] 。 输出：最终输出结果[(k3; v3)] 示例：假设有4组原始文本数据 Text 1: the weather is good Text 2: today is good Text 3: good weather is good Text 4: today has good weather MapReduce处理方式： 使用4个map节点： map节点1: 输入：(text1, “the weather is good”) 输出：(the, 1), (weather, 1), (is, 1), (good, 1) map节点2: 输入：(text2, “today is good”) 输出：(today, 1), (is, 1), (good, 1) map节点3: 输入：(text3, “good weather is good”) 输出：(good, 1), (weather, 1), (is, 1), (good, 1) map节点4: 输入：(text3, “today has good weather”) 输出：(today, 1), (has, 1), (good, 1), (weather, 1) 使用3个reduce节点： reduce节点1： 输入：(good, 1),(good, 1),(good, 1),(good, 1),(good, 1) 输出：(good, 5) reduce节点2： 输入：(has, 1),(is, 1),(is, 1),(is, 1) 输出：(has, 1),(is, 3) reduce节点3： 输入：(the, 1),(today, 1),(today, 1),(weather, 1),(weather, 1),(weather, 1) 输出：(the, 1),(today, 2),(weather, 3) 上升到构架：统一构架，为程序员隐藏系统层细节 MPI等并行计算方法缺少统一的计算框架支持，程序员需要考虑数据存储、划分、分发、结果收集、错误恢复等诸多细节；为此，MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。 各个map函数对所划分的数据并行处理，从不同的输入数据产生不同的中间结果输出 各个reduce也各自并行计算，各自负责处理不同的中间结果数据集合进行reduce处理之前，必须等到所有的map函数做完，因此，在进入reduce前需要有一个同步障(barrier)；这个阶段也负责对map的中间结果数据进行收集整理(aggregation & shuffle)处理，以便reduce更有效地计算最终结果最终汇总所有reduce的输出结果即可获得最终结果。 MapReduce提供一个统一的计算框架，可完成： 计算任务的划分和调度 数据的分布存储和划分 处理数据与计算任务的同步 结果数据的收集整理(sorting, combining, partitioning,…) 系统通信、负载平衡、计算性能优化处理 处理系统节点出错检测和失效恢复 MapReduce运行流程 MapReduce的物理架构 Map-Reduce的处理过程主要涉及以下四个部分： 客户端Client：用于提交Map-reduce任务job JobTracker：协调整个job的运行，其为一个Java进程，其main class为JobTracker TaskTracker：运行此job的task，处理input split，其为一个Java进程，其main class为TaskTracker HDFS：hadoop分布式文件系统，用于在各个进程间共享Job相关的文件 MapReduce的逻辑运行流程 MapReduce运行按照时间顺序包括五个阶段：输入分片（input split）、map阶段、combiner阶段、shuffle阶段和reduce阶段。 输入分片（input split） 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务。 输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组 输入分片（input split）和hdfs的block（块）关系很密切。假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split），那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。 map阶段：就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行。 combiner阶段： Combiner是一个本地化的reduce操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作 shuffle阶段 将map的输出作为reduce的输入的过程就是shuffle了，这个是mapreduce优化的重点地方。 具体shuffle的过程不介绍了。 reduce阶段：和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。 简单的来说： 有一个待处理的大数据，被划分成大小相同的数据库(如64MB)，以及与此相应的用户作业程序。 系统中有一个负责调度的主节点(JobTracker)，以及数据Map和Reduce工作节点(TaskTracker). 用户作业提交给主节点JobTracker。 主节点为作业程序寻找和配备可用的Map节点，并将程序传送给map节点。 主节点也为作业程序寻找和配备可用的Reduce节点，并将程序传送给Reduce节点。 主节点启动每一个Map节点执行程序，每个Map节点尽可能读取本地或本机架的数据进行计算。(实现代码向数据靠拢，减少集群中数据的通信量)。 每个Map节点处理读取的数据块，并做一些数据整理工作(combining,sorting等)并将数据存储在本地机器上；同时通知主节点计算任务完成并告知主节点中间结果数据的存储位置。 主节点等所有Map节点计算完成后，开始启动Reduce节点运行；Reduce节点从主节点所掌握的中间结果数据位置信息，远程读取这些数据。 Reduce节点计算结果汇总输出到一个结果文件，即获得整个处理结果。 YARN运行原理 YARN简介 Yarn是Hadoop集群的分布式资源管理系统。Hadoop2.0对MapReduce框架做了彻底的设计重构，我们称Hadoop2.0中的MapReduce为MRv2或者Yarn，YARN是为了提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。 原MapReduce框架的不足 Hadoop 原 MapReduce 架构如下： 原 MapReduce 程序的流程及设计思路： 首先用户程序 (JobClient) 提交了一个 job，job 的信息会发送到 Job Tracker 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。 TaskTracker 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。 TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以给新提交的 job 分配运行在哪些机器上。上图虚线箭头就是表示消息的发送 - 接收的过程。 随着分布式系统集群的规模和其工作负荷的增长，原框架的问题逐渐浮出水面，主要的问题集中如下： JobTracker 是 Map-reduce 的集中处理点，存在单点故障。 JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗。业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限。 在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一块，很容易出现 OOM。 在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。 源代码层面分析的时候，会发现代码非常的难读，常常因为一个 class 做了太多的事情，代码量达 3000 多行，造成 class 的任务不清晰，增加 bug 修复和版本维护的难度。 从操作的角度来看，现在的 Hadoop MapReduce 框架在有任何重要的或者不重要的变化 ( 例如 bug 修复，性能提升和特性化 ) 时，都会强制进行系统级别的升级更新。更糟的是，它不管用户的喜好，强制让分布式集群系统的每一个用户端同时更新。这些更新会让用户为了验证他们之前的应用程序是不是适用新的 Hadoop 版本而浪费大量时间。 Yarn/MRv2的产生 为从根本上解决旧 MapReduce 框架的性能瓶颈，促进 Hadoop 框架的更长远发展，从 0.23.0 版本开始，Hadoop 的 MapReduce 框架完全重构，发生了根本的变化。新的 Hadoop MapReduce 框架命名为 MapReduceV2 或者叫 Yarn。 在Yarn中把job的概念换成了application，因为在新的Hadoop2.x中，运行的应用不只是MapReduce了，还有可能是其它应用如一个DAG（有向无环图Directed Acyclic Graph，例如storm应用）。 Yarn的另一个目标就是拓展Hadoop，使得它不仅仅可以支持MapReduce计算，还能很方便的管理诸如Hive、Hbase、Pig、Spark/Shark等应用。各种应用就可以互不干扰的运行在同一个Hadoop系统中，共享整个集群资源。 YARN组件与架构 Yarn主要由以下几个组件组成： ResourceManager：Global（全局）的进程 NodeManager：运行在每个节点上的进程 ApplicationMaster：Application-specific（应用级别）的进程。ApplicationMaster是对运行在Yarn中某个应用的抽象，它其实就是某个类型应用的实例，ApplicationMaster是应用级别的，它的主要功能就是向ResourceManager（全局的）申请计算资源（Containers）并且和NodeManager交互来执行和监控具体的task。 Scheduler：是ResourceManager的一个组件。Scheduler是ResourceManager专门进行资源管理的一个组件，负责分配NodeManager上的Container资源，NodeManager也会不断发送自己Container使用情况给ResourceManager。 Container：节点上一组CPU和内存资源。Container是Yarn对计算机计算资源的抽象，它其实就是一组CPU和内存资源，所有的应用都会运行在Container中。 新的 Hadoop MapReduce 框架（Yarn）架构 YARN执行过程 Application在Yarn中的执行过程，整个执行过程可以总结为三步： 应用程序提交 启动应用的ApplicationMaster实例 ApplicationMaster实例管理应用程序的执行 客户端程序向ResourceManager提交应用并请求一个ApplicationMaster实例 ResourceManager找到可以运行一个Container的NodeManager，并在这个Container中启动ApplicationMaster实例 ApplicationMaster向ResourceManager进行注册，注册之后客户端就可以查询ResourceManager获得自己ApplicationMaster的详细信息，以后就可以和自己的ApplicationMaster直接交互了 在平常的操作过程中，ApplicationMaster根据resource-request协议向ResourceManager发送resource-request请求 当Container被成功分配之后，ApplicationMaster通过向NodeManager发送container-launch-specification信息来启动Container， container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料 应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster 在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，交流的协议也是application-specific协议 一但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，用到所有的Container也归还给系统 。 思考题 还是那个经典的题目，一个10G大小的文件，给定1G大小的内存，如何使用Java程序统计出现次数最多的10个单词及次数 参考资料 Hadoop简介 分布式计算开源框架 Hadoop 介绍 Hadoop-介绍 【Hadoop】HDFS的运行原理 分布式计算框架Hadoop原理及架构全解 Hadoop 原理总结 MapReduce原理与设计思想 MapReduce的基本工作原理 Hadoop 之MapReduce 运行原理全解析 hadoop 学习笔记：mapreduce框架详解 Hadoop核心之HDFS 架构设计 Hadoop核心之MapReduce架构设计 Hadoop Yarn详解 Hadoop 新 MapReduce 框架 Yarn 详解 ChangeLog 20190522 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 17:12:50 "},"20190525_installation-and-use-of-hive.html":{"url":"20190525_installation-and-use-of-hive.html","title":"Hive安装与使用","keywords":"","body":"2019-05-25 | Hive安装与使用 2019-05-25 | 大数据学习之路06 Hive简介 这里先简单介绍，明确Hive的目标是什么。后续会详细介绍Hive架构与原理。 Hive是基于Hadoop的数据仓库工具，可以对存储在HDFS中的文件的数据进行数据整理、查询、分析。 Hive提供了类似与SQL的查询语言——HiveQL，可以通过HQL实现简单的MR统计，Hive将HQL语句转换成MR任务进行执行。 Hive下载 Hive与Hadoop对应关系 截止当前（2019-05-25），Hive最新版本有三种：hive-1.2.2、hive-2.3.5、hive-3.1.1。 Hive官网下载页面说明，hive-2.3.5对应Hadoop版本是2.x.y，hive-3.1.1对应Hadoop版本是3.x.y。 本人安装Hadoop版本是2.8.4，故下载hive-2.3.5。 Hive下载地址 下载地址：http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz WZB-MacBook:50_bigdata wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata WZB-MacBook:50_bigdata wangzhibin$ wget http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz Hive安装配置 Hive安装 解压 WZB-MacBook:50_bigdata wangzhibin$ tar zxvf apache-hive-2.3.5-bin.tar.gz 配置.bash_profile WZB-MacBook:50_bigdata wangzhibin$ vi ~/.bash_profile WZB-MacBook:50_bigdata wangzhibin$ source ~/.bash_profile 增加如下配置： # hive export HIVE_HOME=/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin export PATH=$PATH:$HIVE_HOME/bin 验证是否安装成功 WZB-MacBook:50_bigdata wangzhibin$ hive --version Hive 2.3.5 Git git://HW13934/Users/gates/git/hive -r 76595628ae13b95162e77bba365fe4d2c60b3f29 Compiled by gates on Tue May 7 15:45:09 PDT 2019 From source with checksum c7864fc25abcb9cf7a36953ac6be4665 Hive配置 由于hive是默认将元数据保存在本地内嵌的 Derby 数据库中，但是这种做法缺点也很明显，Derby不支持多会话连接，因此本文将选择mysql作为元数据存储。 需要先安装Mysql，本文不做过多介绍，可以自行百度。 需要下载mysql的jdbc，然后将下载后的jdbc放到hive安装包的lib目录下。 WZB-MacBook:50_bigdata wangzhibin$ wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gz WZB-MacBook:50_bigdata wangzhibin$ tar zxvf mysql-connector-java-5.1.47.tar.gz WZB-MacBook:50_bigdata wangzhibin$ cd mysql-connector-java-5.1.47 WZB-MacBook:mysql-connector-java-5.1.47 wangzhibin$ cp mysql-connector-java-5.1.47-bin.jar $HIVE_HOME/lib/ 修改配置hive-site.xml WZB-MacBook:~ wangzhibin$ cd $HIVE_HOME/conf WZB-MacBook:conf wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/conf WZB-MacBook:conf wangzhibin$ cp hive-default.xml.template hive-site.xml WZB-MacBook:conf wangzhibin$ vim hive-site.xml 配置文件如下： javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword mysql javax.jdo.option.ConnectionURL jdbc:mysql://localhost:3306/hive javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver 在mysql中初始化hive的schema（在此之前需要创建mysql下的hive数据库） WZB-MacBook:conf wangzhibin$ cd $HIVE_HOME/bin WZB-MacBook:bin wangzhibin$ schematool -dbType mysql -initSchema hive库中会初始化一些模型表： 到此配置完毕。HDFS中并未初始化数据仓库位置。 Hive使用 创建一个hive测试库 WZB-MacBook:bin wangzhibin$ hive SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] Logging initialized using configuration in jar:file:/Users/wangzhibin/00_dev_suite/50_bigdata/apache-hive-2.3.5-bin/lib/hive-common-2.3.5.jar!/hive-log4j2.properties Async: true Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases. hive> hive> create database hive_1; OK Time taken: 4.089 seconds hive> show databases; OK default hive_1 Time taken: 0.123 seconds, Fetched: 2 row(s) hive> 看看HDFS目录发生了什么变化 WZB-MacBook:conf wangzhibin$ hadoop fs -ls -R /user drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse/hive_1.db drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 15:53 /user/wangzhibin 看看mysql下的hive库有什么变化 mysql> use hive; mysql> select * from DBS; +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ | DB_ID | DESC | DB_LOCATION_URI | NAME | OWNER_NAME | OWNER_TYPE | +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ | 1 | Default Hive database | hdfs://localhost:9000/user/hive/warehouse | default | public | ROLE | | 2 | NULL | hdfs://localhost:9000/user/hive/warehouse/hive_1.db | hive_1 | wangzhibin | USER | +-------+-----------------------+-----------------------------------------------------+---------+------------+------------+ 2 rows in set (0.00 sec) 创建一个hive测试表 hive> use hive_1; OK Time taken: 3.772 seconds hive> create table hive_01 (id int,name string); OK Time taken: 0.582 seconds hive> show tables; OK hive_01 Time taken: 0.087 seconds, Fetched: 1 row(s) hive> 看看HDFS目录发生了什么变化 WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /user drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:22 /user/hive/warehouse drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:28 /user/hive/warehouse/hive_1.db drwxr-xr-x - wangzhibin supergroup 0 2019-05-25 18:28 /user/hive/warehouse/hive_1.db/hive_01 drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 15:53 /user/wangzhibin 看看mysql下的hive库有什么变化 mysql> select * from TBLS; +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ | TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER | RETENTION | SD_ID | TBL_NAME | TBL_TYPE | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT | IS_REWRITE_ENABLED | +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ | 1 | 1558780134 | 2 | 0 | wangzhibin | 0 | 1 | hive_01 | MANAGED_TABLE | NULL | NULL | | +--------+-------------+-------+------------------+------------+-----------+-------+----------+---------------+--------------------+--------------------+--------------------+ 1 row in set (0.00 sec) 看一下web上有什么变化。 以上就是hive的简单使用，说白了，hive与mysql的使用差不多；对应于hdfs，hive_1库是hdfs中的一个目录，hive_01表也是一个目录。 参考资料 Hive基础知识介绍 Hive详细介绍及简单应用 hive简介 Hive安装与配置详解 ChangeLog 20190525 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:08:19 "},"20190527_implementation-of-wordcount-by-hive.html":{"url":"20190527_implementation-of-wordcount-by-hive.html","title":"Hive实现wordcount词频统计","keywords":"","body":"2019-05-27 | Hive实现wordcount词频统计 2019-05-27 | 大数据学习之路07 新建测试文件 WZB-MacBook:tmp wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata/tmp WZB-MacBook:tmp wangzhibin$ vi test.txt 增加内容： hello man what are you doing now my running hello kevin hi man 文件导入到hive 建表并指定文件内容分隔符 hive> use hive_1; OK Time taken: 0.024 seconds hive> create table wc(txt String) row format delimited fields terminated by '\\t'; OK Time taken: 0.715 seconds hive> show tables; OK hive_01 wc Time taken: 0.035 seconds, Fetched: 2 row(s) 导入文件 HDFS初始无数据 WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /user/hive/warehouse/hive_1.db/wc 返回无数据 导入文件 hive> load data local inpath '/Users/wangzhibin/00_dev_suite/50_bigdata/tmp/test.txt' overwrite into table wc; Loading data to table hive_1.wc OK Time taken: 2.235 seconds hive> select * from wc; OK hello man what are you doing now my running hello kevin hi man Time taken: 1.602 seconds, Fetched: 6 row(s) HDFS文件内容 WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /user/hive/warehouse/hive_1.db/wc -rwxr-xr-x 1 wangzhibin supergroup 63 2019-05-27 21:09 /user/hive/warehouse/hive_1.db/wc/test.txt WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -cat /user/hive/warehouse/hive_1.db/wc/test.txt hello man what are you doing now my running hello kevin hi man 使用HQL统计单词 hive> select split(txt,' ') from wc; OK [\"hello\",\"man\"] [\"what\",\"are\",\"you\",\"doing\",\"now\"] [\"my\",\"running\"] [\"hello\"] [\"kevin\"] [\"hi\",\"man\"] Time taken: 0.337 seconds, Fetched: 6 row(s) hive> select explode(split(txt,' ')) from wc; OK hello man what are you doing now my running hello kevin hi man Time taken: 0.094 seconds, Fetched: 13 row(s) hive> select t1.word,count(t1.word) from (select explode(split(txt,' '))word from wc)t1 group by t1.word; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases. Query ID = wangzhibin_20190527214319_1532be66-b6e5-4603-83a9-dc4c7d6ec466 Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks not specified. Defaulting to jobconf value of: 1 In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer= In order to limit the maximum number of reducers: set hive.exec.reducers.max= In order to set a constant number of reducers: set mapreduce.job.reduces= Starting Job = job_1558964487152_0004, Tracking URL = http://WZB-MacBook.local:8088/proxy/application_1558964487152_0004/ Kill Command = /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/bin/hadoop job -kill job_1558964487152_0004 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1 2019-05-27 21:43:25,976 Stage-1 map = 0%, reduce = 0% 2019-05-27 21:43:31,173 Stage-1 map = 100%, reduce = 0% 2019-05-27 21:43:36,365 Stage-1 map = 100%, reduce = 100% Ended Job = job_1558964487152_0004 MapReduce Jobs Launched: Stage-Stage-1: Map: 1 Reduce: 1 HDFS Read: 8923 HDFS Write: 294 SUCCESS Total MapReduce CPU Time Spent: 0 msec OK are 1 doing 1 hello 2 hi 1 kevin 1 man 2 my 1 now 1 running 1 what 1 you 1 Time taken: 17.562 seconds, Fetched: 11 row(s) 小插曲：[执行结果未出来的原因是：没有启动yarn] 总结 split--------------------------列变数组 explode------------------------数组拆分成多行 group by和count----------------对行分组后求各行出现的次数 参考资料： Hive实现wordcount词频统计 ChangeLog 20190527 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:09:01 "},"20190529_architecture-and-principle-of-hive.html":{"url":"20190529_architecture-and-principle-of-hive.html","title":"Hive原理详解","keywords":"","body":"2019-05-29 | Hive原理详解 2019-05-29 | 大数据学习之路08 Hive概述 数据仓库的概念 首先要来看一下数据库与数据仓库的区别。 数据库：传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。 数据仓库：数据仓库系统的主要应用主要是OLAP（On-Line Analytical Processing），支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 主要区别如下： 数据库偏重数据的业务处理（transaction），属于OLTP（Online transaction processing）层面，后者着重于分析，可能会重点面向某个行业，属于OLAP（Online analytical processing）层面。 数据库一般叫“业务型数据库”，数据仓库被称为“分析型数据库”。数据库常采用行式存储，而数据仓库常采用列式存储，数据结构有利于查询和分析。 前者的用户数量大（主要是业务人员），既要执行“读”操作也要执行“写”操作，每次写的量不大，但是对时间敏感。后者的用户数量小（主要是决策人员），一般只需要执行读操作，每次读取的数据量很大，对反应时间不那么敏感。 把所需要的数据从业务型数据库导入分析型数据仓库的过程，称为ETL（Extract-Transform-Load，“抽取-转换-加载”）。 数据库用到的工具主要有MySQL, Oracle, MS SQLServer等，数据仓库用到的工具主要有Hive, AWSRedshift, Green Plum, SAP HANA等。 参考： 数据库 与 数据仓库的本质区别是什么？ \"数据库\" vs. \"数据仓库\": 区别与联系 Hive简介 Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取、转化、加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。 Hive是一个构建于Hadoop顶层的数据仓库工具，可以查询和管理PB级别的分布式数据。 支持大规模数据存储、分析，具有良好的可扩展性 某种程度上可以看作是用户编程接口，本身不存储和处理数据。 依赖分布式文件系统HDFS存储数据。 依赖分布式并行计算模型MapReduce处理数据。 定义了简单的类似SQL 的查询语言——HiveQL。 用户可以通过编写的HiveQL语句运行MapReduce任务。 可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到Hadoop平台上。 是一个可以提供有效、合理、直观组织和使用数据的分析工具。 参考：Hive技术原理解析 Hive与传统数据库的区别 Hive适用场景 Hive不适用于大规模数据集实现低延迟快速查询。 Hive 构建在基于静态批处理的Hadoop 之上，Hadoop 通常都有较高的延迟并且在作业提交和调度的时候需要大量的开销。因此，Hive 并不能够在大规模数据集上实现低延迟快速的查询，例如，Hive 在几百MB 的数据集上执行查询一般有分钟级的时间延迟。 Hive 并不适合那些需要低延迟的应用，例如，联机事务处理（OLTP）。Hive 查询操作过程严格遵守Hadoop MapReduce 的作业执行模型，Hive 将用户的HiveQL 语句通过解释器转换为MapReduce 作业提交到Hadoop 集群上，Hadoop 监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，Hive 并不提供实时的查询和基于行级的数据更新操作。Hive 的最佳使用场合是大数据集的批处理作业，例如，网络日志分析。 Hive适用于以下场景： 数据挖掘：用户行为分析；兴趣分区；区域展示； 非实时分析：日志分析；文本分析。 数据汇总：每天/每周用户点击数，流量统计。 数据仓库：数据抽取，加载，转换（ETL）。 Hive功能与架构 Hive与Hadoop Hive的执行入口是Driver，执行的SQL语句首先提交到Drive驱动，然后调用compiler解释驱动，最终解释成MapReduce任务去执行。 Hive与Hadoop生态系统中其他组件的关系 Hive依赖于HDFS 存储数据 Hive依赖于MapReduce 处理数据 在某些场景下Pig可以作为Hive的替代工具 HBase 提供数据的实时访问 Hive系统架构 （图片来源：Design - Apache Hive） Hive的工作原理简单的说就是一个查询引擎，接收到一个SQL，后续工作包括： 词法分析/语法分析 使用antlr将SQL语句解析成抽象语法树(AST) 语义分析 从Metastore获取模式信息，验证SQL语句中队表名，列名，以及数据类型的检查和隐式转换，以及Hive提供的函数和用户自定义的函数(UDF/UAF) 逻辑计划生成 生成逻辑计划--算子树 逻辑计划优化 对算子树进行优化，包括列剪枝，分区剪枝，谓词下推等 物理计划生成 将逻辑计划生成包含由MapReduce任务组成的DAG的物理计划 物理计划执行 将DAG发送到Hadoop集群进行执行 （图片来源：http://infolab.stanford.edu/~ragho/hive-icde2010.pdf ） Hive的数据模型 Hive的数据存储在HDFS上，基本存储单位是表或者分区，Hive内部把表或者分区称作SD，即Storage Descriptor。一个SD通常是一个HDFS路径，或者其它文件系统路径。SD的元数据信息存储在Hive MetaStore中，如文件路径，文件格式，列，数据类型，分隔符。Hive默认的分格符有三种，分别是\\^A、\\^B和\\^C，即ASCii码的1、2和3，分别用于分隔列，分隔列中的数组元素，和元素Key-Value对中的Key和Value。 分区：数据表可以按照某个字段的值划分分区。 每个分区是一个目录。 分区数量不固定。 分区下可再有分区或者桶。 桶：数据可以根据桶的方式将不同数据放入不同的桶中。 每个桶是一个文件。 建表时指定桶个数，桶内可排序。 数据按照某个字段的值Hash后放入某个桶中。 Hive可以创建托管表和外部表： 默认创建托管表，Hiva会将数据移动到数据仓库的目录。 创建外部表，这时Hiva会到仓库目录以外的位置访问数据。 如果所有处理都由Hive完成，建议使用托管表。 如果要用Hive和其他工具来处理同一个数据集，建议使用外部表。 Hive工作原理 SQL语句转换成MapReduce作业的基本原理 join的实现原理 group by实现原理 Hive中SQL查询转换成MapReduce作用的过程 当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作： 驱动模块接收该命令或查询编译器 对该命令或查询进行解析编译 由优化器对该命令或查询进行优化计算 该命令或查询通过执行器进行执行 第1步：由Hive驱动模块中的编译器对用户输入的SQL语言进行词法和语法解析，将SQL语句转化为抽象语法树的形式。 第2步：抽象语法树的结构仍很复杂，不方便直接翻译为MapReduce算法程序，因此，把抽象语法书转化为查询块。 第3步：把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符。 第4步：重写逻辑查询计划，进行优化，合并多余操作，减少MapReduce任务数量。 第5步：将逻辑操作符转换成需要执行的具体MapReduce任务。 第6步：对生成的MapReduce任务进行优化，生成最终的MapReduce任务执行计划。 第7步：由Hive驱动模块中的执行器，对最终的MapReduce任务进行执行输出。 几点说明： 当启动MapReduce程序时，Hive本身是不会生成MapReduce算法程序的。 需要通过一个表示“Job执行计划”的XML文件驱动执行内置的、原生的Mapper和Reducer模块。 Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在JobTracker所在的管理节点上执行。 通常在大型集群上，会有专门的网关机来部署Hive工具。网关机的作用主要是远程操作和管理节点上的JobTracker通信来执行任务。 数据文件通常存储在HDFS上，HDFS由名称节点管理。 Hive 的数据类型 Hive 的数据存储支持 HDFS 的一些文件格式，比如 CSV, Sequence File, Avro, RC File, ORC, Parquet。也支持访问 HBase。 Hive 支持原子和复杂数据类型，原子数据类型包括：数据值、布尔类型、字符串类型等，复杂的类型包括：Array、Map和Struct。其中Array和Map和java中的Array和Map是相似的，Struct和C语言中的Struct相似。 Create table test( col1 Array, col2 Map, col3 Struct ); 参考资料 Design-Apache Hive Hive原理及查询优化 Hive技术原理 Hive技术原理解析 Hive 工作原理详解 如何通俗地理解Hive的工作原理？ ChangeLog 20190609 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 17:01:55 "},"20180712_commands-of-linux.html":{"url":"20180712_commands-of-linux.html","title":"Linux常用命令","keywords":"","body":"技术 | Linux常用命令 全文检索 find / -print -type f | xargs grep '/var/lib/psql/data' 查看端口占用情况 lsof -i:端口号 netstat -tunlp|grep 端口号 这两种方式都可以查看指定端口被哪个进程占用的情况。 Linux 查看CPU信息，机器型号，内存等信息 系统 # uname -a # 查看内核/操作系统/CPU信息 # lsb_release -a # 查看操作系统版本 (适用于所有的linux，包括Redhat、SuSE、Debian等发行版，但是在debian下要安装lsb) # cat /proc/cpuinfo # 查看CPU信息 # hostname # 查看计算机名 # lspci -tv # 列出所有PCI设备 # lsusb -tv # 列出所有USB设备 # lsmod # 列出加载的内核模块 # env # 查看环境变量 资源 # free -m # 查看内存使用量和交换区使用量 # df -h # 查看各分区使用情况 # du -sh # 查看指定目录的大小 # grep MemTotal /proc/meminfo # 查看内存总量 # grep MemFree /proc/meminfo # 查看空闲内存量 # uptime # 查看系统运行时间、用户数、负载 # cat /proc/loadavg # 查看系统负载 磁盘和分区 # mount | column -t # 查看挂接的分区状态 # fdisk -l # 查看所有分区 # swapon -s # 查看所有交换分区 # hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) # dmesg | grep IDE # 查看启动时IDE设备检测状况 网络 # ifconfig # 查看所有网络接口的属性 # iptables -L # 查看防火墙设置 # route -n # 查看路由表 # netstat -lntp # 查看所有监听端口 # netstat -antp # 查看所有已经建立的连接 # netstat -s # 查看网络统计信息 进程 # ps -ef # 查看所有进程 # top # 实时显示进程状态 # jps # 查看java进程 用户 # w # 查看活动用户 # id # 查看指定用户信息 # last # 查看用户登录日志 # cut -d: -f1 /etc/passwd # 查看系统所有用户 # cut -d: -f1 /etc/group # 查看系统所有组 # crontab -l # 查看当前用户的计划任务 服务 # chkconfig --list # 列出所有系统服务 # chkconfig --list | grep on # 列出所有启动的系统服务 程序 # rpm -qa # 查看所有安装的软件包 查看CPU信息（型号） # cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 8 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz (看到有8个逻辑CPU, 也知道了CPU型号) # cat /proc/cpuinfo | grep physical | uniq -c 4 physical id : 0 4 physical id : 1 (说明实际上是两颗4核的CPU) # getconf LONG_BIT 32 (说明当前CPU运行在32bit模式下, 但不代表CPU不支持64bit) # cat /proc/cpuinfo | grep flags | grep ' lm ' | wc -l 8 (结果大于0, 说明支持64bit计算. lm指long mode, 支持lm则是64bit) 再完整看cpu详细信息, 不过大部分我们都不关心而已. # dmidecode | grep 'Processor Information' 查看内 存信息 # cat /proc/meminfo # uname -a Linux euis1 2.6.9-55.ELsmp #1 SMP Fri Apr 20 17:03:35 EDT 2007 i686 i686 i386 GNU/Linux (查看当前操作系统内核信息) # cat /etc/issue | grep Linux Red Hat Enterprise Linux AS release 4 (Nahant Update 5) (查看当前操作系统发行版信息) 查看机器型号 # dmidecode | grep \"Product Name\" 查看网卡信息 # dmesg | grep -i eth ChangeLog 20190605 | 增加「查看端口占用情况」 20180712 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:27 "},"20180603_introduction-of-markdown.html":{"url":"20180603_introduction-of-markdown.html","title":"初步认识Markdown","keywords":"","body":"初步认识markdown 为什么是Markdown 文本编辑比较 无格式文本：简洁、不依赖工具 富文本：美观，重点突出 Markdown与富文本编辑器异同： 作用一致： 使用者输入纯文字，通过编辑器的处理，使其拥有一份样式，最终得到带格式的文档。 使用区别： 富文本编辑器「所见即所得」 Markdown是一种标记语言，手动切换预览与编辑模式。 Markdown同时兼具无格式文本与富文本的优势。 什么是Markdown Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。 —— 维基百科 Markdown 是一种轻量级的「标记语言」，创始人为约翰·格鲁伯。Markdown 的创始人 John Gruber 这样定义： \"Markdown\" is two things: (1) a plain text formatting syntax; (2) a software tool, that converts the plain text formatting to others. 也就是说Markdown首先意味着是一套语法规则，其次代表了编辑器，把纯文本转换为排版效果的文字。 Markdown语法演进 CommonMark GFM(GitHub Flavored Markdown) 其他语法：PHP Markdown Extra、Maruku、kramdown、RDiscount、Redcarpet、MultiMarkdown 原有的 Markdown 语法的功能稍显不足，Github Flavored Markdown 在前面所说的语法的三个方面都做出了相应的增强。 比如： 标准Markdown要在一行的最后加两个空格符才表示换行，否则是不换行的；但是GFM则没有此要求。 支持把列表变成带勾选框的任务列表 在对段落的处理方面，对原有代码块进行了增强，可以制定不同的语言类型对代码进行语法高亮。 GFM的修改参考：GFM修改，GFM语法参考：GFM语法 语法特点： 用简洁的语法代替排版，其常用的标记符号不超过十个， 相对于更为复杂的 HTML 标记语言来说，Markdown 十分的轻量 一旦熟悉这种语法规则，会有沉浸式编辑的效果。 其他增强型Markdown语法：MultiMarkdown Markdown优势 书写过程流畅 富文本编辑器编辑文字时是两个不连续的动作，输入文字时双手放在键盘上，编辑文字则需要视线和手离开输入框和键盘，去寻找和点击功能按钮。 Markdown 的「书写流畅」就体现在将这两个动作合成一个输入字符的动作。 格式不随编辑器而改变，导出与分享方便 Markdown 格式保持的文件本质上仍是一份纯文本。 书写错误容易发现。 比如Word中，用空格、分页来控制排版，容易出错。而Markdown没有不需要考虑几个空格的问题，如果有几个词语想加粗，没有渲染成功，就说明写错了。 Markdown 的局限性 什么时候不该用 Markdown？ Markdown 无法对「段落」进行灵活处理。比如：文本位置 Markdown 对非纯文本元素的排版能力很差。比如：图文混排 Markdown一开始就定位为「文字输入工具」，不适合对排版格式自定义程度较高的文档进行排版。 Markdown适用场景 网络环境下的写作 利用了 Markdown 「写作即排版」的特点，Markdown 可以让使用者专心于文章书写，而非排版。 文档协作 团队成员间可以自由选用自己喜欢的操作系统和编辑器工具来进行写作，而不局限于 Word 或者 Google Docs等只支持富文本编辑的软件。 文档的展示方式不仅仅是在编辑器中，你可以随时把文档转换成网页，任何时候任何人都可以方便地查看。 利用它「纯文本格式」的优势，用 Markdown 来文档协作会比其他工具更自由。 基础语法 标题 markdown代码： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 ### 文字格式 **This is bold text** *This text is italicized* ~~This was mistaken text~~ **This text is _extremely_ important** 区块引用 尼采说： Was mich nicht umbringt, macht mich stärker. markdown代码： #### 尼采说： > Was mich nicht umbringt, macht mich **stärker**. 列表 无序列表 George Washington John Adams Thomas Jefferson George Washington John Adams Thomas Jefferson 有序列表 James Madison James Monroe John Quincy Adams 嵌套列表 First list item First nested list item Second nested list item markdown代码： #### 无序列表 - George Washington - John Adams - Thomas Jefferson * George Washington * John Adams * Thomas Jefferson #### 有序列表 1. James Madison 2. James Monroe 3. John Quincy Adams #### 嵌套列表 1. First list item - First nested list item - Second nested list item 代码 行内代码 Use git status to list all new or modified files that haven't yet been committed. 代码块 @requires_authorization def somefunc(param1='', param2=0): '''A docstring''' if param1 > param2: # interesting print 'Greater' return (param2 - param1 + 1) or None class SomeClass: pass >>> message = '''interpreter ... prompt''' markdown代码： #### 行内代码 Use `git status` to list all new or modified files that haven't yet been committed. #### 代码块 ​```python @requires_authorization def somefunc(param1='', param2=0): '''A docstring''' if param1 > param2: # interesting print 'Greater' return (param2 - param1 + 1) or None class SomeClass: pass >>> message = '''interpreter ... prompt''' ​`` ` 分割线 这是分割线 这也是分割线 markdown代码： 这是分割线 --- 这也是分割线 *** 链接 网络链接 This site was built using GitHub Pages. This site was built using GitHub Pages. 相对链接 Test.md 图片链接 markdown代码： #### 网络链接 This site was built using [GitHub Pages](https://pages.github.com/). This site was built using [GitHub Pages][1]. #### 相对链接 [Test.md](./test.md) #### 图片链接 ![markdown](http://pic.iloc.cn/2019-06-05-I-love-markdown-syntax-language.png) 脚注 脚注demo 参考1 markdown代码： 脚注[^demo] 参考[^1] 高级语法 注：可能会有渲染不支持的情况。 表格 Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 markdown代码： #### 表格 | Item | Value | Qty | | :------- | -------: | :--: | | Computer | 1600 USD | 5 | | Phone | 12 USD | 12 | | Pipe | 1 USD | 234 | 目录 [TOC] markdown代码： #### 目录 [TOC] 待办事项 使用 - [ ] 和 - [x] 语法可以创建复选框，实现 todo-list 等功能。例如： [x] 已完成事项 [ ] 待办事项1 [ ] 待办事项2 markdown代码： #### 待办事项 使用 `- [ ]` 和 `- [x]` 语法可以创建复选框，实现 todo-list 等功能。例如： - [x] 已完成事项 - [ ] 待办事项1 - [ ] 待办事项2 公式 $$E=mc^2$$ 可以创建行内公式，例如 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。或者块级公式： $$x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ markdown代码： #### 公式 $$E=mc^2$$ 可以创建行内公式，例如 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。或者块级公式： $$x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ 流程图 st=>start: Start op=>operation: Your Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op markdown代码： #### 流程图 ​```flow st=>start: Start op=>operation: Your Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op ​`` ` 序列图 Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks! markdown代码： #### 序列图 ​```sequence Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks! ​`` ` 工具推荐 MWeb（推荐，收费，Mac/IOS） Typora（推荐，免费。全平台） 特点： WYSIWYG（What You See Is What You Get） 表格编辑功能增强 插入图片 代码和数学公式输入 支持显示目录大纲 下载：https://www.typora.io/ 介绍：https://sspai.com/post/30292 参考资料 印象笔记 Markdown 入门指南：https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php Markdown 完全入门（上）：https://sspai.com/post/36610 Markdown 完全入门（下）：https://sspai.com/post/36682 Markdown教程：http://www.markdown.cn/ 创始人 John Gruber 的 Markdown 语法说明：https://daringfireball.net/projects/markdown/syntax Github Flavored Markdown语法：https://help.github.com/articles/basic-writing-and-formatting-syntax/ 印象笔记 Markdown 入门指南：https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php Markdown简易入门教程：https://blog.huihut.com/2017/01/25/MarkdownTutorial/ Typora介绍：https://sspai.com/post/30292 ChangeLog 20190606 | 推荐工具增加MWeb，去掉小书匠、有道云笔记等。 20190606 | 从Evernote转移到GitBook 20180603 | 完成第一版，并在小组内分享 demo. 这是一个脚注。 ↩ 1. 这也是一个脚注。 ↩ Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 19:53:02 "},"20190214_git-code-transfer-to-gitee.html":{"url":"20190214_git-code-transfer-to-gitee.html","title":"Git代码转移到码云相关命令","keywords":"","body":"Git代码转移到码云相关命令 查看当前用户（global）配置 git config --global --list 生成公钥 ssh-keygen -t rsa -C \"youremail@example.com\" 查看本地代码的远程仓库地址 git remote -v 校验本机是否能连接gitee ssh -T git@gitee.com 重新设置远程仓库地址 git remote set-url origin git@gitee.com:xxx/xxxx.git OR git remote set-url origin https://gitee.com/xxxx/xxxx.git git remote -v 第一次pull出现问题的处理方法 git pull --allow-unrelated-histories 代码的换行符与windows不一致的情况，处理方法： git config --global core.autocrlf input ChangeLog 20190606 | 从Evernote迁移到GitBook 20190214 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:27 "},"20190603_doduwiki-installation-on-macos.html":{"url":"20190603_doduwiki-installation-on-macos.html","title":"DokuWiki安装小记","keywords":"","body":"DokuWiki安装小记 2019-06-03 以下为MacOS平台的DokuWiki安装，其他平台类似。 什么是DokuWiki DokuWiki 是一个使用，用途多样的开源 Wiki 软件，并且不需要数据库。 比较wiki 安装准备工作 安装nginx。 安装php。参考MacOS下安装PHP与nginx配置 安装DokuWiki 安装过程参考：DokuWiki with Mac OS X and Apache 下载DokuWiki。可以在官网直接下载，我下载的是2019-05-31 snapshot版本，放到目录/usr/local/Cellar_w/raw下。 WZB-MacBook:raw wangzhibin$ pwd /usr/local/Cellar_w/raw WZB-MacBook:Cellar_w wangzhibin$ wget https://download.dokuwiki.org/src/dokuwiki/dokuwiki-stable.tgz 解压DokuWiki安装包，并创建软连接。 WZB-MacBook:raw wangzhibin$ tar zxvf splitbrain-dokuwiki-upstream-0.0.20091252c-7022-g2e6e11a.tar.gz WZB-MacBook:raw wangzhibin$ cd .. WZB-MacBook:Cellar_w wangzhibin$ pwd /usr/local/Cellar_w WZB-MacBook:Cellar_w wangzhibin$ ln -s raw/splitbrain-dokuwiki-2e6e11a/ dokuwiki 配置nginx代理 打开nginx.conf，在http块添加一行include servers/*.conf;（默认已经存在）。MacOS使用brew安装nginx的配置文件目录在/usr/local/etc/nginx/。 在/usr/local/etc/nginx/servesr下创建dokuwiki.conf。 增加以下内容： server { listen 8090; root /usr/local/Cellar_w/dokuwiki; server_name localhost; index index.php index.html doku.php; location ~ ^/(data|conf|bin|inc) { return 404; } location ~ ^/lib.*\\.(gif|png|ico|jpg)$ { expires 31d; } #location / { # try_files $uri $uri/ @dokuwiki; #} location @dokuwiki { rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&id=$2 last; rewrite ^/tag/(.*) /doku.php?id=tag:$1&do=showtag&tag=tag:$1 last; rewrite ^/(.*) /doku.php?id=$1&$args last; } location ~ .+\\.php($|/) { include fastcgi_params; set $real_script_name $uri; set $path_info \"/\"; if ($fastcgi_script_name ~ \"^(.+\\.php)(/.+)$\") { set $real_script_name $1; set $path_info $2; } fastcgi_param SCRIPT_FILENAME $document_root$real_script_name; fastcgi_param SCRIPT_NAME $real_script_name; fastcgi_param PATH_INFO $path_info; fastcgi_pass 127.0.0.1:9000;#监听9000端口 fastcgi_index index.php; } } 重新启动nginx，下面两种命令都可以。 sudo brew services restart nginx /usr/local/bin/nginx -s reload 在浏览器中打开http://localhost:8090，即可访问dokuWiki首页了。 使用DokuWiki 初始化安装与ACL启用 中文文件名乱码问题 在界面中搜索“测试页面” 点击红色的“:测试页面”，即可创建中文词条“测试页面”。随便写个内容，保存。 在后台pages目录下该词条名称乱码。 下面就开始修改源码，使得中文词条名称正常。 在dokuwiki/conf/local.php文件最后一行加上。（如果conf目录仅发现local.php.dist文件，这是没有install的缘故。） $conf[ 'fnencode' ] = 'utf-8'; #注意分号不能少。 再次尝试创建中文词条 发现pages目录正常。 Doku主题 官方主题排行 官方主题按照受欢迎程度排名靠前的如下： 主题安装 以主题Breeze为例。 下载主题Breeze Template，解压到lib/tpl/目录下，命名为breeze。 在conf/local.php文件中增加： $conf['template']='breeze'; // 配置的是tpl下主题目录名。 推荐主题 推荐的几个主题。在官网主题区可以搜索到。 * bootstrap3 * vector (强烈推荐) * sprintdoc * breeze * white 修改vector主题样式 修改侧边栏的宽度 修改lib/tpl/vector/static/3rd/vector/main-ltr.css中的样式： div#panel { ... width: 13em; ... } ... /* Content */ div#content { margin-left: 13em; ... } ... /* Footer */ div#footer { margin-left: 13em; ... } ... /* Navigation Containers */ #left-navigation { ... left: 13em; ... } 更换logo 使用Vector主题时，logo位置为：background-image:url(/lib/tpl/vector/static/3rd/dokuwiki/logo.png); 替换即可。 DokuWiki插件 侧边栏插件indexmenu 安装indexmenu插件 在sidebar页面增加配置，左侧会自动出现全部页面导航。 indexmenu使用 仅显示:wiki里面的内容 默认展开到两层 仅展现wiki下的，并且去掉toc、右键菜单。 Vector主题下不能展示sidebar的修改方法，在lib/tpl/vector/conf/default.php中修改： //$conf[\"vector_navigation_location\"] = \":wiki:navigation\"; //page/article used to store the navigation $conf[\"vector_navigation_location\"] = \":sidebar\"; //page/article used to store the navigation 增加排序 新增页面插件Add New Page 在扩展管理器中搜索“Add New Page”，安装此插件。 在sidebar页面下方增加，表示仅在wiki命名空间下增加页面。 ------------------------ false 有了page新增管理，那么命名空间怎么新增呢？ 直接在浏览器输入http://localhost:8091/doku.php?id=wiki:test:新目录:新文件，即可创建。 如何删除文件？ 进入编辑页面，文章内容全部删除后，保存，该文章就被删除了。与此同时，如果命名空间下没有文章，命名空间也被删除了 移动插件move 在扩展管理器中搜索“move”，安装此插件。 使用方法，在管理界面，会出现“页面移动/重命名...”的工具，可以进入管理界面。 贡献插件authorstats 安装authorstats插件。 在文章中增加 保存后即可查看 评论区插件discussion 插件：plugin:discussion 用法：~~DISCUSSION~~，插入该语句到 wiki 中，即可在 wiki 内容后添加评论区。 配置：管理->配置管理->Discussion，比较有用的配置： 订阅评论区 通知版主有新评论 允许未注册用户评论 可通过 wiki 语法评论 其他可直接安装使用的插件 编辑器支持markdown语法插件：plugin:markdowku 编辑器支持直接粘贴图片：plugin:imgpaste 导出Word文件plugin:OpenOffice.org 附录 local.php文件内容 sidebar.txt文件内容 {{indexmenu>:wiki#2|js#vista.png notoc nomenu tsort}} --------- {{NEWPAGE>wiki}} start内容底部包括贡献内容 . 参考资料 dokuwiki 搭建 官方插件 用 Dokuwiki 管理小团队知识 dokuwiki安装使用教程（支持中文、editor.md、粘贴上传图片） ChangeLog 20190603 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 12:09:45 "},"20190513_fast-connection-of-ssh-with-iterm.html":{"url":"20190513_fast-connection-of-ssh-with-iterm.html","title":"iTerm2快速SSH连接并保存密码","keywords":"","body":"2019-05-13 | iTerm2快速SSH连接并保存密码 2019-05-15 发布博客。 背景 Mac自带terminal，以及比较好用的iTerm2命令行工具，都缺乏一个功能，就是远程SSH连接，无法保存密码。 一种方法是将本机的ssh_key放到远程服务器中实现无密码登录。这种方法在很多情况下无法实现，因为远程服务器大多是客户的。 本文介绍一个简单、轻量级的命令行工具——sshpass，通过它我们能够向命令提示符本身提供密码（非交互式密码验证），这样就可以实现自动连接远程服务器，而且能自动执行远程命令。 安装sshpass 下载sshpass：https://sourceforge.net/projects/sshpass/files/ 进入 sshpass目录 运行【./configure】 运行【sudo make install】 运行【sshpass 】 来测试是否安装成功 sshpass使用 Usage: sshpass [-f|-d|-p|-e] [-hV] command parameters -f filename Take password to use from file -d number Use number as file descriptor for getting password -p password Provide password as argument (security unwise) -e Password is passed as env-var \"SSHPASS\" With no parameters - password will be taken from stdin -h Show help (this screen) -V Print version information At most one of -f, -d, -p or -e should be used 使用用户名和密码登录到远程Linux ssh服务器（10.42.0.1），并检查文件系统磁盘使用情况，如图所示。 $ sshpass -p 'my_pass_here' ssh aaronkilik@10.42.0.1 'df -h' 也可以使用sshpass 通过scp传输文件或者rsync备份/同步文件，如下所示： ------- Transfer Files Using SCP ------- $ scp -r /var/www/html/example.com --rsh=\"sshpass -p 'my_pass_here' ssh -l aaronkilik\" 10.42.0.1:/var/www/html ------- Backup or Sync Files Using Rsync ------- $ rsync --rsh=\"sshpass -p 'my_pass_here' ssh -l aaronkilik\" 10.42.0.1:/data/backup/ /backup/ iTerm2集成sshpass实现快速SSH连接 打开iTerm2的Profiles菜单，进入Profiles设置。 点击Edit Profiles。 增加SSH连接。 Name：名称 Tags：分组或者标签名称 Title：设置窗口名称 Command：/usr/local/bin/sshpass -p 'xxxx' ssh root@192.168.129.116 快速连接 参考资料 sshpass：一个很棒的免交互SSH登录工具，但不要用在生产服务器上 iTerm2 保存ssh用户名密码 ChangeLog 20190609 | 转移到GitBook 20190515 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 14:19:17 "},"20190603_macos_brew_update_too_slow.html":{"url":"20190603_macos_brew_update_too_slow.html","title":"MacOS的brew update很慢问题解决方案","keywords":"","body":"2019-06-03 | MacOS的brew update很慢问题解决方案 替换为中科大源 替换brew.git cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.git 替换homebrew-core.git cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git 替换Homebrew Bottles源 就是在~/.bashrc或者~/.bash_profile文件末尾加 export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles 这两个文件可以自己创建，~/.bashrc和~/.bash_profile都可以 切换回官方源： 重置brew.git cd \"$(brew --repo)\" git remote set-url origin https://github.com/Homebrew/brew.git 重置homebrew-core cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://github.com/Homebrew/homebrew-core.git 替换为清华源 brew清华源 参考资料 Homebrew国内源设置与常用命令 ChangeLog 20190603 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-08 17:11:27 "},"20190603_php_installation_on_macos.html":{"url":"20190603_php_installation_on_macos.html","title":"MacOS下安装PHP与nginx配置","keywords":"","body":"2019-06-03 | MacOS下安装PHP与nginx配置 MacOS系统自带PHP的，不过还是建议自己安装。 使用brew安装PHP # brew update # brew install php@7.2 ... ... To have launchd start php@7.2 now and restart at login: brew services start php@7.2 Or, if you don't want/need a background service you can just run: php-fpm 注：如何brew update很慢的话，可以参考MacOS的brew update很慢问题解决方案进行设置。 修改配置文件 执行命令： sudo cp /private/etc/php-fpm.conf.default /private/etc/php-fpm.conf sudo cp /private/etc/php-fpm.d/www.conf.default /private/etc/php-fpm.d/www.conf 找到/private/etc/目录下的 php-fpm 文件 /private/etc/php-fpm.conf 找到24行的 error_log ，改为（整行替换，注意 ‘;’ 和空格，就是要把‘;’也删除掉） error_log = /usr/local/var/log/php-fpm.log 否则 php-fpm 时会报错： ERROR: failed to open error_log (/usr/var/log/php-fpm.log): No such file or directory 启动PHP 如果希望后台自动启动，执行以下命令： brew services start php@7.2 如果只是本次执行，直接运行： php-fpm 配置Nginx运行php 打开nginx.conf ，在http块添加一行include servers/*.conf;。MacOS使用brew安装nginx的配置文件目录在/usr/local/etc/nginx/。 在/usr/local/etc/nginx/servesr下创建xxx.conf。我这里创建的是dokuwiki.conf 增加以下内容，尤其注意location ~ .+\\.php($|/)部分。 server { listen 8090; root /usr/local/Cellar_w/dokuwiki; server_name localhost; index index.php index.html doku.php; location ~ ^/(data|conf|bin|inc) { return 404; } location ~ ^/lib.*\\.(gif|png|ico|jpg)$ { expires 31d; } #location / { # try_files $uri $uri/ @dokuwiki; #} location @dokuwiki { rewrite ^/_media/(.*) /lib/exe/fetch.php?media=$1 last; rewrite ^/_detail/(.*) /lib/exe/detail.php?media=$1 last; rewrite ^/_export/([^/]+)/(.*) /doku.php?do=export_$1&id=$2 last; rewrite ^/tag/(.*) /doku.php?id=tag:$1&do=showtag&tag=tag:$1 last; rewrite ^/(.*) /doku.php?id=$1&$args last; } location ~ .+\\.php($|/) { include fastcgi_params; set $real_script_name $uri; set $path_info \"/\"; if ($fastcgi_script_name ~ \"^(.+\\.php)(/.+)$\") { set $real_script_name $1; set $path_info $2; } fastcgi_param SCRIPT_FILENAME $document_root$real_script_name; fastcgi_param SCRIPT_NAME $real_script_name; fastcgi_param PATH_INFO $path_info; fastcgi_pass 127.0.0.1:9000;#监听9000端口 fastcgi_index index.php; } } 重新启动nginx，下面两种命令都可以。 sudo brew services restart nginx /usr/local/bin/nginx -s reload 遇到的坑 问题一：Nginx代理PHP，报错Connection reset by peer nginx.log中错误信息如下： [error] 85581#0: *4 kevent() reported about an closed connection (54: Connection reset by peer) while reading response header from upstream, client: 127.0.0.1, server: localhost, request: \"GET / HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"localhost:8088\" 出错的原因是php-fpm未启动。 解决：sudo php-fpm 问题二、使用sudo php-fpm会报路径出错问题 可能是你配置路径那里没有修改，具体可以看上面的 修改配置文件章节。 问题二：No pool defined [root@localhost etc]# service php-fpm start Starting php-fpm [28-Nov-2016 17:13:23] WARNING: Nothing matches the include pattern ‘/usr/local/php/etc/php-fpm.d/*.conf’ from /usr/local/php/etc/php-fpm.conf at line 125. [28-Nov-2016 17:13:23] ERROR: No pool defined. at least one pool section must be specified in config file [28-Nov-2016 17:13:23] ERROR: failed to post process the configuration [28-Nov-2016 17:13:23] ERROR: FPM initialization failed 解决方法: 进入PHP安装目录/etc/php-fpm.d cp www.conf.default www.conf 参考资料 MACOS下安装PHP运行环境 Mac Nginx+php环境配置，看我就够了 在 macOS High Sierra 10.13 搭建 PHP 开发环境 php-fpm:No pool defined解决方法 ChangeLog 20190603 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-09 17:05:32 "}}