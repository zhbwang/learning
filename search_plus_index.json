{"./":{"url":"./","title":"前言","keywords":"","body":"前言 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-04-03 20:00:24 "},"20190606_how-to-build-your-own-knowledge-base.html":{"url":"20190606_how-to-build-your-own-knowledge-base.html","title":"如何建立自己的知识库","keywords":"","body":"如何建立自己的知识库 知识库建立原则 使用知识的三层模型构建自己的知识库：卡片、文件、项目。 知识库建立最终极的目的是生成一套知识体系，提升生成新知识的能力，完成知识创造的伟大历程。 知识库类型。 知识库包括三大类：工作类、学习类、生活类 工作类包括一切公司内部公共知识与产品文档、项目文档等。 学习类包括一切通过学习获取的知识，包括技术学习、阅读书籍、听书、浏览文章等。这些都是已有的形式知识，需要通过学习总结不断内化为自己的暗隐知识，从而生成新的知识。 生活类主要是记录生活的重要事情，比如：买车买房的过程、日记等。 知识库操作指南 知识库的建立少不了工具的支持，工具的选择必须要精简，减少工具使用过程中不断做选择的精力耗散。 收集过程：使用Evernote、滴答清单进行收集。使用WorkFlowy也可以进行快速记录。 卡片类：所有的卡片全部记录在WorkFlowy中。 文件类：文件的目的是为了分享，所以每一篇文章都需要具备成果意识。用Markdown来写，使用MWeb工具。 学习与生活类文件的组织逻辑：放在 gitbook.zhbwang.com 域名文件夹中。 文件层级尽量不超过两层。比如学习类文件全部放在一个文件夹 wiki@learning 中；生活类文件全部放在一个文件夹 wiki@life 中。这种方法有一个很大的好处，就是所有文章都在一个文件夹，能形成合力。 在文件夹中新建 drafts 文件夹，临时草稿区。根目录中就是已经定稿的。 工作类的文件暂时还没有想好。 gitbook.zhbwang.com 域名文件夹使用坚果云存储。 项目类：项目是一个成果，比如一本书，一个产品，或者一个完成的产品文档。如果说把所有资料都丢掉，只能保留很小的一部分，那么能保留的部分就是这个项目类的成果了。所以项目类的成果一定要精简，并且有结构。 在文件类，已经将一篇一篇的文章写好了，那么项目就要使用README或者SUMMARY文件驱动，将文章形成有结构的一本书，或者一个体系。 这部分也同样使用 GitBook 完成。 在另一个目录新建 GitBook 文件夹，将生活类文件夹、学习类文件夹建立一个软连接到 GitBook 文件夹，写个脚本，自动编译为 html，本机中的 nginx 代理，通过 http://localhost 即可访问。 可以使用 Fluid 将 http://localhost 网址转换为APP的形式，可以很方便的打开。 下一步思考：工作类文件应该如何处理？ 参考资料 ChangeLog 20190606 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 19:39:53 "},"20180603_introduction-of-markdown.html":{"url":"20180603_introduction-of-markdown.html","title":"初步认识Markdown","keywords":"","body":"初步认识markdown 为什么是Markdown 文本编辑比较 无格式文本：简洁、不依赖工具 富文本：美观，重点突出 Markdown与富文本编辑器异同： 作用一致： 使用者输入纯文字，通过编辑器的处理，使其拥有一份样式，最终得到带格式的文档。 使用区别： 富文本编辑器「所见即所得」 Markdown是一种标记语言，手动切换预览与编辑模式。 Markdown同时兼具无格式文本与富文本的优势。 什么是Markdown Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。 —— 维基百科 Markdown 是一种轻量级的「标记语言」，创始人为约翰·格鲁伯。Markdown 的创始人 John Gruber 这样定义： \"Markdown\" is two things: (1) a plain text formatting syntax; (2) a software tool, that converts the plain text formatting to others. 也就是说Markdown首先意味着是一套语法规则，其次代表了编辑器，把纯文本转换为排版效果的文字。 Markdown语法演进 CommonMark GFM(GitHub Flavored Markdown) 其他语法：PHP Markdown Extra、Maruku、kramdown、RDiscount、Redcarpet、MultiMarkdown 原有的 Markdown 语法的功能稍显不足，Github Flavored Markdown 在前面所说的语法的三个方面都做出了相应的增强。 比如： 标准Markdown要在一行的最后加两个空格符才表示换行，否则是不换行的；但是GFM则没有此要求。 支持把列表变成带勾选框的任务列表 在对段落的处理方面，对原有代码块进行了增强，可以制定不同的语言类型对代码进行语法高亮。 GFM的修改参考：GFM修改，GFM语法参考：GFM语法 语法特点： 用简洁的语法代替排版，其常用的标记符号不超过十个， 相对于更为复杂的 HTML 标记语言来说，Markdown 十分的轻量 一旦熟悉这种语法规则，会有沉浸式编辑的效果。 其他增强型Markdown语法：MultiMarkdown Markdown优势 书写过程流畅 富文本编辑器编辑文字时是两个不连续的动作，输入文字时双手放在键盘上，编辑文字则需要视线和手离开输入框和键盘，去寻找和点击功能按钮。 Markdown 的「书写流畅」就体现在将这两个动作合成一个输入字符的动作。 格式不随编辑器而改变，导出与分享方便 Markdown 格式保持的文件本质上仍是一份纯文本。 书写错误容易发现。 比如Word中，用空格、分页来控制排版，容易出错。而Markdown没有不需要考虑几个空格的问题，如果有几个词语想加粗，没有渲染成功，就说明写错了。 Markdown 的局限性 什么时候不该用 Markdown？ Markdown 无法对「段落」进行灵活处理。比如：文本位置 Markdown 对非纯文本元素的排版能力很差。比如：图文混排 Markdown一开始就定位为「文字输入工具」，不适合对排版格式自定义程度较高的文档进行排版。 Markdown适用场景 网络环境下的写作 利用了 Markdown 「写作即排版」的特点，Markdown 可以让使用者专心于文章书写，而非排版。 文档协作 团队成员间可以自由选用自己喜欢的操作系统和编辑器工具来进行写作，而不局限于 Word 或者 Google Docs等只支持富文本编辑的软件。 文档的展示方式不仅仅是在编辑器中，你可以随时把文档转换成网页，任何时候任何人都可以方便地查看。 利用它「纯文本格式」的优势，用 Markdown 来文档协作会比其他工具更自由。 基础语法 标题 markdown代码： # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 ### 文字格式 **This is bold text** *This text is italicized* ~~This was mistaken text~~ **This text is _extremely_ important** 区块引用 尼采说： Was mich nicht umbringt, macht mich stärker. markdown代码： #### 尼采说： > Was mich nicht umbringt, macht mich **stärker**. 列表 无序列表 George Washington John Adams Thomas Jefferson George Washington John Adams Thomas Jefferson 有序列表 James Madison James Monroe John Quincy Adams 嵌套列表 First list item First nested list item Second nested list item markdown代码： #### 无序列表 - George Washington - John Adams - Thomas Jefferson * George Washington * John Adams * Thomas Jefferson #### 有序列表 1. James Madison 2. James Monroe 3. John Quincy Adams #### 嵌套列表 1. First list item - First nested list item - Second nested list item 代码 行内代码 Use git status to list all new or modified files that haven't yet been committed. 代码块 @requires_authorization def somefunc(param1='', param2=0): '''A docstring''' if param1 > param2: # interesting print 'Greater' return (param2 - param1 + 1) or None class SomeClass: pass >>> message = '''interpreter ... prompt''' markdown代码： #### 行内代码 Use `git status` to list all new or modified files that haven't yet been committed. #### 代码块 ​```python @requires_authorization def somefunc(param1='', param2=0): '''A docstring''' if param1 > param2: # interesting print 'Greater' return (param2 - param1 + 1) or None class SomeClass: pass >>> message = '''interpreter ... prompt''' ​`` ` 分割线 这是分割线 这也是分割线 markdown代码： 这是分割线 --- 这也是分割线 *** 链接 网络链接 This site was built using GitHub Pages. This site was built using GitHub Pages. 相对链接 Test.md 图片链接 markdown代码： #### 网络链接 This site was built using [GitHub Pages](https://pages.github.com/). This site was built using [GitHub Pages][1]. #### 相对链接 [Test.md](./test.md) #### 图片链接 ![markdown](http://pic.iloc.cn/2019-06-05-I-love-markdown-syntax-language.png) 脚注 脚注demo 参考1 markdown代码： 脚注[^demo] 参考[^1] 高级语法 表格 Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 markdown代码： #### 表格 | Item | Value | Qty | | :------- | -------: | :--: | | Computer | 1600 USD | 5 | | Phone | 12 USD | 12 | | Pipe | 1 USD | 234 | 目录 [TOC] markdown代码： #### 目录 [TOC] 待办事项 使用 - [ ] 和 - [x] 语法可以创建复选框，实现 todo-list 等功能。例如： [x] 已完成事项 [ ] 待办事项1 [ ] 待办事项2 markdown代码： #### 待办事项 使用 `- [ ]` 和 `- [x]` 语法可以创建复选框，实现 todo-list 等功能。例如： - [x] 已完成事项 - [ ] 待办事项1 - [ ] 待办事项2 公式 $$E=mc^2$$ 可以创建行内公式，例如 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。或者块级公式： $$x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ markdown代码： #### 公式 $$E=mc^2$$ 可以创建行内公式，例如 $\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N$。或者块级公式： $$x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ 流程图 st=>start: Start op=>operation: Your Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op markdown代码： #### 流程图 ​```flow st=>start: Start op=>operation: Your Operation cond=>condition: Yes or No? e=>end st->op->cond cond(yes)->e cond(no)->op ​ ### 序列图 ```sequence Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks! markdown代码： #### 序列图 ​```sequence Alice->Bob: Hello Bob, how are you? Note right of Bob: Bob thinks Bob-->Alice: I am good thanks! ​ ``` 工具推荐 MWeb（推荐，收费，Mac/IOS） Typora（推荐，免费。全平台） 特点： WYSIWYG（What You See Is What You Get） 表格编辑功能增强 插入图片 代码和数学公式输入 支持显示目录大纲 下载：https://www.typora.io/ 介绍：https://sspai.com/post/30292 参考资料 印象笔记 Markdown 入门指南：https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php Markdown 完全入门（上）：https://sspai.com/post/36610 Markdown 完全入门（下）：https://sspai.com/post/36682 Markdown教程：http://www.markdown.cn/ 创始人 John Gruber 的 Markdown 语法说明：https://daringfireball.net/projects/markdown/syntax Github Flavored Markdown语法：https://help.github.com/articles/basic-writing-and-formatting-syntax/ 印象笔记 Markdown 入门指南：https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php Markdown简易入门教程：https://blog.huihut.com/2017/01/25/MarkdownTutorial/ Typora介绍：https://sspai.com/post/30292 ChangeLog 20190606 | 推荐工具增加MWeb，去掉小书匠、有道云笔记等。 20190606 | 从Evernote转移到GitBook 20180603 | 完成第一版，并在小组内分享 demo. 这是一个脚注。 ↩ 1. 这也是一个脚注。 ↩ Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 19:50:22 "},"20190214_git-code-transfer-to-gitee.html":{"url":"20190214_git-code-transfer-to-gitee.html","title":"Git代码转移到码云相关命令","keywords":"","body":"Git代码转移到码云相关命令 查看当前用户（global）配置 git config --global --list 生成公钥 ssh-keygen -t rsa -C \"youremail@example.com\" 查看本地代码的远程仓库地址 git remote -v 校验本机是否能连接gitee ssh -T git@gitee.com 重新设置远程仓库地址 git remote set-url origin git@gitee.com:xxx/xxxx.git OR git remote set-url origin https://gitee.com/xxxx/xxxx.git git remote -v 第一次pull出现问题的处理方法 git pull --allow-unrelated-histories 代码的换行符与windows不一致的情况，处理方法： git config --global core.autocrlf input ChangeLog 20190606 | 从Evernote迁移到GitBook 20190214 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:27 "},"20190515_let-hadoop-run.html":{"url":"20190515_let-hadoop-run.html","title":"让Hadoop在MacOS上跑起来","keywords":"","body":"让Hadoop在MacOS上跑起来 大数据学习之路系列01 已发布博客：腾讯云社区、CSDN博客、语雀。 本安装文档是在MacOS中安装单机版Hadoop。 安装目录 WZB-MacBook:50_bigdata wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata 准备工作 JDK Mac安装JDK的过程略，参考：MAC下安装多版本JDK和切换几种方式 WZB-MacBook:50_bigdata wangzhibin$ java -version java version \"1.7.0_80\" Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) WZB-MacBook:50_bigdata wangzhibin$ echo $JAVA_HOME /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 下载Hadoop brew install wget wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/core/hadoop-2.8.4/hadoop-2.8.4.tar.gz WZB-MacBook:50_bigdata wangzhibin$ tar -zxvf hadoop-2.8.4.tar.gz 安装与配置Hadoop 修改JDK配置 WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hadoop-env.sh export JAVA_HOME=${JAVA_HOME}改为 export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 验证Hadoop WZB-MacBook:hadoop-2.8.4 wangzhibin$ bin/hadoop Usage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar run a jar file note: please use \"yarn jar\" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive classpath prints the class path needed to get the credential interact with credential providers Hadoop jar and the required libraries daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settings Most commands print help when invoked w/o parameters. 单机模式执行 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar grep input output 'dfs[a-z.]+' $ cat output/* 1 dfsadmin 配置core-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ mkdir -p hdfs/tmp WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/core-site.xml 增加如下配置： hadoop.tmp.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp Abase for other temporary directories. fs.defaultFS hdfs://localhost:9000 配置hdfs-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hdfs-site.xml 增加如下配置： dfs.replication 1 dfs.namenode.name.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name dfs.datanode.data.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/data 启动与停止Hadoop 配置.bash_profile # set hadoop export HADOOP_HOME=/Users/wangzhibin/00_dev_suite/50_bigdata/hadoop export PATH=$PATH:$HADOOP_HOME/bin 第一次启动hdfs需要格式化 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./bin/hdfs namenode -format ... 19/05/15 22:30:47 INFO common.Storage: Storage directory /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name has been successfully formatted. ... 启动HDFS ./sbin/start-dfs.sh 停止HDFS ./sbin/stop-dfs.sh HDFS启动状态查看 HDFS 状态：http://localhost:50070/dfshealth.html#tab-overview Secordary NameNode 状态：http://localhost:50090/status.html 本地官方文档：API文档 验证HDFS 简单的验证hadoop命令： $ hadoop fs -mkdir /test WZB-MacBook:hadoop wangzhibin$ hadoop fs -ls / Found 1 items drwxr-xr-x - wangzhibin supergroup 0 2019-05-16 11:26 /test 启动时遇到的坑 一、sh: connect to host localhost port 22: Connection refused 此时可能会出现如下错误。是因为没有配置ssh免密登录。 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:38:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: ssh: connect to host localhost port 22: Connection refused localhost: ssh: connect to host localhost port 22: Connection refused Starting secondary namenodes [0.0.0.0] 0.0.0.0: ssh: connect to host 0.0.0.0 port 22: Connection refused 19/05/15 22:38:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 采用如下方法解决： 1）解决方法是选择系统偏好设置->选择共享->点击远程登录 2）设置免密登录 $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys $ ssh localhost 二、Unable to load native-hadoop library for your platform WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:50:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: starting namenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-namenode-WZB-MacBook.local.out localhost: starting datanode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-datanode-WZB-MacBook.local.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out 19/05/15 22:50:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 参考： 官方-Native Libraries Guide Mac OSX 下 Hadoop 使用本地库提高效率 Hadoop native libraries: Installation on Mac Osx 解决方案：重新编译hadoop，将编译后的hadoop-dist/target/hadoop-2.8.4/lib/native替换$HADOOP_HOME/lib/native。 安装基础组件 $ brew install gcc autoconf automake libtool cmake snappy gzip bzip2 zlib 安装protobuf。 wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz tar zxvf protobuf-2.5.0.tar.gz cd protobuf-2.5.0 ./configure make make install 重新编译hadoop wget http://apache.fayea.com/hadoop/common/hadoop-2.8.4/hadoop-2.8.4-src.tar.gz tar zxvf hadoop-2.8.4-src.tar.gz cd hadoop-2.8.4-src mvn package -Pdist,native -DskipTests -Dtar -e cp -r /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-dist/target/hadoop-2.8.4/lib/native . 三、An Ant BuildException has occured: exec returned WZB-MacBook:hadoop-2.8.4-src wangzhibin$ mvn package -Pdist,native -DskipTests -Dtar -e ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 [ERROR] around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml [ERROR] -> [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml 参考：mac下编译Hadoop 2.8.1报错An Ant BuildException has occured: exec returned: 1，排错过程 解决方案：配置环境变量OPENSSL_ROOT_DIR、OPENSSL_INCLUDE_DIR。修改~/.bash_profile # openssl export OPENSSL_ROOT_DIR=/usr/local/Cellar/openssl/1.0.2r export OPENSSL_INCLUDE_DIR=$OPENSSL_ROOT_DIR/include 配置与启动yarn 配置mapred-site.xml cd $HADOOP_HOME/etc/hadoop/ cp mapred-site.xml.template mapred-site.xml vim mapred-site.xml mapreduce.framework.name yarn 配置yarn-site.xml vim yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle yarn启动与停止 启动 cd $HADOOP_HOME ./sbin/start-yarn.sh ./sbin/stop-yarn.sh 浏览器查看：http://localhost:8088 jps查看进程 WZB-MacBook:hadoop wangzhibin$ jps 534 NutstoreGUI 49135 DataNode 49834 ResourceManager 49234 SecondaryNameNode 49973 Jps 67596 49912 NodeManager 49057 NameNode 到此，hadoop单机模式就配置成功了！ 命令与验证 Resource Manager: http://localhost:50070 JobTracker: http://localhost:8088/ Node Specific Info: http://localhost:8042/ Command $ jps $ yarn // For resource management more information than the web interface. $ mapred // Detailed information about jobs 参考资料 Hadoop: Setting up a Single Node Cluster. centos7 hadoop 单机模式安装配置 Hadoop in OSX El-Capitan Installing Hadoop on Mac OS X 10.9.4 macOS上搭建伪分布式Hadoop环境 本地官方API文档 ChangeLog 20190515 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:32 "},"20190517_the-first-mapreduce-program.html":{"url":"20190517_the-first-mapreduce-program.html","title":"第一个MapReduce程序","keywords":"","body":"第一个MapReduce程序 2019-05-17 | 大数据学习之路系列02 已发布博客：腾讯云社区、CSDN博客、语雀。 目标 单词计数是最简单也是最能体现 MapReduce 思想的程序之一，可以称为 MapReduce 版“Hello World”。 单词计数主要完成功能是：统计一系列文本文件中每个单词出现的次数，如下图所示。 准备工作 新建目录 WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/input WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/output WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/input drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 准备文件 WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file1.txt Hello World WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file2.txt Hello Hadoop WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 14:50 /practice/20190517_mr/input -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 14:49 /practice/20190517_mr/input/file1.txt -rw-r--r-- 1 wangzhibin supergroup 13 2019-05-17 14:49 /practice/20190517_mr/input/file2.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 运行例子 在集群上运行 WordCount 程序 备注:以 input 作为输入目录，output 目录作为输出目录。 已经编译好的 WordCount 的 Jar 在“$HADOOP_HOME/share/hadoop/mapreduce/”下面，就是“hadoop-mapreduce-examples-2.8.4.jar”， MapReduce 执行过程显示信息 执行命令： hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 执行过程： WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:39:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:39:20 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558078701666_0002 19/05/17 15:39:20 INFO impl.YarnClientImpl: Submitted application application_1558078701666_0002 19/05/17 15:39:20 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1558078701666_0002/ 19/05/17 15:39:20 INFO mapreduce.Job: Running job: job_1558078701666_0002 19/05/17 15:39:28 INFO mapreduce.Job: Job job_1558078701666_0002 running in uber mode : false 19/05/17 15:39:28 INFO mapreduce.Job: map 0% reduce 0% 19/05/17 15:39:33 INFO mapreduce.Job: map 100% reduce 0% 19/05/17 15:39:39 INFO mapreduce.Job: map 100% reduce 100% 19/05/17 15:39:39 INFO mapreduce.Job: Job job_1558078701666_0002 completed successfully 19/05/17 15:39:39 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=55 FILE: Number of bytes written=474472 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=271 HDFS: Number of bytes written=25 HDFS: Number of read operations=9 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=2 Launched reduce tasks=1 Data-local map tasks=2 Total time spent by all maps in occupied slots (ms)=6213 Total time spent by all reduces in occupied slots (ms)=2848 Total time spent by all map tasks (ms)=6213 Total time spent by all reduce tasks (ms)=2848 Total vcore-milliseconds taken by all map tasks=6213 Total vcore-milliseconds taken by all reduce tasks=2848 Total megabyte-milliseconds taken by all map tasks=6362112 Total megabyte-milliseconds taken by all reduce tasks=2916352 Map-Reduce Framework Map input records=2 Map output records=4 Map output bytes=41 Map output materialized bytes=61 Input split bytes=246 Combine input records=4 Combine output records=4 Reduce input groups=3 Reduce shuffle bytes=61 Reduce input records=4 Reduce output records=3 Spilled Records=8 Shuffled Maps =2 Failed Shuffles=0 Merged Map outputs=2 GC time elapsed (ms)=97 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=603979776 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=25 File Output Format Counters Bytes Written=25 查看结果 WZB-MacBook:hadoop wangzhibin$ hadoop dfs -ls -R /practice/20190517_mr/output/ -rw-r--r-- 1 wangzhibin supergroup 0 2019-05-17 15:39 /practice/20190517_mr/output/_SUCCESS -rw-r--r-- 1 wangzhibin supergroup 25 2019-05-17 15:39 /practice/20190517_mr/output/part-r-00000 WZB-MacBook:hadoop wangzhibin$ hadoop fs -cat /practice/20190517_mr/output/* Hadoop 1 Hello 2 World 1 遇到的坑 问题一：执行到Running job: job_1557977819409_0004的地方就不往下执行了。 WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoopoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:01:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:01:03 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:01:03 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:01:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557977819409_0004 19/05/17 15:01:04 INFO impl.YarnClientImpl: Submitted application application_1557977819409_0004 19/05/17 15:01:04 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1557977819409_0004/ 19/05/17 15:01:04 INFO mapreduce.Job: Running job: job_1557977819409_0004 参考： Hadoop相关总结 Hadoop 运行wordcount任务卡在job running的一种解决办法 hadoop2.7.x运行wordcount程序卡住在INFO mapreduce.Job: Running job:job _1469603958907_0002 Can't run a MapReduce job on hadoop 2.4.0 解决方案：在$HADOOP_HOME/etc/hadoop/yarn-site.xml中增加配置。 yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 最终的yarn-site.xml如下： yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce.shuffle.class org.apache.hadoop.mapred.ShuffleHandler yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 重启yarn： ./sbin/stop-yarn.sh ./sbin/start-yarn.sh ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:02:47 "},"20190517_detailed-hadoop-commands.html":{"url":"20190517_detailed-hadoop-commands.html","title":"Hadoop常用命令详解","keywords":"","body":"Hadoop常用命令详解 大数据学习之路03 已发布博客：腾讯云社区、CSDN博客、语雀。 Hadoop基本命令 version 查看Hadoop版本。 WZB-MacBook:target wangzhibin$ hadoop version Hadoop 2.8.4 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674 Compiled by jdu on 2018-05-08T02:50Z Compiled with protoc 2.5.0 From source with checksum b02a59bb17646783210e979bea443b0 This command was run using /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/share/hadoop/common/hadoop-common-2.8.4.jar HDFS基础命令 命令格式 hadoop fs -cmd ls 列出hdfs文件系统根目录下的目录和文件 hadoop fs -ls / 列出hdfs文件系统所有的目录和文件 hadoop fs -ls -R / mkdir 一级一级的建目录，父目录不存在的话使用这个命令会报错 command: hadoop fs -mkdir eg: WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir /test/20190517 WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 所创建的目录如果父目录不存在就创建该父目录 hadoop fs -mkdir -p put 上传文件。hdfs file的父目录一定要存在，否则命令不会执行 command: hadoop fs -put eg: $ hadoop fs -put tmp/tmp.txt /test $ hadoop fs -ls -R /test 上传目录。hdfs dir 一定要存在，否则命令不会执行 command: hadoop fs -put ... eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put tmp/ /test WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 从键盘读取输入到hdfs file中，按Ctrl+D（Control+D）结束输入。hdfs file不能存在，否则命令不会执行 command: hadoop fs -put - eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put - /test/20190517.tmp.txt hello world WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 10:44 /test/20190517.tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cat 在标准输出中显示文件内容 command: hadoop fs -cat eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -cat /test/20190517.tmp.txt hello world tail 在标准输出中显示文件末尾的1KB数据 command: hadoop fs -tail eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -tail /test/20190517.tmp.txt hello world get 从hdfs中下载文件到本地。local file不能和 hdfs file名字不能相同，否则会提示文件已存在，没有重名的文件会复制到本地 command: hadoop fs -get eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test/20190517.tmp.txt . WZB-MacBook:tmp wangzhibin$ ls 20190517.tmp.txt tmp.txt 拷贝多个文件或目录到本地时，本地要为文件夹路径 command: hadoop fs -get ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test . WZB-MacBook:tmp wangzhibin$ ls -l test/ total 24 drwxr-xr-x 2 wangzhibin staff 64 5 17 10:56 20190517 -rw-r--r-- 1 wangzhibin staff 12 5 17 10:56 20190517.tmp.txt drwxr-xr-x 3 wangzhibin staff 96 5 17 10:56 tmp -rw-r--r-- 1 wangzhibin staff 4662 5 17 10:56 tmp.txt 注意：如果用户不是root， local 路径要为用户文件夹下的路径，否则会出现权限问题。 rm 每次可以删除多个文件或目录 command: hadoop fs -rm ... hadoop fs -rm -r ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -rm /test/20190517.tmp.txt Deleted /test/20190517.tmp.txt WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cp HDFS拷贝文件或文件夹。目标文件或者文件夹不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件还存在。 command: hadoop fs -cp hadoop fs -cp ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -cp /test/tmp.txt /test/20190517/ WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:03 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt mv HDFS移动文件或文件夹。目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件不存在。源路径有多个时，目标路径必须为目录，且必须存在。 command: hadoop fs -mv hadoop fs -mv ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -mv /test/20190517/tmp.20190519.txt /test/tmp WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:04 /test/tmp/tmp.20190519.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 注意：跨文件系统的移动（local到hdfs或者反过来）都是不允许的 count 统计hdfs对应路径下的目录个数，文件个数，文件总计大小 显示为目录个数，文件个数，文件总计大小，输入路径 command: hadoop fs -count eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -count /test 3 4 18648 /test du 显示hdfs对应路径下每个文件夹和文件的大小 command: hadoop fs -du eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du /test 4662 /test/20190517 9324 /test/tmp 4662 /test/tmp.txt 显示hdfs对应路径下所有文件和的大小 command: hadoop fs -du -s eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -s /test 18648 /test 显示hdfs对应路径下每个文件夹和文件的大小,文件的大小用方便阅读的形式表示，例如用64M代替67108864 command: hadoop fs -du -h eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -h /test 4.6 K /test/20190517 9.1 K /test/tmp 4.6 K /test/tmp.txt HDFS高级命令 以下命令参考：hadoop HDFS常用文件操作命令。没有实践。 moveFromLocal hadoop fs -moveFromLocal ... 与put相类似，命令执行后源文件 local src 被删除，也可以从从键盘读取输入到hdfs file中 copyFromLocal hadoop fs -copyFromLocal ... 与put相类似，也可以从从键盘读取输入到hdfs file中 moveToLocal 当前版本中还未实现此命令 copyToLocal hadoop fs -copyToLocal ... 与get相类似 getmerge hadoop fs -getmerge 将hdfs指定目录下所有文件排序后合并到local指定的文件中，文件不存在时会自动创建，文件存在时会覆盖里面的内容 hadoop fs -getmerge -nl 加上nl后，合并到local file中的hdfs文件之间会空出一行 text hadoop fs -text 将文本文件或某些格式的非文本文件通过文本格式输出 setrep hadoop fs -setrep -R 3 改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作 stat hdoop fs -stat [format] 返回对应路径的状态信息 [format]可选参数有：%b（文件大小），%o（Block大小），%n（文件名），%r（副本个数），%y（最后一次修改日期和时间） 可以这样书写hadoop fs -stat %b%o%n ，不过不建议，这样每个字符输出的结果不是太容易分清楚 archive hadoop archive -archiveName name.har -p * 命令中参数name：压缩文件名，自己任意取； ：压缩文件所在的父目录；：要压缩的文件名；：压缩文件存放路径\\示例：hadoop archive -archiveName hadoop.har -p /user 1.txt 2.txt /des* 示例中将hdfs中/user目录下的文件1.txt，2.txt压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下，如果1.txt，2.txt不写就是将/user目录下所有的目录和文件压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下 显示har的内容可以用如下命令： hadoop fs -ls /des/hadoop.jar 显示har压缩的是那些文件可以用如下命令 hadoop fs -ls -R har:///des/hadoop.har 注意：har文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少NameNode和DataNode过多的空间浪费。 balancer hdfs balancer 如果管理员发现某些DataNode保存数据过多，某些DataNode保存数据相对较少，可以使用上述命令手动启动内部的均衡过程 dfsadmin hdfs dfsadmin -help 管理员可以通过dfsadmin管理HDFS，用法可以通过上述命令查看 hdfs dfsadmin -report 显示文件系统的基本数据 hdfs dfsadmin -safemode enter：进入安全模式；leave：离开安全模式；get：获知是否开启安全模式； wait：等待离开安全模式 distcp 用来在两个HDFS之间拷贝数据 MapReduce命令 命令帮助 WZB-MacBook:target wangzhibin$ mapred -help Usage: mapred [--config confdir] [--loglevel loglevel] COMMAND where COMMAND is one of: pipes run a Pipes job job manipulate MapReduce jobs queue get information regarding JobQueues classpath prints the class path needed for running mapreduce subcommands historyserver run job history servers as a standalone daemon distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive archive-logs combine aggregated logs into hadoop archives hsadmin job history server admin interface Most commands print help when invoked w/o parameters. 列出所有任务 WZB-MacBook:target wangzhibin$ mapred job -list all 19/05/20 10:22:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Total jobs:1 JobId State StartTime UserName Queue Priority UsedContainers RsvdContainers UsedMem RsvdMem NeededMem AM info job_1558104288185_0001 SUCCEEDED 1558104322342 wangzhibin default DEFAULT N/A N/A N/A N/A N/A http://WZB-MacBook.local:8088/proxy/application_1558104288185_0001/ 强制停止任务 mapred job -kill 参考资料 1.0.4版本官方文档-Hadoop Shell命令 hadoop HDFS常用文件操作命令 大数据基本组件（Hadoop、HDFS、MapRed、YARN）入门命令 ChangeLog 20190517 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:14 "},"20180712_commands-of-linux.html":{"url":"20180712_commands-of-linux.html","title":"Linux常用命令","keywords":"","body":"技术 | Linux常用命令 全文检索 find / -print -type f | xargs grep '/var/lib/psql/data' 查看端口占用情况 lsof -i:端口号 netstat -tunlp|grep 端口号 这两种方式都可以查看指定端口被哪个进程占用的情况。 Linux 查看CPU信息，机器型号，内存等信息 系统 # uname -a # 查看内核/操作系统/CPU信息 # lsb_release -a # 查看操作系统版本 (适用于所有的linux，包括Redhat、SuSE、Debian等发行版，但是在debian下要安装lsb) # cat /proc/cpuinfo # 查看CPU信息 # hostname # 查看计算机名 # lspci -tv # 列出所有PCI设备 # lsusb -tv # 列出所有USB设备 # lsmod # 列出加载的内核模块 # env # 查看环境变量 资源 # free -m # 查看内存使用量和交换区使用量 # df -h # 查看各分区使用情况 # du -sh # 查看指定目录的大小 # grep MemTotal /proc/meminfo # 查看内存总量 # grep MemFree /proc/meminfo # 查看空闲内存量 # uptime # 查看系统运行时间、用户数、负载 # cat /proc/loadavg # 查看系统负载 磁盘和分区 # mount | column -t # 查看挂接的分区状态 # fdisk -l # 查看所有分区 # swapon -s # 查看所有交换分区 # hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) # dmesg | grep IDE # 查看启动时IDE设备检测状况 网络 # ifconfig # 查看所有网络接口的属性 # iptables -L # 查看防火墙设置 # route -n # 查看路由表 # netstat -lntp # 查看所有监听端口 # netstat -antp # 查看所有已经建立的连接 # netstat -s # 查看网络统计信息 进程 # ps -ef # 查看所有进程 # top # 实时显示进程状态 # jps # 查看java进程 用户 # w # 查看活动用户 # id # 查看指定用户信息 # last # 查看用户登录日志 # cut -d: -f1 /etc/passwd # 查看系统所有用户 # cut -d: -f1 /etc/group # 查看系统所有组 # crontab -l # 查看当前用户的计划任务 服务 # chkconfig --list # 列出所有系统服务 # chkconfig --list | grep on # 列出所有启动的系统服务 程序 # rpm -qa # 查看所有安装的软件包 查看CPU信息（型号） # cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 8 Intel(R) Xeon(R) CPU E5410 @ 2.33GHz (看到有8个逻辑CPU, 也知道了CPU型号) # cat /proc/cpuinfo | grep physical | uniq -c 4 physical id : 0 4 physical id : 1 (说明实际上是两颗4核的CPU) # getconf LONG_BIT 32 (说明当前CPU运行在32bit模式下, 但不代表CPU不支持64bit) # cat /proc/cpuinfo | grep flags | grep ' lm ' | wc -l 8 (结果大于0, 说明支持64bit计算. lm指long mode, 支持lm则是64bit) 再完整看cpu详细信息, 不过大部分我们都不关心而已. # dmidecode | grep 'Processor Information' 查看内 存信息 # cat /proc/meminfo # uname -a Linux euis1 2.6.9-55.ELsmp #1 SMP Fri Apr 20 17:03:35 EDT 2007 i686 i686 i386 GNU/Linux (查看当前操作系统内核信息) # cat /etc/issue | grep Linux Red Hat Enterprise Linux AS release 4 (Nahant Update 5) (查看当前操作系统发行版信息) 查看机器型号 # dmidecode | grep \"Product Name\" 查看网卡信息 # dmesg | grep -i eth ChangeLog 20190605 | 增加「查看端口占用情况」 20180712 | 创建文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-06 18:03:27 "}}