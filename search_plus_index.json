{"./":{"url":"./","title":"前言","keywords":"","body":"前言 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-04-03 20:00:24 "},"20190515_let-hadoop-run.html":{"url":"20190515_let-hadoop-run.html","title":"让Hadoop在MacOS上跑起来","keywords":"","body":"让Hadoop在MacOS上跑起来 2019-05-15 | 大数据学习之路系列01 已发布博客：腾讯云社区、CSDN博客、语雀。 本安装文档是在MacOS中安装单机版Hadoop。 安装目录 WZB-MacBook:50_bigdata wangzhibin$ pwd /Users/wangzhibin/00_dev_suite/50_bigdata 准备工作 JDK Mac安装JDK的过程略，参考：MAC下安装多版本JDK和切换几种方式 WZB-MacBook:50_bigdata wangzhibin$ java -version java version \"1.7.0_80\" Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) WZB-MacBook:50_bigdata wangzhibin$ echo $JAVA_HOME /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 下载Hadoop brew install wget wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/core/hadoop-2.8.4/hadoop-2.8.4.tar.gz WZB-MacBook:50_bigdata wangzhibin$ tar -zxvf hadoop-2.8.4.tar.gz 安装与配置Hadoop 修改JDK配置 WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hadoop-env.sh export JAVA_HOME=${JAVA_HOME}改为 export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 验证Hadoop WZB-MacBook:hadoop-2.8.4 wangzhibin$ bin/hadoop Usage: hadoop [--config confdir] [COMMAND | CLASSNAME] CLASSNAME run the class named CLASSNAME or where COMMAND is one of: fs run a generic filesystem user client version print the version jar run a jar file note: please use \"yarn jar\" to launch YARN applications, not this command. checknative [-a|-h] check native hadoop and compression libraries availability distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive classpath prints the class path needed to get the credential interact with credential providers Hadoop jar and the required libraries daemonlog get/set the log level for each daemon trace view and modify Hadoop tracing settings Most commands print help when invoked w/o parameters. 单机模式执行 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar grep input output 'dfs[a-z.]+' $ cat output/* 1 dfsadmin 配置core-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ mkdir -p hdfs/tmp WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/core-site.xml 增加如下配置： hadoop.tmp.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp Abase for other temporary directories. fs.defaultFS hdfs://localhost:9000 配置hdfs-site.xml WZB-MacBook:hadoop-2.8.4 wangzhibin$ vi etc/hadoop/hdfs-site.xml 增加如下配置： dfs.replication 1 dfs.namenode.name.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name dfs.datanode.data.dir /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/data 启动与停止Hadoop 配置.bash_profile # set hadoop export HADOOP_HOME=/Users/wangzhibin/00_dev_suite/50_bigdata/hadoop export PATH=$PATH:$HADOOP_HOME/bin 第一次启动hdfs需要格式化 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./bin/hdfs namenode -format ... 19/05/15 22:30:47 INFO common.Storage: Storage directory /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/hdfs/tmp/dfs/name has been successfully formatted. ... 启动HDFS ./sbin/start-dfs.sh 停止HDFS ./sbin/stop-dfs.sh HDFS启动状态查看 HDFS 状态：http://localhost:50070/dfshealth.html#tab-overview Secordary NameNode 状态：http://localhost:50090/status.html 本地官方文档：API文档 验证HDFS 简单的验证hadoop命令： $ hadoop fs -mkdir /test WZB-MacBook:hadoop wangzhibin$ hadoop fs -ls / Found 1 items drwxr-xr-x - wangzhibin supergroup 0 2019-05-16 11:26 /test 启动时遇到的坑 一、sh: connect to host localhost port 22: Connection refused 此时可能会出现如下错误。是因为没有配置ssh免密登录。 WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:38:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: ssh: connect to host localhost port 22: Connection refused localhost: ssh: connect to host localhost port 22: Connection refused Starting secondary namenodes [0.0.0.0] 0.0.0.0: ssh: connect to host 0.0.0.0 port 22: Connection refused 19/05/15 22:38:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 采用如下方法解决： 1）解决方法是选择系统偏好设置->选择共享->点击远程登录 2）设置免密登录 $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys $ ssh localhost 二、Unable to load native-hadoop library for your platform WZB-MacBook:hadoop-2.8.4 wangzhibin$ ./sbin/start-dfs.sh 19/05/15 22:50:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [localhost] localhost: starting namenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-namenode-WZB-MacBook.local.out localhost: starting datanode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-datanode-WZB-MacBook.local.out Starting secondary namenodes [0.0.0.0] 0.0.0.0: starting secondarynamenode, logging to /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/logs/hadoop-wangzhibin-secondarynamenode-WZB-MacBook.local.out 19/05/15 22:50:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 参考： 官方-Native Libraries Guide Mac OSX 下 Hadoop 使用本地库提高效率 Hadoop native libraries: Installation on Mac Osx 解决方案：重新编译hadoop，将编译后的hadoop-dist/target/hadoop-2.8.4/lib/native替换$HADOOP_HOME/lib/native。 安装基础组件 $ brew install gcc autoconf automake libtool cmake snappy gzip bzip2 zlib 安装protobuf。 wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz tar zxvf protobuf-2.5.0.tar.gz cd protobuf-2.5.0 ./configure make make install 重新编译hadoop wget http://apache.fayea.com/hadoop/common/hadoop-2.8.4/hadoop-2.8.4-src.tar.gz tar zxvf hadoop-2.8.4-src.tar.gz cd hadoop-2.8.4-src mvn package -Pdist,native -DskipTests -Dtar -e cp -r /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-dist/target/hadoop-2.8.4/lib/native . 三、An Ant BuildException has occured: exec returned WZB-MacBook:hadoop-2.8.4-src wangzhibin$ mvn package -Pdist,native -DskipTests -Dtar -e ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 [ERROR] around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml [ERROR] -> [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-pipes: An Ant BuildException has occured: exec returned: 1 around Ant part ...... @ 5:152 in /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4-src/hadoop-tools/hadoop-pipes/target/antrun/build-main.xml 参考：mac下编译Hadoop 2.8.1报错An Ant BuildException has occured: exec returned: 1，排错过程 解决方案：配置环境变量OPENSSL_ROOT_DIR、OPENSSL_INCLUDE_DIR。修改~/.bash_profile # openssl export OPENSSL_ROOT_DIR=/usr/local/Cellar/openssl/1.0.2r export OPENSSL_INCLUDE_DIR=$OPENSSL_ROOT_DIR/include 配置与启动yarn 配置mapred-site.xml cd $HADOOP_HOME/etc/hadoop/ cp mapred-site.xml.template mapred-site.xml vim mapred-site.xml mapreduce.framework.name yarn 配置yarn-site.xml vim yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle yarn启动与停止 启动 cd $HADOOP_HOME ./sbin/start-yarn.sh ./sbin/stop-yarn.sh 浏览器查看：http://localhost:8088 jps查看进程 WZB-MacBook:hadoop wangzhibin$ jps 534 NutstoreGUI 49135 DataNode 49834 ResourceManager 49234 SecondaryNameNode 49973 Jps 67596 49912 NodeManager 49057 NameNode 到此，hadoop单机模式就配置成功了！ 命令与验证 Resource Manager: http://localhost:50070 JobTracker: http://localhost:8088/ Node Specific Info: http://localhost:8042/ Command $ jps $ yarn // For resource management more information than the web interface. $ mapred // Detailed information about jobs 参考资料 Hadoop: Setting up a Single Node Cluster. centos7 hadoop 单机模式安装配置 Hadoop in OSX El-Capitan Installing Hadoop on Mac OS X 10.9.4 macOS上搭建伪分布式Hadoop环境 本地官方API文档 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-05 22:00:51 "},"20190517_the-first-mapreduce-program.html":{"url":"20190517_the-first-mapreduce-program.html","title":"第一个MapReduce程序","keywords":"","body":"第一个MapReduce程序 2019-05-17 | 大数据学习之路系列02 已发布博客：腾讯云社区、CSDN博客、语雀。 目标 单词计数是最简单也是最能体现 MapReduce 思想的程序之一，可以称为 MapReduce 版“Hello World”。 单词计数主要完成功能是：统计一系列文本文件中每个单词出现的次数，如下图所示。 准备工作 新建目录 WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/input WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir -p /practice/20190517_mr/output WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/input drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 准备文件 WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file1.txt Hello World WZB-MacBook:~ wangzhibin$ hadoop fs -put - /practice/20190517_mr/input/file2.txt Hello Hadoop WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /practice/20190517_mr drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 14:50 /practice/20190517_mr/input -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 14:49 /practice/20190517_mr/input/file1.txt -rw-r--r-- 1 wangzhibin supergroup 13 2019-05-17 14:49 /practice/20190517_mr/input/file2.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 13:53 /practice/20190517_mr/output 运行例子 在集群上运行 WordCount 程序 备注:以 input 作为输入目录，output 目录作为输出目录。 已经编译好的 WordCount 的 Jar 在“$HADOOP_HOME/share/hadoop/mapreduce/”下面，就是“hadoop-mapreduce-examples-2.8.4.jar”， MapReduce 执行过程显示信息 执行命令： hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 执行过程： WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:39:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:39:20 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:39:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1558078701666_0002 19/05/17 15:39:20 INFO impl.YarnClientImpl: Submitted application application_1558078701666_0002 19/05/17 15:39:20 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1558078701666_0002/ 19/05/17 15:39:20 INFO mapreduce.Job: Running job: job_1558078701666_0002 19/05/17 15:39:28 INFO mapreduce.Job: Job job_1558078701666_0002 running in uber mode : false 19/05/17 15:39:28 INFO mapreduce.Job: map 0% reduce 0% 19/05/17 15:39:33 INFO mapreduce.Job: map 100% reduce 0% 19/05/17 15:39:39 INFO mapreduce.Job: map 100% reduce 100% 19/05/17 15:39:39 INFO mapreduce.Job: Job job_1558078701666_0002 completed successfully 19/05/17 15:39:39 INFO mapreduce.Job: Counters: 49 File System Counters FILE: Number of bytes read=55 FILE: Number of bytes written=474472 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=271 HDFS: Number of bytes written=25 HDFS: Number of read operations=9 HDFS: Number of large read operations=0 HDFS: Number of write operations=2 Job Counters Launched map tasks=2 Launched reduce tasks=1 Data-local map tasks=2 Total time spent by all maps in occupied slots (ms)=6213 Total time spent by all reduces in occupied slots (ms)=2848 Total time spent by all map tasks (ms)=6213 Total time spent by all reduce tasks (ms)=2848 Total vcore-milliseconds taken by all map tasks=6213 Total vcore-milliseconds taken by all reduce tasks=2848 Total megabyte-milliseconds taken by all map tasks=6362112 Total megabyte-milliseconds taken by all reduce tasks=2916352 Map-Reduce Framework Map input records=2 Map output records=4 Map output bytes=41 Map output materialized bytes=61 Input split bytes=246 Combine input records=4 Combine output records=4 Reduce input groups=3 Reduce shuffle bytes=61 Reduce input records=4 Reduce output records=3 Spilled Records=8 Shuffled Maps =2 Failed Shuffles=0 Merged Map outputs=2 GC time elapsed (ms)=97 CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=0 Total committed heap usage (bytes)=603979776 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=25 File Output Format Counters Bytes Written=25 查看结果 WZB-MacBook:hadoop wangzhibin$ hadoop dfs -ls -R /practice/20190517_mr/output/ -rw-r--r-- 1 wangzhibin supergroup 0 2019-05-17 15:39 /practice/20190517_mr/output/_SUCCESS -rw-r--r-- 1 wangzhibin supergroup 25 2019-05-17 15:39 /practice/20190517_mr/output/part-r-00000 WZB-MacBook:hadoop wangzhibin$ hadoop fs -cat /practice/20190517_mr/output/* Hadoop 1 Hello 2 World 1 遇到的坑 问题一：执行到Running job: job_1557977819409_0004的地方就不往下执行了。 WZB-MacBook:hadoop wangzhibin$ hadoop jar /Users/wangzhibin/00_dev_suite/50_bigdata/hadoopoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /practice/20190517_mr/input /practice/20190517_mr/output 19/05/17 15:01:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 19/05/17 15:01:03 INFO input.FileInputFormat: Total input files to process : 2 19/05/17 15:01:03 INFO mapreduce.JobSubmitter: number of splits:2 19/05/17 15:01:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1557977819409_0004 19/05/17 15:01:04 INFO impl.YarnClientImpl: Submitted application application_1557977819409_0004 19/05/17 15:01:04 INFO mapreduce.Job: The url to track the job: http://WZB-MacBook.local:8088/proxy/application_1557977819409_0004/ 19/05/17 15:01:04 INFO mapreduce.Job: Running job: job_1557977819409_0004 参考： Hadoop相关总结 Hadoop 运行wordcount任务卡在job running的一种解决办法 hadoop2.7.x运行wordcount程序卡住在INFO mapreduce.Job: Running job:job _1469603958907_0002 Can't run a MapReduce job on hadoop 2.4.0 解决方案：在$HADOOP_HOME/etc/hadoop/yarn-site.xml中增加配置。 yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 最终的yarn-site.xml如下： yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.aux-services.mapreduce.shuffle.class org.apache.hadoop.mapred.ShuffleHandler yarn.nodemanager.disk-health-checker.min-healthy-disks 0.0 yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 100.0 重启yarn： ./sbin/stop-yarn.sh ./sbin/start-yarn.sh Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-05 21:59:28 "},"20190517_detailed-hadoop-commands.html":{"url":"20190517_detailed-hadoop-commands.html","title":"Hadoop常用命令详解","keywords":"","body":"Hadoop常用命令详解 2019-05-17 | 大数据学习之路03 已发布博客：腾讯云社区、CSDN博客、语雀。 Hadoop基本命令 version 查看Hadoop版本。 WZB-MacBook:target wangzhibin$ hadoop version Hadoop 2.8.4 Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 17e75c2a11685af3e043aa5e604dc831e5b14674 Compiled by jdu on 2018-05-08T02:50Z Compiled with protoc 2.5.0 From source with checksum b02a59bb17646783210e979bea443b0 This command was run using /Users/wangzhibin/00_dev_suite/50_bigdata/hadoop-2.8.4/share/hadoop/common/hadoop-common-2.8.4.jar HDFS基础命令 命令格式 hadoop fs -cmd ls 列出hdfs文件系统根目录下的目录和文件 hadoop fs -ls / 列出hdfs文件系统所有的目录和文件 hadoop fs -ls -R / mkdir 一级一级的建目录，父目录不存在的话使用这个命令会报错 command: hadoop fs -mkdir eg: WZB-MacBook:~ wangzhibin$ hadoop fs -mkdir /test/20190517 WZB-MacBook:~ wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 所创建的目录如果父目录不存在就创建该父目录 hadoop fs -mkdir -p put 上传文件。hdfs file的父目录一定要存在，否则命令不会执行 command: hadoop fs -put eg: $ hadoop fs -put tmp/tmp.txt /test $ hadoop fs -ls -R /test 上传目录。hdfs dir 一定要存在，否则命令不会执行 command: hadoop fs -put ... eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put tmp/ /test WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 从键盘读取输入到hdfs file中，按Ctrl+D（Control+D）结束输入。hdfs file不能存在，否则命令不会执行 command: hadoop fs -put - eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -put - /test/20190517.tmp.txt hello world WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -ls -R /test -rw-r--r-- 1 wangzhibin supergroup 12 2019-05-17 10:44 /test/20190517.tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cat 在标准输出中显示文件内容 command: hadoop fs -cat eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -cat /test/20190517.tmp.txt hello world tail 在标准输出中显示文件末尾的1KB数据 command: hadoop fs -tail eg: WZB-MacBook:50_bigdata wangzhibin$ hadoop fs -tail /test/20190517.tmp.txt hello world get 从hdfs中下载文件到本地。local file不能和 hdfs file名字不能相同，否则会提示文件已存在，没有重名的文件会复制到本地 command: hadoop fs -get eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test/20190517.tmp.txt . WZB-MacBook:tmp wangzhibin$ ls 20190517.tmp.txt tmp.txt 拷贝多个文件或目录到本地时，本地要为文件夹路径 command: hadoop fs -get ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -get /test . WZB-MacBook:tmp wangzhibin$ ls -l test/ total 24 drwxr-xr-x 2 wangzhibin staff 64 5 17 10:56 20190517 -rw-r--r-- 1 wangzhibin staff 12 5 17 10:56 20190517.tmp.txt drwxr-xr-x 3 wangzhibin staff 96 5 17 10:56 tmp -rw-r--r-- 1 wangzhibin staff 4662 5 17 10:56 tmp.txt 注意：如果用户不是root， local 路径要为用户文件夹下的路径，否则会出现权限问题。 rm 每次可以删除多个文件或目录 command: hadoop fs -rm ... hadoop fs -rm -r ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -rm /test/20190517.tmp.txt Deleted /test/20190517.tmp.txt WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:49 /test/20190517 drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt cp HDFS拷贝文件或文件夹。目标文件或者文件夹不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件还存在。 command: hadoop fs -cp hadoop fs -cp ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -cp /test/tmp.txt /test/20190517/ WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:03 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 10:42 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt mv HDFS移动文件或文件夹。目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件不存在。源路径有多个时，目标路径必须为目录，且必须存在。 command: hadoop fs -mv hadoop fs -mv ... eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -mv /test/20190517/tmp.20190519.txt /test/tmp WZB-MacBook:tmp wangzhibin$ hadoop fs -ls -R /test drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/20190517 -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:03 /test/20190517/tmp.txt drwxr-xr-x - wangzhibin supergroup 0 2019-05-17 11:06 /test/tmp -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 11:04 /test/tmp/tmp.20190519.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:42 /test/tmp/tmp.txt -rw-r--r-- 1 wangzhibin supergroup 4662 2019-05-17 10:40 /test/tmp.txt 注意：跨文件系统的移动（local到hdfs或者反过来）都是不允许的 count 统计hdfs对应路径下的目录个数，文件个数，文件总计大小 显示为目录个数，文件个数，文件总计大小，输入路径 command: hadoop fs -count eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -count /test 3 4 18648 /test du 显示hdfs对应路径下每个文件夹和文件的大小 command: hadoop fs -du eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du /test 4662 /test/20190517 9324 /test/tmp 4662 /test/tmp.txt 显示hdfs对应路径下所有文件和的大小 command: hadoop fs -du -s eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -s /test 18648 /test 显示hdfs对应路径下每个文件夹和文件的大小,文件的大小用方便阅读的形式表示，例如用64M代替67108864 command: hadoop fs -du -h eg: WZB-MacBook:tmp wangzhibin$ hadoop fs -du -h /test 4.6 K /test/20190517 9.1 K /test/tmp 4.6 K /test/tmp.txt HDFS高级命令 以下命令参考：hadoop HDFS常用文件操作命令。没有实践。 moveFromLocal hadoop fs -moveFromLocal ... 与put相类似，命令执行后源文件 local src 被删除，也可以从从键盘读取输入到hdfs file中 copyFromLocal hadoop fs -copyFromLocal ... 与put相类似，也可以从从键盘读取输入到hdfs file中 moveToLocal 当前版本中还未实现此命令 copyToLocal hadoop fs -copyToLocal ... 与get相类似 getmerge hadoop fs -getmerge 将hdfs指定目录下所有文件排序后合并到local指定的文件中，文件不存在时会自动创建，文件存在时会覆盖里面的内容 hadoop fs -getmerge -nl 加上nl后，合并到local file中的hdfs文件之间会空出一行 text hadoop fs -text 将文本文件或某些格式的非文本文件通过文本格式输出 setrep hadoop fs -setrep -R 3 改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作 stat hdoop fs -stat [format] 返回对应路径的状态信息 [format]可选参数有：%b（文件大小），%o（Block大小），%n（文件名），%r（副本个数），%y（最后一次修改日期和时间） 可以这样书写hadoop fs -stat %b%o%n ，不过不建议，这样每个字符输出的结果不是太容易分清楚 archive hadoop archive -archiveName name.har -p * 命令中参数name：压缩文件名，自己任意取； ：压缩文件所在的父目录；：要压缩的文件名；：压缩文件存放路径\\示例：hadoop archive -archiveName hadoop.har -p /user 1.txt 2.txt /des* 示例中将hdfs中/user目录下的文件1.txt，2.txt压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下，如果1.txt，2.txt不写就是将/user目录下所有的目录和文件压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下 显示har的内容可以用如下命令： hadoop fs -ls /des/hadoop.jar 显示har压缩的是那些文件可以用如下命令 hadoop fs -ls -R har:///des/hadoop.har 注意：har文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少NameNode和DataNode过多的空间浪费。 balancer hdfs balancer 如果管理员发现某些DataNode保存数据过多，某些DataNode保存数据相对较少，可以使用上述命令手动启动内部的均衡过程 dfsadmin hdfs dfsadmin -help 管理员可以通过dfsadmin管理HDFS，用法可以通过上述命令查看 hdfs dfsadmin -report 显示文件系统的基本数据 hdfs dfsadmin -safemode enter：进入安全模式；leave：离开安全模式；get：获知是否开启安全模式； wait：等待离开安全模式 distcp 用来在两个HDFS之间拷贝数据 MapReduce命令 命令帮助 WZB-MacBook:target wangzhibin$ mapred -help Usage: mapred [--config confdir] [--loglevel loglevel] COMMAND where COMMAND is one of: pipes run a Pipes job job manipulate MapReduce jobs queue get information regarding JobQueues classpath prints the class path needed for running mapreduce subcommands historyserver run job history servers as a standalone daemon distcp copy file or directories recursively archive -archiveName NAME -p * create a hadoop archive archive-logs combine aggregated logs into hadoop archives hsadmin job history server admin interface Most commands print help when invoked w/o parameters. 列出所有任务 WZB-MacBook:target wangzhibin$ mapred job -list all 19/05/20 10:22:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 Total jobs:1 JobId State StartTime UserName Queue Priority UsedContainers RsvdContainers UsedMem RsvdMem NeededMem AM info job_1558104288185_0001 SUCCEEDED 1558104322342 wangzhibin default DEFAULT N/A N/A N/A N/A N/A http://WZB-MacBook.local:8088/proxy/application_1558104288185_0001/ 强制停止任务 mapred job -kill 参考资料 1.0.4版本官方文档-Hadoop Shell命令 hadoop HDFS常用文件操作命令 大数据基本组件（Hadoop、HDFS、MapRed、YARN）入门命令 Copyright © zhbwang all right reserved，powered by Gitbook修订时间： 2019-06-05 21:59:28 "}}